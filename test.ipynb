{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WVUnKT7kYkGa",
        "outputId": "233889d3-8dbd-4c71-82c5-5805a65753ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.9/486.9 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.2/168.2 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain huggingface_hub sentence_transformers faiss-cpu unstructured chromadb Cython tiktoken unstructured[local-inference]  groq -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAiAo4-3YsBX",
        "outputId": "fe30b6a4-e35f-48ba-d620-321c27b6d2f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Provide your HUGGINGFACEHUB TOKEN··········\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Provide your HUGGINGFACEHUB TOKEN\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qKMgCNqF3G5"
      },
      "source": [
        "#Text File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0eMqY3ZCKRw8",
        "outputId": "10c7460b-bb53-4d6c-e327-6ad8904a1eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.11-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.9)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting langchain<0.4.0,>=0.3.11 (from langchain-community)\n",
            "  Downloading langchain-0.3.11-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-core<0.4.0,>=0.3.24 (from langchain-community)\n",
            "  Downloading langchain_core-0.3.24-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.11->langchain-community) (2.9.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.28.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.11->langchain-community) (2.23.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.2)\n",
            "Downloading langchain_community-0.3.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain-0.3.11-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.24-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: httpx-sse, pydantic-settings, langchain-core, langchain, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.21\n",
            "    Uninstalling langchain-core-0.3.21:\n",
            "      Successfully uninstalled langchain-core-0.3.21\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.9\n",
            "    Uninstalling langchain-0.3.9:\n",
            "      Successfully uninstalled langchain-0.3.9\n",
            "Successfully installed httpx-sse-0.4.0 langchain-0.3.11 langchain-community-0.3.11 langchain-core-0.3.24 pydantic-settings-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5yoUsJkY0R0",
        "outputId": "f9f33664-df95-4a81-a56a-d9002dca767b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total documents loaded: 219\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_directory = \"research_papers\"\n",
        "\n",
        "# List to store all loaded documents\n",
        "documents = []\n",
        "\n",
        "# Iterate through the files in the directory\n",
        "for file in os.listdir(pdf_directory):\n",
        "    if file.endswith(\".pdf\"):\n",
        "        pdf_path = os.path.join(pdf_directory, file)  # Create full path to the PDF\n",
        "        try:\n",
        "            # Load the PDF file\n",
        "            loader = PyPDFLoader(pdf_path)\n",
        "            # Extend the documents list with pages from the current PDF\n",
        "            documents.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "# Check the total number of documents loaded\n",
        "print(f\"Total documents loaded: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "IQjUOkj_Y1lt",
        "outputId": "969f4ff1-8658-4355-af27-e6b4354cd90a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 0}, page_content='Transformers: Learning with Purely Attention\\nBased Networks\\nLecture Notes on Deep Learning\\nAvi Kak and Charles Bouman\\nPurdue University\\nSaturday 4th May, 2024 05:38\\n© 2024 Avinash Kak, Purdue University\\nPurdue University 1'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 1}, page_content='Preamble To TOC To HowTo\\nSo far you have seen two major architectural elements in the neural networks\\nmeant for deep learning (DL): convolutional layers and recurrence layers. Until\\nrecently, they were the primary reasons for the fame and glory that have been\\nbestowed on DL during recent years.\\nBut now we have another element: attention layers.\\nThat difficult problems could be solved with neural networks through purely\\nattention based logic — that is, without convolutions and recurrence — was first\\nrevealed in the paper ”Attention is All You Need” by Vaswani et el. that you can\\naccess here:\\nhttps://arxiv.org/pdf/1706.03762.pdf\\nThe goal of this lecture is to explain the basic concepts of attention-based\\nlearning with neural networks.\\nMy explanations of the fundamental ideas involved will be in the context of\\nsequence-to-sequence learning as required for automatic translation. In particular,\\nI will focus on English-to-Spanish translation as a case study. Later I’ll talk about\\nhow to apply these ideas for solving problems in computer vision.\\nPurdue University 2'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 2}, page_content='Preamble (contd.)\\nFor seq2seq learning in general, attention takes two forms: self-attention and\\ncross-attention. However, for solving recognition problems in vision (or in\\nlanguages), you may need only self-attention.\\nSelf-attention means for a neural network to figure out on its own what parts of a\\nsequence, such as a sentence of words or a sequence of patches in images,\\ntogether contribute to solving the problem at hand. For example, for language\\ntranslation, the goal of self-attention would be to figure out which words together\\nin the source language contribute to the production of any single word in the\\ntarget language. In image recognition, on the other hand, self-attention would\\nhelp a network figure out which patches together contribute the most for correctly\\npredicting the class label.\\nTo elaborate, in seq2seq learning, consider the following sentence in English:\\nI was talking to my friend about his old car to find out if it was still\\nrunning reliably.\\nFor a machine to understand this sentence, it has to figure out that the pronoun\\n“it” is strongly related to the noun “car” occurring earlier in the sentence.\\nPurdue University 3'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 3}, page_content='Preamble (contd.)\\nA neural network with self-attention would be able to accomplish what is\\nmentioned at the bottom of the previous slide. Such a network would therefore be\\nable to answer the question:\\nWhat is the current state of Charlie’s old car?\\nassuming that system already knows that “my friend” in the sentence is referring\\nto Charlie.\\nFor another example, again in seq2seq learning, consider the following Spanish\\ntranslation for the above sentence:\\nYo estaba hablando con mi amigo acerca su viejo coche para averiguar\\nsi todav ´ ıa funcionaba de manera confiable.\\nIn Spanish-to-English translation, the phrase “ su viejo coche ” could go into “ his\\nold car”, “her old car ”, or “ its old car ”. Choosing the correct form would require\\nfor the neural-network based translation system to have established the\\nrelationship between the phrase “ su viejo coche ” and the phrase “ mi amigo”.\\nAgain, a neural network endowed with self-attention should be able to make that\\nconnection.Purdue University 4'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 4}, page_content='Preamble (contd.)\\nWhile self-attention allows a neural network to establish the sort of intra-sentence\\nword-level and phrase-level relationships mentioned above, a seq2seq translation\\nnetwork also needs what’s known as cross-attention.\\nCross attention means discovering what parts of a sentence in the source language are\\nrelevant to the production of each word and each phrase in the target language.\\nTo see the need for cross-attention, consider the fact that in the\\nEnglish-to-Spanish translation example mentioned previously, the Spanish word\\n“averiguar” has several nuances in what it means: it can stand for “to discover”,\\n“to figure out”, “to find out”, etc.\\nWith cross-attention, during the training phase, the neural network would learn\\nthat when the context in the English sentence is “friend”, it would be appropriate\\nto use “averiguar” for the translation because one of its meanings is “to find out.”\\nAlong the same lines, in English-to-Spanish translation, ordinarily the English\\nword “running” would be translated into the gerund “corriendo” in Spanish,\\nhowever, on account of the context that would not be appropriate here.\\nPurdue University 5'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 5}, page_content='Preamble (contd.)\\nTo continue with the example at the bottom of the previous slide, on account of\\nthe context “car” and through the mechanism of cross-attention the neural\\nnetwork would learn that “running” is being used in the context of a “car\\nengine”, implying that that a more appropriate Spanish translation would be\\nbased on the verb “funcionar”.\\nIn this lecture, I’ll be teaching purely-attention based learning with the following\\nthree inner classes in the Transformers module of DLStudio:\\nTransformerFG\\nTransformerPreLN\\nvisTransformer\\nThe first two, meant for seq2seq learning, are only slightly different variants of\\nthe same implementation. I have kept them separate for educational reasons. The\\nlast one shows how to use the self-attention in Transformers for solving image\\nrecognition problems in computer vision.\\nPurdue University 6'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 6}, page_content='Preamble (contd.)\\nThe suffix “FG” in TransformerFG stands for “First Generation”. And the suffix\\n“PreLN” inn TransformerPreLN stands for “Pre Layer Norm”.\\nThe TransformerFG implementation is based on the transformers as first envisioned\\nin the seminal paper ” Attention is All You Need ” by Vaswani et el.:\\nhttps://arxiv.org/pdf/1706.03762.pdf\\nThe class, TransformerPreLN, incorporates the modifications suggested in ” On\\nLayer Normalization in the Transformer Architecture ” by Xiong et al.:\\nhttps://arxiv.org/pdf/2002.04745.pdf\\nThe class, visTransformer, meant for solving image recognition problems, is based\\non the paper “ An Image is Worth 16 × 16 Words: Transformers for Image\\nRecognition at Scale ” by Dosovitskiy et al.:\\nhttps://arxiv.org/pdf/2010.11929.pdf\\nAll three Transformer classes mentioned above are defined in the module file\\nTransformers.py in DLStudio.\\nPurdue University 7'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 7}, page_content='Preamble (contd.)\\nAbout the dataset I’ll be using to demonstrate Transformers for seq2seq learning,\\nDLStudio comes with the following data archive:\\nen_es_xformer_8_90000.tar.gz\\nIn the name of the archive, the number 8 refers to the maximum number of words\\nin a sentence, which translates into sentences with a maximum length of 10 when\\nyou include the SOS and EOS tokens at the two ends of a sentence. The number\\n90,000 is for how many English-Spanish sentence pairs are there in the archive.\\nThe following two scripts in the ExamplesTransformers directory of the distribution\\nare your main entry points for experimenting with the seq2seq Transformer code\\nin DLStudio:\\nseq2seq_with_transformerFG.py\\nseq2seq_with_transformerPreLN.py\\nFor the image recognition class visTransformer, I’ll use the CIFAR-10 dataset that\\nyou are already very familiar with. The following two scripts in the same\\nExamplesTransformers directory as mentioned above are your main entry points for\\nplaying with the vision related Transformer code in DLStudio:\\nimage_recog_with_visTransformer.py\\ntest_checkpoint_for_visTransformer.pyPurdue University 8'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 8}, page_content='Preamble – How to Learn from These Slides\\nAt your first reading of these slides, just focus on thoroughly understanding the\\nfollowing three topics:\\nWhat do we mean by attention and the theory and implementation of the QKV attention\\nthat the transformers are based on. This is explained on Slides 12 through 19, for a total\\nof 7 slides.\\nYour next topic should be coming to grips with the notion of multi-headed attention and\\nits DLStudio implementation as explained on Slides 20 through 29, for a total of 10 slides.\\nNow jump to the end and spend some time on the vision transformer on Slides 85 through\\n95, for a total of 10 slides.\\nThat makes for a total of 27 slides for your first reading. Note, however, in\\nthis lecture in particular, the rest of the material not included above is just\\nas important. However, after you have understood the core concept of\\nwhat exactly is meant by Attention, you should be able to breeze through\\nthe rest with relative ease.\\nPurdue University 9'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 9}, page_content='Outline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 10'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 10}, page_content='The Basic Idea of Dot-Product Attention\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 11'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 11}, page_content='The Basic Idea of Dot-Product Attention\\nOn Explaining the Attention in Transformers\\nModern attention networks were developed originally for solving\\nseq2seq learning problems as required for automatic translation from\\none language to another. Even though more recently attention\\nnetworks have also been used for solving problems in other domains\\n— for example, computer vision — seq2seq learning still feels like the\\nmost natural “domain” for a first introduction to the concept of\\ntransformers.\\nComputation of attention requires representing the basic units of your\\ninput domain with a triple of vectors denoted q for Query, k for Key,\\nand v for Value. When all of the input units are considered together\\nas tensors in each of the three categories, the same vectors become\\ntensors and are denoted Q for Query, K for Key, and V for Value.\\nAfter you have become comfortable with representing the domain\\ninformation with Query, Key, and Value vectors, the next idea you’d\\nneed to conquer is dot-product attention. In this section, I’ll introduce\\nthese ideas and the develop the notion of Single-Headed Attention.Purdue University 12'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 12}, page_content='The Basic Idea of Dot-Product Attention\\nExpressing Words Through Their (q, k, v) Vectors\\nWhat makes attention networks unique in deep learning is that the\\nQuery, Key, and Value vectors are created neither by convolution nor\\nby recurrence, but by direct matrix multiplication.\\nFor seq2seq learning, we want to express each word w in a sentence\\nthrough a query vector q, a key vector k, and a value vector v, with\\nthese three vectors being obtained through three learnable matrices\\nWq, Wk , and Wv as follows:\\nq = w · Wq k = w · Wk v = w · Wv (1)\\nwhere w is a vector of numbers that numerically represents a word in\\nthe input sentence. As you know, we refer to such vector\\nrepresentations as embeddings. Assume that the embedding size is M\\nand that all three matrices Wq, Wk and Wv are of size M × M.\\nYou can think of the q, k, and v vectors as a word w’s three\\nrepresentatives for assessing the importance of the word in question\\nto every other word in a sentence.Purdue University 13'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 13}, page_content='The Basic Idea of Dot-Product Attention\\nThe (q, k, v) Vectors for the Words (contd.)\\nContinuing with the thought in the last bullet of the previous slide, a\\nword w1 would consider a dot product of its own q vector with\\nanother word w2’s k vector for estimating its relevance to w2 and use\\nthe result of that dot product to modify its own v vector.\\nObviously, loosely speaking, there is likely to be a certain mutuality\\nand symmetry to how the v vectors for the different words get\\nmodified in this manner.\\nThe figure shown below should help with the visualization of the idea.\\n[This figure is somewhat misleading because it does NOT show the q for one word engaged in a dot-product with\\nthe k of another word. This issue disappears in the tensor formulation you will see next. ]\\nPurdue University 14'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 14}, page_content='The Basic Idea of Dot-Product Attention\\nFrom Word-Based (q, k, v) Vectors to\\nSentence-Based (Q, K, V ) Tensors\\nIn the explanation so far, I considered each word separately because\\nmy goal was to convey the basic idea of what is meant by the\\ndot-product attention. In practice, one packs all the words in a\\nsentence in a tensor of two axes, with one axis representing the\\nindividual words of the input sentence and other axis standing for the\\nembedding vectors for the words. In what follows, I’ll use X to denote\\nthe input sentence tensor. (NOTE that, for a moment, I am ignoring\\nthe fact that X will also have a batch axis.)\\nWith all the words of a sentence packed into the tensor X , we can set\\nthings up so that the network learns all of the matrices Wq, Wk , and\\nWv for all the words in a sentence simultaneously. We can therefore\\nvisualize a triplet of learnable tensors ( WQ, WK , WV ) whose different\\naxes would correspond to the individual-word ( Wq, Wk , Wv ) matrices.\\nPurdue University 15'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 15}, page_content='The Basic Idea of Dot-Product Attention\\nCalculating the (Q, K, V ) Tensors\\nCalculation of the sentence-level Query, Key, and Value tensors can\\nbe expressed more accurately and compactly as\\nQ = X · WQ K = X · WK V = X · WV (2)\\nThe tensor Q packs all the word-based query vectors into a single\\ndata object. The tensor K does the same for the word-based key\\nvectors, and the tensor V for the value vectors.\\nUsing Nw to denote the number of words in a sentence, we have\\n[Nw , M] for the shape of the input tensor X . We set the three matrices\\nWQ , WK , and WV each to be of size M × M. As a result, we’ll have\\n[Nw , M] for the shapes of the output Q, K, and V tensors.\\nUsing the Q, K, and V tensors, we can express more compactly the\\ncalculation of the attention through a modification of the V tensor via\\nthe dot-products Q · KT as shown on the next slide.\\nPurdue University 16'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 16}, page_content='The Basic Idea of Dot-Product Attention\\nCalculating Attention with (Q, K, V ) Tensors\\nUsing Q, K, and V tensors, the visual depiction of the attention\\ncalculation shown earlier on Slide 14 can be displayed more compactly\\nas:\\nRecall that in the above depiction, Nw is the number of words in a\\nsentence, M the size of the embedding vectors for the words, and M is\\nalso the size of the word-level original q, k, v vectors.\\nPurdue University 17'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 17}, page_content='The Basic Idea of Dot-Product Attention\\nCalculating Attention with (Q, K, V ) (contd.)\\nIn Python, the dot product of the Q and K tensors can be carried out\\nwith a statement like\\nQK_dot_prod = Q @ K.transpose(2,1)\\nwhere @ is Python’s infix operator for matrix multiplication. As you\\ncan see, the transpose operator is only applied to the axes indexed 1\\nand 2. Axis 0 would be for the batch index.\\nA tensor-tensor dot-product of Q and K directly carries out all the\\ndot-products at every word position in the input sentence. Since Q\\nand K are each of shape (Nw , M) for an Nw -word sentence, the\\ninner-product Q · KT is of shape Nw × Nw , whose first Nw -element row\\ncontains the values obtained by taking the dot-product of the\\nfirst-word query vector q1 with each of the k1, k2, k3, ..., kNw key vectors\\nfor each of the Nw words in the sentence. The second row of Q · KT\\nwill likewise represent the dot product of the second query vector with\\nevery key vector, and so on.\\nPurdue University 18'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 18}, page_content='The Basic Idea of Dot-Product Attention\\nCalculating Attention with (Q, K, V ) (contd.)\\nThe dot-product attention is expressed in a probabilistic form through\\nits normalization by nn.Softmax() as shown below.\\nThe following formula shows us calculating the attention — meaning\\nthe attention-weighted values for the Value tensor V — using the\\nnn.Softmax normalized dot-products:\\nZ =\\nnn.Softmax(Q · KT )\\n√\\nM\\n· V (3)\\nThe nn.Softmax is applied along the word axis (Axis 1).\\nThe additional normalization by\\n√\\nM in the formula shown above is\\nneeded to counter the property that large dimensionality for the embedding\\nvectors can result in large variances associated with the output of the dot\\nproducts.\\nThe above can be established by the fact that if you assume\\nzero-mean unit-variance independent values for the components xi\\nand yi in the summation z = PN\\ni=1 xi · yi , the output z will also be\\nzero-mean but its variance will equal N.Purdue University 19'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 19}, page_content='Multi-Headed Attention\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 20'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 20}, page_content='Multi-Headed Attention\\nMulti-Headed Attention\\nWhat I have described in the previous section is referred to as a\\nSingle-Headed Attention. As it turns out, single-headed attention is\\nnot sufficiently rich in its representational power for capturing all the\\nneeded inter-word dependencies in a sentence.\\nShown on the next slide is an illustration of Multi-Headed Attention.\\nWe now partition the input tensor X along its embedding axis into\\nNH slices and apply single-headed attention to each slice as shown in\\nthe figure.\\nThat is, each Attention Head gets to focus on a slice along the\\nembedding dimension of the input sentence tensor.\\nFor reasons that I’ll make clear later, I’ll denote the size of the\\nembedding slice given to each Attention Head by the same notation\\nsqkv that you saw earlier.\\nPurdue University 21'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 21}, page_content='Multi-Headed Attention\\nMulti-Headed Attention (contd.)\\nFigure: Correction: In the upper part of the figure, read ZK as ZNH . And, in the middle of the figure, read AHk as\\nAHNH . The symbol NH stands for the number of Attention Heads used.\\nPurdue University 22'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 22}, page_content='Multi-Headed Attention\\nMulti-Headed Attention (contd.)\\nContinuing with the notations used for Multi-Headed Attention, I’ll\\nuse NH to denote the number of Attention Heads used. Since sqkv is\\nthe size of the embedding slice fed into any single attention head, we\\nhave\\nsqkv =\\nM\\nNH\\n(4)\\nEach Attention Head learns its own values for the Q, K, and V\\ntensors with its own matrices for WQ , WK , and WV .\\nWhile each Attention Head receives only a sqkv -sized slice from the\\nembedding axis of the input sentence, the output tensors Q, K, V will\\nstill be of shape (Nw , sqkv ) for the same reason as described in the\\nprevious section.\\nSince for each Attention Head, Q and K are of shape (Nw , sqkv ) for an\\nNw -word sentence, the inner-product Q · KT is of the same shape as\\nin the previous section, that is Nw × Nw .\\nPurdue University 23'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 23}, page_content='Implementation of Attention in DLStudio’s Transformers\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 24'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 24}, page_content='Implementation of Attention in DLStudio’s Transformers\\nAttention Head Implementation\\nThe next slide shows the implementation of the AttentionHead in the\\nthree transformer classes in DLStudio.\\nIn the code shown on the next slide, all the dot-products mentioned\\npreviously are calculated in line (N). Next, as shown in line (O) we\\napply the nn.Softmax normalization to each row of the Nw × Nw -sized\\nQ · KT dot-products calculated in line (N).\\nThe resulting Nw × Nw matrix is then used to multiply the\\nNw × sqkv -sized V tensor as shown in line (V). The operations carried\\nout in lines (M) through (Q) of the code shown below can be\\nexpressed more compactly as:\\nZ =\\nnn.Softmax(Q.KT )\\n√\\nM\\n· V\\nAt this point, the shape of Z will be Nw × sqkv — ignoring again the\\nbatch axis. This is the shape of the data object returned by each\\nAttentionHead instance.\\nPurdue University 25'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 25}, page_content='Implementation of Attention in DLStudio’s Transformers\\nAttention Head Class in DLStudio’s Transformers Class\\nclass AttentionHead(nn.Module):\\ndef __init__(self, dl_studio, max_seq_length, qkv_size, num_atten_heads):\\nsuper(TransformerFG.AttentionHead, self).__init__()\\nself.dl_studio = dl_studio\\nself.qkv_size = qkv_size\\nself.max_seq_length = max_seq_length ## (A)\\nself.WQ = nn.Linear( self.qkv_size, self.qkv_size ) ## (B)\\nself.WK = nn.Linear( self.qkv_size, self.qkv_size ) ## (C)\\nself.WV = nn.Linear( self.qkv_size, self.qkv_size ) ## (D)\\nself.softmax = nn.Softmax(dim=1) ## (E)\\ndef forward(self, sent_embed_slice): ## sent_embed_slice == sentence_embedding_slice ## (F)\\nQ = self.WQ( sent_embed_slice ) ## (G)\\nK = self.WK( sent_embed_slice ) ## (H)\\nV = self.WV( sent_embed_slice ) ## (I)\\nA = K.transpose(2,1) ## (J)\\nQK_dot_prod = Q @ A ## (K)\\nrowwise_softmax_normalizations = self.softmax( QK_dot_prod ) ## (L)\\nZ = rowwise_softmax_normalizations @ V ## (M)\\ncoeff = 1.0/torch.sqrt(torch.tensor([self.qkv_size]).float()).to(self.dl_studio.device) ## (N)\\nZ = coeff * Z ## (O)\\nreturn Z\\nPurdue University 26'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 26}, page_content='Implementation of Attention in DLStudio’s Transformers\\nSelf-Attention in DLStudio’s Transformers Co-Class\\nThe AttentionHead class on the previous slide is the building block in a\\nSelfAttention layer that concatenates the outputs from all the\\nAttentionHead instances and presents the result as its own output.\\nIn the code shown for SelfAttention in Slide 29, for an input sentence\\nconsisting of Nw words and the embedding size denoted by M, the\\nsentence tensor at the input to forward() of SelfAttention in Line (B)\\non Slide 29 will be of shape (B, Nw , M) where B is the batch size.\\nAs explained earlier, this tensor is sliced off into num atten heads\\nsections along the embedding axis and each slice shipped off to a\\ndifferent instance of AttentionHead.\\nTherefore, the shape of what is seen by each AttentionHead in its\\nforward() in Line (F) is [B, Nw , sqkv ] where sqkv equals M/num atten heads.\\nThe slicing of the sentence tensor, shipping off of each slice to an\\nAttentionHead instance, and the concatenation of the results returned by\\nthe AttentionHead instances happens in the loop in line (C) on Slide 29.Purdue University 27'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 27}, page_content='Implementation of Attention in DLStudio’s Transformers\\nSelf Attention (contd.)\\nYou will add significantly to your understanding of how the attention\\nmechanism works if you realize that the shape of the output tensor\\nproduced by a SelfAttention layer is exactly the same as the shape of\\nits input. That is, if the shape of the input argument sentence tensor in\\nLine (B) on the next slide is [B, Nw , M], that will also be the shape of\\nthe output produced by layer.\\nIf you would not mind ignoring the batch axis for a moment, the\\ninput/output tensor shapes for a SelfAttention layer are both [Nw , M]\\nwhere Nw is the number of words in the input sentence and M the size\\nof the embedding vector for each word. You could therefore say that\\nthe basic purpose of self-attention is to generate attention-enriched\\nversions of the embedding vectors for the words.\\nAs you will see later, the statement made above applies to all of the\\ncomponents of a transformer.\\nPurdue University 28'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 28}, page_content='Implementation of Attention in DLStudio’s Transformers\\nSelf Attention (contd.)\\nclass SelfAttention(nn.Module):\\ndef __init__(self, dls, xformer, num_atten_heads):\\nsuper(TransformerFG.SelfAttention, self).__init__()\\nself.dl_studio = dls\\nself.max_seq_length = xformer.max_seq_length\\nself.embedding_size = xformer.embedding_size\\nself.num_atten_heads = num_atten_heads\\nself.qkv_size = self.embedding_size // num_atten_heads\\nself.attention_heads_arr = nn.ModuleList([xformer.AttentionHead(dls,\\nself.max_seq_length, self.qkv_size, num_atten_heads) for _ in range(num_atten_heads)]) ## (A)\\ndef forward(self, sentence_tensor): ## (B)\\nconcat_out_from_atten_heads = torch.zeros( sentence_tensor.shape[0],\\nself.max_seq_length, self.num_atten_heads * self.qkv_size).float()\\nfor i in range(self.num_atten_heads): ## (C)\\nsentence_embed_slice = sentence_tensor[:, :, i * self.qkv_size : (i+1) * self.qkv_size]\\nconcat_out_from_atten_heads[:, :, i * self.qkv_size : (i+1) * self.qkv_size] = \\\\\\nself.attention_heads_arr[i](sentence_embed_slice)\\nreturn concat_out_from_atten_heads\\nPurdue University 29'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 29}, page_content='The Encoder-Decoder Architecture of a Transformer\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 30'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 30}, page_content='The Encoder-Decoder Architecture of a Transformer\\nThe Transformer Architecture\\nNow that you understand the basics of the attention mechanism in a\\ntransformer, it is time to jump to a higher perspective on the overall\\narchitecture of a transformer.\\nFor seq2seq learning, the overall architecture of a transformer is that\\nof an Encoder-Decoder. The job of the Encoder is to create an\\nattention map for the sentences in the source language and the job of\\nthe Decoder is to use that attention map for translating the\\nsource-language sentence into a target-language sentence.\\nDuring training, the loss calculated at the output of the Decoder\\npropagates backwards through both the Decoder and the Encoder.\\nThis process ensures that the attention map produced by the Encoder\\nat its output reflects the intra-word dependencies amongst the\\nsource-language sentence that take into account what’s needed for\\nachieving the ground-truth translation in the target language.\\nPurdue University 31'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 31}, page_content='The Encoder-Decoder Architecture of a Transformer\\nThe Transformer Architecture (contd.)\\nWhile Encoder-Decoder is a simple way to characterize the overall\\narchitecture of a transformer, describing the actual architecture is\\nmade a bit complicated by the fact that the Encoder is actually a\\nstack of encoders and the Decoder actually a stack of decoders as\\nshown on Slide 35.\\nIn order to make a distinction between the overall encoder and the\\nencoding elements contained therein, I refer to the overall encoder as\\nthe Master Encoder that is implemented by the class MasterEncoder in\\nDLStudio’s Transformers module. I refer to each individual encoder\\ninsider the Master Encoder as a Basic Encoder that is an instances of\\nthe class BasicEncoder.\\nPurdue University 32'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 32}, page_content='The Encoder-Decoder Architecture of a Transformer\\nThe Transformer Architecture (contd.)\\nSimilarly, on the decoder side, I refer to the overall decoder as the\\nMaster Decoder that is implemented in the class MasterDecoder. I refer\\nto each decoder in the Master Decoder as a Basic Decoder that I\\nhave implemented with the class BasicDecoder.\\nThe implementation classes mentioned above are explained in greater\\ndetail in the several sections that follow.\\nEarlier I mentioned that, ignoring the batch axis, if the sentence\\ntensor at the input to a layer of SelfAttention is of shape (Nw , M),\\nthat’s also the shape of its output.\\nAs it turns out, that shape constancy applies throughout the\\nprocessing chains on the encoder and the decoder side. The final\\noutput of the Master Encoder will also be of shape (Nw , M), as will be\\nthe shape of the input to the Master Decoder and the shape of the\\noutput from the Master Decoder.\\nPurdue University 33'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 33}, page_content='The Encoder-Decoder Architecture of a Transformer\\nThe Transformer Architecture (contd.)\\nThe number of words as represented by Nw is the value of the variable\\nmax seq length in the transformer code presented later in this section.\\nTherefore, one way of looking at all of the layers in the architecture\\nshown on the next slide is that they are all engaged in using attention\\nto enrich the embedding vectors of the words in order to allow the\\nwords to play different roles in different contexts and vis-a-vis what’s\\nneeded for sequence-to-sequence translation to work correctly.\\nPurdue University 34'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 34}, page_content='The Encoder-Decoder Architecture of a Transformer\\nEncoder-Decoder Architecture for a Transformer\\nPurdue University 35'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 35}, page_content='The Master Encoder Class\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 36'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 36}, page_content='The Master Encoder Class\\nMaster Encoder\\nThe main purpose of the MasterEncoder is to invoke a stack of\\nBasicEncoder instances on a source-language sentence tensor.\\nThe output of each BasicEncoder is fed as input to the next BasicEncoder\\nin the cascade, as illustrated in the loop in Line (B) below. The stack\\nof BasicEncoder instances is constructed in Line (A).\\nclass MasterEncoder(nn.Module):\\ndef __init__(self, dls, xformer, how_many_basic_encoders, num_atten_heads):\\nsuper(TransformerFG.MasterEncoder, self).__init__()\\nself.max_seq_length = xformer.max_seq_length\\nself.basic_encoder_arr = nn.ModuleList( [xformer.BasicEncoder(dls, xformer,\\nnum_atten_heads) for _ in range(how_many_basic_encoders)] ) ## (A)\\ndef forward(self, sentence_tensor):\\nout_tensor = sentence_tensor\\nfor i in range(len(self.basic_encoder_arr)): ## (B)\\nout_tensor = self.basic_encoder_arr[i](out_tensor)\\nreturn out_tensor\\nPurdue University 37'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 37}, page_content='The Basic Encoder Class\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 38'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 38}, page_content='The Basic Encoder Class\\nBasic Encoder\\nThe BasicEncoder consists of a layer of self-attention (SA) followed by a\\npurely feed-forward layer (FFN). You already know what is\\naccomplished by SA. The role played by FFN is the same as it does in\\nany neural network — to enhance the discrimination ability of the\\nnetwork.\\nThe output of SA goes through FFN and the output of FFN becomes\\nthe output of the BasicEncoder.\\nTo mitigate the problem of vanishing gradients, the output of each of\\nthe two components — SA and FFN — is subject to Layer Norm. In\\naddition, we use residual connections, one that wraps around the SA\\nlayer and the other that wraps around the FFN layer as shown in the\\nfigure on Slide 35.\\nDeploying a stack of BasicEncoder instances becomes easier if the\\noutput tensor from a BasicEncoder has the same shape as its input\\ntensor.Purdue University 39'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 39}, page_content='The Basic Encoder Class\\nBasic Encoder (contd.)\\nAs shown on Slide 29, the SelfAttention layer in a Basic Encoder\\nconsists of a number of AttentionHead instances, with each AttentionHead\\nmaking an independent assessment of what to say about the\\ninter-relationships between the different parts of an input sequence.\\nAs you also know already, it is the embedding axis that is segmented\\nout into disjoint slices for each AttentionHead instance. The calling\\nSelfAttention layer concatenates the outputs from all its AttentionHead\\ninstances and presents the concatenated tensor as its own output.\\nclass BasicEncoder(nn.Module):\\ndef __init__(self, dls, xformer, num_atten_heads):\\nsuper(TransformerFG.BasicEncoder, self).__init__()\\nself.dls = dls\\nself.embedding_size = xformer.embedding_size\\nself.max_seq_length = xformer.max_seq_length\\nself.num_atten_heads = num_atten_heads\\nself.self_attention_layer = xformer.SelfAttention(dls, xformer, num_atten_heads) ## (A)\\nself.norm1 = nn.LayerNorm(self.embedding_size) ## (B)\\n## What follows are the linear layers for the FFN (Feed Forward Network) part of a BasicEncoder\\nself.W1 = nn.Linear( self.max_seq_length * self.embedding_size, self.max_seq_length * 2 * self.embedding_size )\\nself.W2 = nn.Linear( self.max_seq_length * 2 * self.embedding_size, self.max_seq_length * self.embedding_size )\\nself.norm2 = nn.LayerNorm(self.embedding_size) ## (C)\\ndef forward(self, sentence_tensor):\\nsentence_tensor = sentence_tensor.float()\\nself_atten_out = self.self_attention_layer(sentence_tensor).to(self.dls.device) ## (D)\\nnormed_atten_out = self.norm1(self_atten_out + sentence_tensor) ## (E)\\nbasic_encoder_out = nn.ReLU()(self.W1( normed_atten_out.view(sentence_tensor.shape[0],-1) )) ## (F)\\nbasic_encoder_out = self.W2( basic_encoder_out ) ## (G)\\nbasic_encoder_out = basic_encoder_out.view(sentence_tensor.shape[0], self.max_seq_length, self.embedding_size )\\n## for the residual connection and layer norm for FC layer:\\nbasic_encoder_out = self.norm2(basic_encoder_out + normed_atten_out) ## (H)\\nreturn basic_encoder_out\\nPurdue University 40'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 40}, page_content='Cross Attention\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 41'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 41}, page_content='Cross Attention\\nCross Attention Class in DLStudio’s Transformers Class\\nBefore presenting the decoder side of a transformer network, I must\\nfirst explain what is meant by Cross Attention and how I have\\nimplemented it in DLStudio’s transformers.\\nWhereas self-attention consists of taking dot products of the Query\\nvectors for the individual words in a sentence with the Key vectors for\\nall the words in order to discover the inter-word relevancies in a\\nsentence, in cross-attention we take the dot products of the Query\\nvectors for the individual words in the target-language sentence with\\nthe Key vectors at the output of the Master Encoder for a given\\nsource-language sentence. These dot products then modify the Value\\nvectors supplied by the Master Encoder.\\nIn what follows, I’ll use X enc represent the tensor at the output of\\nthe MasterEncoder. Its shape will be the same as that of the source\\nsentence supplied to the MasterEncoder instance.\\nPurdue University 42'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 42}, page_content='Cross Attention\\nCross Attention (contd.)\\nIf Nw is the maximum number of words allowed in a sentence in either\\nlanguage, the X tensor that is input into the MasterEncoder will be of\\nshape (B, Nw , M) where B is the batch size, and M the size of the\\nembedding vectors for the words.\\nTherefore, the shape of the output of the MasterEncoder, X enc, is also\\n(B, Nw , M). Now let X target represent the tensor form of the\\ncorresponding target language sentences. Its shape will also be\\n(B, Nw , M).\\nThe idea of CrossAttention is to ship off the embedding-axis slices of\\nthe X enc and X target tensors to the CrossAttentionHead instances for\\nthe calculation of the dot products and, subsequently, for the output\\nof the dot products to modify the Value vectors in what was supplied\\nby the MasterEncoder.\\nPurdue University 43'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 43}, page_content='Cross Attention\\nCross Attention (contd.)\\nclass CrossAttention(nn.Module):\\ndef __init__(self, dls, xformer, num_atten_heads):\\nsuper(TransformerFG.CrossAttention, self).__init__()\\nself.dl_studio = dls\\nself.max_seq_length = xformer.max_seq_length\\nself.embedding_size = xformer.embedding_size\\nself.num_atten_heads = num_atten_heads\\nself.qkv_size = self.embedding_size // num_atten_heads\\nself.attention_heads_arr = nn.ModuleList( [xformer.CrossAttentionHead(dls,\\nself.max_seq_length, self.qkv_size, num_atten_heads) for _ in range(num_atten_heads)] )\\ndef forward(self, basic_decoder_out, final_encoder_out):\\nconcat_out_from_atten_heads = torch.zeros( basic_decoder_out.shape[0], self.max_seq_length,\\nself.num_atten_heads * self.qkv_size).float()\\nfor i in range(self.num_atten_heads):\\nbasic_decoder_slice = basic_decoder_out[:, :, i * self.qkv_size : (i+1) * self.qkv_size]\\nfinal_encoder_slice = final_encoder_out[:, :, i * self.qkv_size : (i+1) * self.qkv_size]\\nconcat_out_from_atten_heads[:, :, i * self.qkv_size : (i+1) * self.qkv_size] = \\\\\\nself.attention_heads_arr[i](basic_decoder_slice, final_encoder_slice)\\nreturn concat_out_from_atten_heads\\nPurdue University 44'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 44}, page_content='Cross Attention\\nThe CrossAttentionHead Class\\nCrossAttentionHead works the same as the regular AttentionHead described\\nearlier, except that now, in keeping with the explanation for the\\nCrossAttention class, the dot products involve the Query vector slices\\nfrom the target sequence and the Key vector slices from the\\nMasterEncoder output for the source sequence.\\nThe dot products eventually modify the Value vector slices that are\\nalso from the MasterEncoder output for the source sequence. About the\\nword ”slice” here, as mentioned earlier, what each attention head sees\\nis a slice along the embedding axis for the words in a sentence.\\nIf X target and X source represent the embedding-axis slices of the\\ntarget sentence tensor and the MasterEncoder output for the source\\nsentences, each CrossAttentionHead will compute the following dot\\nproducts:\\nQ = Xtarget · WQ K = Xsource · WK V = Xsource · WV (5)\\nPurdue University 45'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 45}, page_content='Cross Attention\\nCrossAttentionHead Class (contd.)\\nNote that the Queries Q are derived from the target sentence,\\nwhereas the Keys K and the Values V come from the source\\nsentences.\\nThe operations carried out in lines (N) through (R) can be described\\nmore compactly as:\\nZcross =\\nnn.Sofmax(Qsource · Ktarget T )\\n√\\nM\\n· Vsource (6)\\nclass CrossAttentionHead(nn.Module):\\ndef __init__(self, dl_studio, max_seq_length, qkv_size, num_atten_heads):\\nsuper(TransformerFG.CrossAttentionHead, self).__init__()\\nself.dl_studio = dl_studio\\nself.qkv_size = qkv_size\\nself.max_seq_length = max_seq_length\\nself.WQ = nn.Linear( max_seq_length * self.qkv_size, max_seq_length * self.qkv_size ) ## (B)\\nself.WK = nn.Linear( max_seq_length * self.qkv_size, max_seq_length * self.qkv_size ) ## (C)\\nself.WV = nn.Linear( max_seq_length * self.qkv_size, max_seq_length * self.qkv_size ) ## (D)\\nself.softmax = nn.Softmax(dim=1) ## (E)\\ndef forward(self, basic_decoder_slice, final_encoder_slice): ## (F)\\nQ = self.WQ( basic_decoder_slice.reshape(final_encoder_slice.shape[0],-1).float() ) ## (G)\\nK = self.WK( final_encoder_slice.reshape(final_encoder_slice.shape[0],-1).float() ) ## (H)\\nV = self.WV( final_encoder_slice.reshape(final_encoder_slice.shape[0],-1).float() ) ## (I)\\nQ = Q.view(final_encoder_slice.shape[0], self.max_seq_length, self.qkv_size) ## (J)\\nK = K.view(final_encoder_slice.shape[0], self.max_seq_length, self.qkv_size) ## (K)\\nV = V.view(final_encoder_slice.shape[0], self.max_seq_length, self.qkv_size) ## (L)\\nA = K.transpose(2,1) ## (M)\\nQK_dot_prod = Q @ A ## (N)\\nrowwise_softmax_normalizations = self.softmax( QK_dot_prod ) ## (O)\\nZ = rowwise_softmax_normalizations @ V ## (P)\\ncoeff = 1.0/torch.sqrt(torch.tensor([self.qkv_size]).float()).to(self.dl_studio.device) ## (Q)\\nZ = coeff * Z ## (R)\\nreturn Z\\nPurdue University 46'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 46}, page_content='The Basic Decoder Class\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 47'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 47}, page_content='The Basic Decoder Class\\nThe BasicDecoderWithMasking Class\\nAs with the BasicEncoder class, while a Basic Decoder also consists of a\\nlayer of SelfAttention followed by a Feedforward Network (FFN) layer,\\nbut now there is a layer of CrossAttention interposed between the two.\\nThe output from each of these three components of a Basic Decoder\\ninstance passes through a LayerNorm layer. Additionally, you have a\\nresidual connection that wraps around each component as shown in\\nthe figure on Slide 35.\\nThe Basic Decoder class in DLStudio’s transformer code is named\\nBasicDecoderWithMasking for the reason described below.\\nAn important feature of the Basic Decoder is the masking of the\\ntarget sentences during the training phase in order to ensure that\\neach predicted word in the target language depends only on those\\ntarget words that were seen PRIOR to that point.\\nPurdue University 48'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 48}, page_content='The Basic Decoder Class\\nThe BasicDecoderWithMasking Class (contd.)\\nThis recursive backward dependency is referred to as autoregressive\\nmasking. In the implementation shown below, the masking is initiated\\nand its updates established by the MasterDecoderWithMasking class to be\\ndescribed in the next section.\\nclass BasicDecoderWithMasking(nn.Module):\\ndef __init__(self, dls, xformer, num_atten_heads):\\nsuper(TransformerFG.BasicDecoderWithMasking, self).__init__()\\nself.dls = dls\\nself.embedding_size = xformer.embedding_size\\nself.max_seq_length = xformer.max_seq_length\\nself.num_atten_heads = num_atten_heads\\nself.qkv_size = self.embedding_size // num_atten_heads\\nself.self_attention_layer = xformer.SelfAttention(dls, xformer, num_atten_heads)\\nself.norm1 = nn.LayerNorm(self.embedding_size)\\nself.cross_attn_layer = xformer.CrossAttention(dls, xformer, num_atten_heads)\\nself.norm2 = nn.LayerNorm(self.embedding_size)\\n## What follows are the linear layers for the FFN (Feed Forward Network) part of a BasicDecoder\\nself.W1 = nn.Linear( self.max_seq_length * self.embedding_size, self.max_seq_length * 2 * self.embedding_size )\\nself.W2 = nn.Linear( self.max_seq_length * 2 * self.embedding_size, self.max_seq_length * self.embedding_size )\\nself.norm3 = nn.LayerNorm(self.embedding_size)\\ndef forward(self, sentence_tensor, final_encoder_out, mask):\\n## self attention\\nmasked_sentence_tensor = sentence_tensor\\nif mask is not None:\\nmasked_sentence_tensor = self.apply_mask(sentence_tensor, mask, self.max_seq_length, self.embedding_size)\\nZ_concatenated = self.self_attention_layer(masked_sentence_tensor).to(self.dls.device)\\nZ_out = self.norm1(Z_concatenated + masked_sentence_tensor)\\n## for cross attention\\nZ_out2 = self.cross_attn_layer( Z_out, final_encoder_out).to(self.dls.device)\\nZ_out2 = self.norm2( Z_out2 )\\n## for FFN:\\nbasic_decoder_out = nn.ReLU()(self.W1( Z_out2.view(sentence_tensor.shape[0],-1) ))\\nbasic_decoder_out = self.W2( basic_decoder_out )\\nbasic_decoder_out = basic_decoder_out.view(sentence_tensor.shape[0], self.max_seq_length, self.embedding_size )\\nbasic_decoder_out = basic_decoder_out + Z_out2\\nbasic_decoder_out = self.norm3( basic_decoder_out )\\nreturn basic_decoder_out\\ndef apply_mask(self, sentence_tensor, mask, max_seq_length, embedding_size):\\nout = torch.zeros(sentence_tensor.shape[0], max_seq_length, embedding_size).float().to(self.dls.device)\\nout[:,:,:len(mask)] = sentence_tensor[:,:,:len(mask)]\\nreturn out\\nPurdue University 49'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 49}, page_content='The Master Decoder Class\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 50'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 50}, page_content='The Master Decoder Class\\nMaster Decoder\\nThe primary job of the Master Decoder is to orchestrate the\\ninvocation of a stack of BasicDecoderWithMasking instances. The number\\nof BasicDecoderWithMasking instances used is a user-defined parameter.\\nThe masking that is used in each BasicDecoderWithMasking instance is set\\nhere by the Master Decoder.\\nIn Line (B) on Slide 53, we define the BasicDecoderWithMasking instances\\nneeded. The linear layer in Line (C) is needed because what the\\ndecoder side produces must ultimately be mapped as a probability\\ndistribution over the entire vocabulary for the target language.\\nWith regard to the data flow through the network, note how the mask\\nis initialized in Line (D) on Slide 53. The mask is a vector of one’s\\nthat grows with the prediction for each output word. We start by\\nsetting it equal to just a single-element vector containing a single ”1”.\\nPurdue University 51'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 51}, page_content='The Master Decoder Class\\nMasterDecoderWithMasking (contd.)\\nLines (E) and (F) in the code on the next slide declare the tensors\\nthat will store the final output of the Master Decoder. This final\\noutput consists of two tensors:\\nOne tensor holds the integer index to the target-language vocabulary\\nword where the output log-prob is maximum. [This index is needed at\\ninference time to output the words in the translation.]\\nThe other tensor holds the log-probs over the target language\\nvocabulary. The log-probs are produced by the nn.LogSoftmax in Line\\n(L).\\nPurdue University 52'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 52}, page_content='The Master Decoder Class\\nMasterDecoderWithMasking (contd.)\\nclass MasterDecoderWithMasking(nn.Module):\\ndef __init__(self, dls, xformer, how_many_basic_decoders, num_atten_heads):\\nsuper(TransformerFG.MasterDecoderWithMasking, self).__init__()\\nself.dls = dls\\nself.max_seq_length = xformer.max_seq_length\\nself.embedding_size = xformer.embedding_size\\nself.target_vocab_size = xformer.vocab_es_size ## (A)\\nself.basic_decoder_arr = nn.ModuleList([xformer.BasicDecoderWithMasking( dls, xformer,\\nnum_atten_heads) for _ in range(how_many_basic_decoders)]) ## (B)\\n## Need the following layer because we want the prediction of each target word to be a probability\\n## distribution over the target vocabulary. The conversion to probs would be done by the criterion\\n## nn.CrossEntropyLoss in the training loop:\\nself.out = nn.Linear(self.embedding_size, self.target_vocab_size) ## (C)\\ndef forward(self, sentence_tensor, final_encoder_out): ## (D)\\n## This part is for training:\\nmask = torch.ones(1, dtype=int) ## (E)\\n## A tensor with two axes, one for the batch instance and the other for storing the predicted\\n## word ints for that batch instance:\\npredicted_word_index_values = torch.ones(sentence_tensor.shape[0], self.max_seq_length,\\ndtype=torch.long).to(self.dls.device) ## (F)\\n## A tensor with two axes, one for the batch instance and the other for storing the log-prob\\n## of predictions for that batch instance. The log_probs for each predicted word over the entire\\n## target vocabulary:\\npredicted_word_logprobs = torch.zeros( sentence_tensor.shape[0], self.max_seq_length,\\nself.target_vocab_size, dtype=float).to(self.dls.device) ## (G)\\nfor mask_index in range(1, sentence_tensor.shape[1]):\\nmasked_target_sentence = self.apply_mask(sentence_tensor, mask, self.max_seq_length,\\nself.embedding_size) ## (H)\\n## out_tensor will start as just the first word, then two first words, etc.\\nout_tensor = masked_target_sentence ## (I)\\nfor i in range(len(self.basic_decoder_arr)): ## (J)\\nout_tensor = self.basic_decoder_arr[i](out_tensor, final_encoder_out, mask)\\nlast_word_tensor = out_tensor[:,mask_index] ## (K)\\nlast_word_onehot = self.out(last_word_tensor.view(sentence_tensor.shape[0],-1)) ## (L)\\noutput_word_logprobs = nn.LogSoftmax(dim=1)(last_word_onehot) ## (M)\\n_, idx_max = torch.max(output_word_logprobs, 1) ## (N)\\npredicted_word_index_values[:,mask_index] = idx_max ## (P)\\npredicted_word_logprobs[:,mask_index] = output_word_logprobs ## (Q)\\nmask = torch.cat( ( mask, torch.ones(1, dtype=int) ) ) ## (R)\\nreturn predicted_word_logprobs, predicted_word_index_values ## (S)\\ndef apply_mask(self, sentence_tensor, mask, max_seq_length, embedding_size):\\nout = torch.zeros_like(sentence_tensor).float().to(self.dls.device)\\nout[:,:len(mask),:] = sentence_tensor[:,:len(mask),:]\\nreturn out\\nPurdue University 53'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 53}, page_content='Positional Encoding for the Words\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 54'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 54}, page_content='Positional Encoding for the Words\\nPositional Encoding for the Words\\nThe main goal of positional encoding is to sensitize a neural network\\nto the position of each word in a sentence and also to each\\nembedding-vector cell for each word.\\nPositional encoding can be achieved by first constructing an array of\\nfloating-point values as illustrated on the next slide and then adding\\nthat array of numbers to the sentence tensor.\\nThe alternating columns of the 2D array shown on the next slide are\\nfilled using sine and cosine functions whose periodicities vary with the\\ncolumn index in the pattern.\\nNote that whereas the periodicities are column-specific, the\\nnumerators of the args to the sine and cosine functions are\\nword-position-specific. In the depiction shown on the next slide, each\\nrow is an embedding vector for a specific word.\\nPurdue University 55'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 55}, page_content='Positional Encoding for the Words\\nPositional Encoding (contd.)\\nIn the pattern shown below to illustrate positional encoding, I am\\nassuming that the size of the word embedding vectors is 512 and that\\nwe have a max of 10 words in the input sentence.\\nalong the embedding vector index i -->\\ni=0 | i=1 | i=2 | i=3 | ........... | i=511\\n-------------------------------------------------------------------------\\nw pos=0 | | | | |\\no -------------------------------------------------------------------------\\nr pos=1 | | | | |\\nd -------------------------------------------------------------------------\\npos=2 | | | | |\\ni -------------------------------------------------------------------------\\nn .\\nd .\\ne .\\nx -------------------------------------------------------------------------\\npos=9 | | | | |\\n-------------------------------------------------------------------------\\n| |\\n| |\\n| |_________________\\n| |\\n| |\\nV V\\npos pos ## (D)\\nsin( ------------- ) cos( ------------- )\\n100^{2i/512} 100^{2i/512} ## (E)\\nIn this case, the sentence tensor is of shape (10 , 512). So the array of\\npositional-encoding numbers we need to construct will also be of\\nshape (10, 512). We need to fill the alternating columns of this\\n(10, 512) array with sin() and cos() values as shown above.Purdue University 56'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 56}, page_content='Positional Encoding for the Words\\nPositional Encoding (contd.)\\nTo appreciate the significance of the values shown on the previous\\nslide, first note that one period of a sinusoidal function like sin(pos) is\\n2 ∗ π with respect to the word index pos. That would amount to only\\nabout six words. That is, there would only be roughly six words in\\none period if we just use sin(pos) for the positional indexing needed\\nfor the pattern shown on the previous slide.\\nOn the other hand, one period of a sinusoidal function like sin(pos/k)\\nis 2 ∗ pi ∗ k with respect to the word index pos. So if k = 100, we\\nhave a periodicity of about 640 word positions along the pos axis.\\nThe important point is that every individual column in the 2D pattern\\nshown above gets a unique periodicity and that the alternating\\ncolumns are characterized by sine and cosine functions.\\nShown on the next slide is the function in DLStudio’s transformer\\ncode that implements positional encoding.\\nPurdue University 57'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 57}, page_content='Positional Encoding for the Words\\nPositional Encoding (contd.)\\ndef apply_positional_encoding(self, sentence_tensor):\\nposition_encodings = torch.zeros_like( sentence_tensor, dtype=float )\\n## Calling unsqueeze() with arg 1 causes the \"row tensor\" to turn into a \"column tensor\"\\n## which is needed in the products in lines (F) and (G). We create a 2D pattern by\\n## taking advantage of how PyTorch has overloaded the definition of the infix ’*’\\n## tensor-tensor multiplication operator. It in effect creates an output-product of\\n## of what is essentially a column vector with what is essentially a row vector.\\nword_positions = torch.arange(0, self.max_seq_length).unsqueeze(1)\\ndiv_term = 1.0 / (100.0 ** ( 2.0 * torch.arange(0,\\nself.embedding_size, 2) / float(self.embedding_size) ))\\nposition_encodings[:, :, 0::2] = torch.sin(word_positions * div_term) ## (F)\\nposition_encodings[:, :, 1::2] = torch.cos(word_positions * div_term) ## (G)\\nreturn sentence_tensor + position_encodings\\nPurdue University 58'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 58}, page_content='TransformerFG and TransformerPreLN\\nClasses in DLStudio\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 59'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 59}, page_content='TransformerFG and TransformerPreLN\\nClasses in DLStudio\\nThe Two Transformer Classes in DLStudio\\nEverything I have said so far in this lecture is for the transformers as\\noriginally envisioned in the much celebrated Vaswani et el. paper. In\\nDLStudio, my implementation for that architecture is in the class\\nTransformerFG where the suffix “FG” stands for “First Generation”.\\nAuthors who followed that original publication observed that the\\nVaswani et el. architecture was difficult to train and that was the\\nreason why it required a carefully designed “warm-up” phase during\\ntraining in which the learning-rate was at first increased very slowly\\nand then decreased again.\\nIn particular, it was observed by by Xiong et al. in their paper “ On\\nLayer Normalization in the Transformer Architecture ” that using LayerNorm\\nafter each residual connection in the Vaswani et al. design\\ncontributed significantly to the stability of the learning process.\\nPurdue University 60'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 60}, page_content='TransformerFG and TransformerPreLN\\nClasses in DLStudio\\nTransformerFG vs. TransformerPreLN\\nXiong et al. advocated changing the point at which the LayerNorm is\\ninvoked in the original design. In the two diagrams shown on the next\\nslide, the one at left is for the encoder layout in TransformerFG and the\\none on right for the same in TransformerPreLN for the design proposed\\nby Xiong et al.\\nAs you can see in the diagrams, in TransformerFG, each of the two\\ncomponents in the BasicEncoder — Self Attention and FFN — is\\nfollowed with a residual connection that wraps around the\\ncomponent. That is, in TransformerFG, the residual connection is\\nfollowed by LayerNorm.\\nOn the other hand, in TransformerPreLN, the LayerNorm for each\\ncomponent is used prior to the component and the residual\\nconnection wraps around both the LayerNorm layer and the component,\\nas shown at right below.\\nPurdue University 61'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 61}, page_content='TransformerFG and TransformerPreLN\\nClasses in DLStudio\\nTransformerFG vs. TransformerPreLN (contd.)\\nPurdue University 62'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 62}, page_content='TransformerFG and TransformerPreLN\\nClasses in DLStudio\\nTransformerFG vs. TransformerPreLN (contd.)\\nWhile the the difference between TransformerFG and TransformerPreLN\\ndepicted in the diagram on the previous slide specifically addresses the\\nbasic encoder, the same difference carries over to the decoder side.\\nIn TransformerPreLN, inside each Basic Decoder, you will have three\\ninvocations of LayerNorm, one before the Self-Attention layer, another\\none before the call to Cross-Attention and, finally, one more\\napplication of LayerNorm prior to the FFN layer.\\nPurdue University 63'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 63}, page_content='Regarding the Difficulty of Training a Transformer Network\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 64'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 64}, page_content='Regarding the Difficulty of Training a Transformer Network\\nTraining Transformer Networks and the Sudden\\nModel Divergence\\nTransformers, in general, are difficult to train and that’s especially the\\ncase with TransformerFG. Using the same learning rate throughout\\nthe training process either results in excessively slow learning if the\\nlearning-rate is too small, or unstable learning if the learning-rate is\\nnot small enough.\\nWhen transformer learning becomes unstable, you get what’s known\\nas sudden model divergence , which means roughly the same thing as\\nmode collapse for the case of training a GAN.\\nAs you are training a transformer model, you would want to use some\\nmetric to measure the performance of the current state of the model\\nso that you can be sure that the model is still learning and that it has\\nnot suddenly regressed into a divergence. Obviously, for such a check\\non the model, you would use an assortment of sentence pairs drawn\\nfrom the corpus.Purdue University 65'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 65}, page_content='Regarding the Difficulty of Training a Transformer Network\\nBLEU for Measuring Checkpoint Performance\\nDLStudio makes it easier to carry out such checks through the\\ncheckpoints it writes out to the disk memory every 5 epochs. You can\\nthen apply the very popular BLEU metric to the checkpoints. You\\nhave model divergence when the value returned by this metric stays\\nat 0. BLEU stands for “BiLingual Evaluation Understudy”.\\nBLEU score measures the performance of a language translation\\nframework by measuring the frequencies of the n-grams in the\\npredicted sentences in the target language for the n-grams that exist\\nin the ground-truth sentences. By n-gram here, I mean a sequence of\\nconsecutively occurring words — the qualifier n refers to the length of\\nthe sequence.\\nGiven a sentence pair, one predicted and the other the target, for a\\ngiven value of n, BLEU counts the number of n-grams in the\\npredicted sentence for each n-gram that exists in the target sentence\\nfor a set of n values.\\nPurdue University 66'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 66}, page_content='Regarding the Difficulty of Training a Transformer Network\\nThe BLEU Metric for Checkpoint Performance (contd.)\\nWhen comparing the n-grams between the predicted and the target\\nsentences, you do NOT seek a position based matching of the\\nn-grams. For a given value of n, what BLEU calculates is the\\noccurrence count for an n-gram in the predicted sentence that has a\\nmatching n-gram anywhere in the target sentence . The ratio of this\\nnumber to the total number of such n-grams in the predicted\\nsentence is the translation precision as measured for that n. Typically,\\none constructs a weighted average of these ratios for n ∈ {1, 2, 3, 4}.\\nThe above formula requires a critical modification in order to be\\neffective: You do not want the occurrence based count for an n-gram\\nin a predicted sentence to exceed the count for the same n-gram in\\nthe target sentence. [To cite an example provided by the original authors of BLEU, consider the case\\nwhen the predicted sentence is a gibberish repetition of a commonly occurring word like “the” as in the predicted\\nsentence “the the the the the the the”. Assume that the target sentence is “the cat is on the mat”. A unigram\\nbased precision in this case would return a value of 7\\n7 = 1 since the unigram “the” occurs 7 times in the predicted\\nsentence and it does occur at least once in the target sentence. To remedy this shortcoming, we require that the\\ncount returned for any n-gram not exceed the count for same n-gram in the target sentence. With that modification,\\nthe value returned for the example would be 2\\n7 . You would impose this constraint for all n in the n-grams used. ]\\nPurdue University 67'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 67}, page_content='Regarding the Difficulty of Training a Transformer Network\\nThe BLEU Metric for Checkpoint Performance (contd.)\\nSince the n-gram based counts are based solely on the predicted\\nsentences (albeit on the basis that the same n-grams exist in the\\ntarget sentences), predicted sentences much shorter than the target\\nsentences will in general score higher. [ Consider the case when when the predicted\\nsentence is “the cat is” for the target sentence “the cat is on the mat”. In this case, all of the unigram, digram,\\ntrigram based scores for the quality of the translation will be perfect. ] To guard against, the\\nBLEU metric multiplies the n-gram based scores with the factor\\ne(1−r\\nc ) when c < r where c is the length of the predicted sentence\\nand r the length of the target sentence.\\nYou use the BLEU metric in you code by calling on its implementation\\nprovided by the Natural Language Toolkit (NLTK) library. If you\\nwish, you can download the source code for the BLEU metric from:\\nhttps://www.nltk.org/_modules/nltk/translate/bleu_score.html\\nPurdue University 68'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 68}, page_content='Regarding the Difficulty of Training a Transformer Network\\nStabilizing the Learning for TransformerFG\\nFor the case of TransformerFG, the original authors of the paper on\\nwhich TransformerFG is based showed that they could prevent model\\ndivergence by starting with a very small learning rates, say 1e-9, and\\nthen ramping up linearly with each iteration of training.\\nThis is known as the learning-rate warm-up and it requires that you\\nspecify the number of training iterations for the warm-up phase.\\nTypically, during this phase, you increment the learning rate linearly\\nwith the iteration index.\\nNote that the more stable TransformerPreLN does NOT require a\\nlearning-rate warm-up — because that transformer is inherently more\\nstable. The price you pay for that stability is the much slower\\nconvergence of the model.\\nIn my own rather informal and unscientific comparisons, the\\nperformance I get with about 40 epochs of TransformerFG takes more\\nthan 100 epochs with TransformerPreLN.Purdue University 69'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 69}, page_content='Results on the English-Spanish Dataset\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 70'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 70}, page_content='Results on the English-Spanish Dataset\\nResults on the English-Spanish Dataset\\nFigure: Training loss vs. iterations for 20 epochs with the TransformerFG class in DLStudio.\\nFigure: Training loss vs. iterations for 60 epochs with the TransformerPreLN class in DLStudio.Purdue University 71'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 71}, page_content='Results on the English-Spanish Dataset\\nTranslations Produced by TransformerFG\\nAfter 40 epochs of training with TransformerFG and with 90,000 pairs\\nof English-Spanish sentences, what follows are the results produced\\non 20 randomly selected sentences from the dataset.\\nThe training was carried out on RVL Cloud using a single GPU\\n(NVIDIA GeForce RTX 2080 ) and by executing the following command\\nin the ExamplesTransformers directory of DLStudio:\\npython3 seq2seq_with_transformerFG.py\\nHere are the parameters used for training the transformer network:\\nBatch size: 50\\nEmbedding_size: 256\\nNumber Basic Encoders: 4\\nNumber Basic Decoders: 4\\nNumber Attention Heads: 4\\nNumber of Warmup Steps: 4000\\nMasking: False\\nPurdue University 72'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 72}, page_content='Results on the English-Spanish Dataset\\nTranslations Produced by TransformerFG (contd.)\\nAnd here is the timing performance:\\nTraining time per 200 iterations: 167 seconds\\nTraining time per epoch: 9 * 167 seconds = 25.05 minutes\\nTotal training time for 40 epochs: 16 hours\\nThe results are shown starting with the next slide.\\nPurdue University 73'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 73}, page_content='Results on the English-Spanish Dataset\\nTranslations Produced by TransformerFG (contd.)\\nSize of the English vocab in the dataset: 11258\\nSize of the Spanish vocab in the dataset: 21823\\nThe number of learnable parameters in the Master Encoder: 124583936\\nThe number of layers in the Master Encoder: 128\\nThe number of learnable parameters in the Master Decoder: 149886015\\nThe number of layers in the Master Decoder: 234\\nNumber of sentence pairs in the dataset: 90000\\nNo sentence is longer than 10 words (including the SOS and EOS tokens)\\nTRANSLATIONS PRODUCED:\\n1. The input sentence pair: [’SOS anybody can read it EOS’] [’SOS cualquiera puede leerlo EOS’]\\nThe translation produced by TransformerFG: EOS cualquiera puede leerlo EOS EOS EOS EOS EOS EOS [CORRECT]\\n2. The input sentence pair: [’SOS is he your teacher EOS’] [’SOS es tu profesor EOS’]\\nThe translation produced by TransformerFG: EOS es tu profesor EOS EOS EOS EOS EOS EOS [CORRECT]\\n3. The input sentence pair: [’SOS i wanted to study french EOS’] [’SOS quer´ ıa estudiar franc´ es EOS’]\\nThe translation produced by TransformerFG: EOS quer´ ıa estudiar franc´ es EOS EOS EOS EOS EOS EOS [CORRECT]\\n4. The input sentence pair: [’SOS what are you doing next monday EOS’] [’SOS qu´ e vas a hacer el pr´ oximo lunes EOS’]\\nThe translation produced by TransformerFG: EOS qu´ e vas a hacer el pr´ oximo lunes EOS EOS [CORRECT]\\n5. The input sentence pair: [’SOS it was a beautiful wedding EOS’] [’SOS fue un hermoso casamiento EOS’]\\nThe translation produced by TransformerFG: EOS fue un hermoso hermoso EOS EOS EOS EOS EOS [WRONG]\\n(Continued on the next slide .....)\\nPurdue University 74'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 74}, page_content='Results on the English-Spanish Dataset\\nTranslations Produced by TransformerFG (contd.)\\n(...... continued from the previous slide)\\n6. The input sentence pair: [’SOS there were two glasses under the mirror EOS’] [’SOS bajo el espejo hab´ ıa dos vasos EOS’]\\nThe translation produced by TransformerFG: EOS bajo el espejo hab´ ıa dos vasos EOS EOS EOS [CORRECT]\\n7. The input sentence pair: [’SOS he has a very interesting book EOS’] [’SOS ´ el tiene un libro muy divertido EOS’]\\nThe translation produced by TransformerFG: EOS ´ el tiene un libro muy divertido EOS EOS EOS [CORRECT]\\n8. The input sentence pair: [’SOS i was waiting for tom EOS’] [’SOS estaba esperando a tom EOS’]\\nThe translation produced by TransformerFG: EOS estaba esperando a tom EOS EOS EOS EOS EOS [CORRECT]\\n9. The input sentence pair: [’SOS mary has curlers in her hair EOS’] [’SOS mary lleva rulos en el pelo EOS’]\\nThe translation produced by TransformerFG: EOS mary lleva tengo en el pelo EOS EOS EOS [WRONG]\\n10. The input sentence pair: [’SOS tom thought about mary a lot EOS’] [’SOS tom pens´ o mucho acerca de mar´ ıa EOS’]\\nThe translation produced by TransformerFG: EOS tom pens´ o mucho acerca de mar´ ıa EOS EOS EOS [CORRECT]\\n11. The input sentence pair: [’SOS you are so shallow EOS’] [’SOS eres tan superficial EOS’]\\nThe translation produced by TransformerFG: EOS eres tan superficial EOS EOS EOS EOS EOS EOS [CORRECT]\\n12. The input sentence pair: [’SOS can you solve this problem EOS’] [’SOS pod´ eis resolver este problema EOS’]\\nThe translation produced by TransformerFG: EOS puedes resolver este problema EOS EOS EOS EOS EOS [CORRECT]\\n13. The input sentence pair: [’SOS they were listening to the radio EOS’] [’SOS ellos estaban escuchando la radio EOS’]\\nThe translation produced by TransformerFG: EOS ellos estaban escuchando la radio EOS EOS EOS EOS [CORRECT]\\n(Continued on the next slide .....)\\nPurdue University 75'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 75}, page_content='Results on the English-Spanish Dataset\\nTranslations Produced by TransformerFG (contd.)\\n(...... continued from the previous slide)\\n14. The input sentence pair: [’SOS come right in EOS’] [’SOS ven adentro EOS’]\\nThe translation produced by TransformerFG: EOS entra aqu´ ı EOS EOS EOS EOS EOS EOS EOS [Semantically CORRECT]\\n15. The input sentence pair: [’SOS when did you learn to swim EOS’] [’SOS cu´ ando aprendiste a nadar EOS’]\\nThe translation produced by TransformerFG: EOS cu´ ando aprendiste a nadar EOS EOS EOS EOS EOS [CORRECT]\\n16. The input sentence pair: [’SOS tom has been busy all morning EOS’] [’SOS tom estuvo ocupado toda la ma~ nana EOS’]\\nThe translation produced by TransformerFG: EOS tom ha estado toda toda ma~ nana EOS EOS EOS [WRONG]\\n17. The input sentence pair: [’SOS i just want to read EOS’] [’SOS solo quiero leer EOS’]\\nThe translation produced by TransformerFG: EOS solo quiero leer EOS EOS EOS EOS EOS EOS [CORRECT]\\n18. The input sentence pair: [’SOS tell us something EOS’] [’SOS d´ ıganos algo EOS’]\\nThe translation produced by TransformerFG: EOS dinos algo EOS EOS EOS EOS EOS EOS EOS [Semantically CORRECT]\\n19. The input sentence pair: [’SOS how often does tom play hockey EOS’] [’SOS con qu´ e frecuencia juega tom al hockey EOS’]\\nThe translation produced by TransformerFG: EOS con qu´ e frecuencia juega tom al hockey EOS EOS [CORRECT]\\n20. The input sentence pair: [’SOS he was reelected mayor EOS’] [’SOS ´ el fue reelegido alcalde EOS’]\\nThe translation produced by TransformerFG: EOS ´ el fue a alcalde EOS EOS EOS EOS EOS [WRONG]\\nPurdue University 76'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 76}, page_content='Results on the English-Spanish Dataset\\nThe Results Look Great — But What Does That Mean?\\nOn the basis of the quality of the translations shown on the previous\\nthree slides for a random collection of sentences, the results produced\\nby the TransformerFG-based network look very impressive. Does that\\nmean that I have presented a viable solution for automatic\\nEnglish-to-Spanish translation?\\nThe answer to the above question is: Not by a long shot!\\nThe most likely reason for the excellent results: Overfitting of the\\nmodel to the training data.\\nA dataset of just 90,000 sentence pairs is much too small to create a\\ngeneralizable model given the overall complexity of the transformer\\nnetwork. [Despite the fact that my transformer network is small compared to the networks used in corporate\\nlabs, it still has around 300 million learnable parameters (see Slide 74). That’s still too large a model for the\\navailable dataset. ]\\nI could have gotten more “juice” out of my small dataset if I had also\\nincorporated in the learning framework the commonly used step of\\ntokenization as a front-end and trained the model with the tokens.Purdue University 77'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 77}, page_content='Results on the English-Spanish Dataset\\nThe Results Look Great, But ... (contd.)\\nThe smallness of the dataset mentioned on the previous slide can also\\nbe measured by the size of the vocabulary. As shown on Slide 74, the\\nEnglish vocab has just 11,258 words. At the least you are going to\\nneed a vocabulary that’s five times the size I have at my disposal if\\nyou want to train a model with any power of generalization. And I’m\\nonly talking about just ordinary conversational sentences.\\nAnd that brings me to a fundamental challenge associated with\\ndeveloping deep-learning based solutions for novel problems, especially\\nif the problems require complex models like those based on\\ntransformers: The high cost of creating labeled datasets.\\nA possible solution to this challenge: Non-supervised pre-conditioning\\nof the network with unlabeled data (that’s always available in\\nabundance), followed by using the available labeled data in\\ndiscriminative learning for fine-tuning the learnable parameters for the\\ntask at hand.\\nPurdue University 78'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 78}, page_content='Results on the English-Spanish Dataset\\nThe Results Look Great, But ... (contd.)\\nTo follow up on the last bullet on the previous slide, here is an\\ninfluential 2010 paper “ Why Does Unsupervised Pre-training Help Deep\\nLearning?” by Erhan et al. with this message:\\nhttps://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf\\nHere is a very insightful quote from this paper:\\n“In virtually all instances of deep learning, the objective function is a highly non-convex function of the parameters,\\nwith the potential for many distinct local minima in the model parameter space. The principal difficulty is that not\\nall of these minima provide equivalent generalization errors and, we suggest, that for deep architectures, the\\nstandard training schemes (based on random initialization) tend to place the parameters in regions of the\\nparameters space that generalize poorly. ”\\nWhat it says is that the standard practice of initializing the learnable\\nparameters with a uniform random distributions may not lead to a\\nmodel that generalizes well. We can only expect this problem to\\nbecome worse when there’s a dearth of labeled training data.\\nPurdue University 79'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 79}, page_content='Results on the English-Spanish Dataset\\nThe Results Look Great, But ... (contd.)\\nAbout the potential of unsupervised pre-training to remediate this\\nproblem, the authors Erhan et el. go on to say:\\n“... unsupervised pre-training as an unusual form of regularization: minimizing variance and introducing bias towards\\nconfigurations of the parameter space that are useful for unsupervised learning. ”\\nThat is, we can think of pre-training from unlabeled data as a form of\\n“initialization with regularization” for the learnable parameters.\\nA rather simple way to carry out such pre-training would be to change\\nthe output of your network by possibly extending it with a\\nfully-connected layer so that the entire network acts like an\\nautoencoder. Now you can sensitize the learning weights in the\\ntransformer model by requiring that the inputs match the outputs\\nwhile you feed unlabeled data into the input.\\nIn the next section, I’ll briefly talk about a particular class of data\\npre-conditioning strategies mentioned in the above paper that are\\nknown as Generative Pre-Trained Transformer (GPT) strategies.\\nPurdue University 80'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 80}, page_content='Transformers with Generative Pre-Training (GPT)\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 81'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 81}, page_content='Transformers with Generative Pre-Training (GPT)\\nGenerative Pre-Trained Transformer (GPT)\\nThe last couple of slides talked about the general case of\\nunsupervised pre-training of the model using unlabeled datasets for\\nperformance boost especially when the labeled datasets are small.\\nGenerative pretraining (GPT) is a special case of that. [ More accurately\\nspeaking, the acronym GPT stands for Generative Pre-trained Transformer. ]\\nAs to why “generative”, as was observed by Erhan et el., suppose X\\nrepresents the input to a network and Y its output. A purely\\ndiscriminative network is only concerned about the conditional\\nP(Y |X ). On the other hand, a generative network is concerned about\\nthe joint P(X , Y ).\\n[That is, while a discriminative network focuses on just getting Y right for whatever X it is presented with. On the\\nother hand, a generative network places both the input X and the output Y on an equal footing. For these reasons,\\ngenerative approaches are less prone to overfitting than purely discriminative approaches. Becoming aware of P(X)\\nwould be akin to applying PCA to the unlabeled data in traditional machine learning. The same things happens in\\nthe deep-learning context when the input data is first mapped to embeddings with the expectation that similar\\nelements at the input would result in embedding vectors that are closer together in value. ]\\nI’ll now present some insights gleaned from the paper “ Improving\\nLanguage Understanding by Generative Pre-Training ” by Radford et al.:\\nhttps://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdfPurdue University 82'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 82}, page_content='Transformers with Generative Pre-Training (GPT)\\nGPT for Transformers (contd.)\\nIn the context of creating language models with transformers, the\\nfocus of the paper by Radford et al. is exclusively on generative\\napproaches to make such a model aware of P(X ) with unsupervised\\ntraining using unlabeled datasets.\\nThe generative pretraining as presented as proposed by Radford et al.\\nconsists of maximizing the likelihood L given by\\nL(U) =\\nX\\ni\\nP(ui\\n\\x0c\\x0c\\x0cui−k , . . .ui−1; Θ)\\nwhere U represents the “tokens” in the corpus and k the size of the\\ncontext window. [Using the words directly in creating a language model can result in too large a\\nvocabulary — you’ll need a separate representation for every possible inflection of each noun and every possible\\nconjugation of each verb. Besides, you will also run into problems with “synthesized” words like\\n“overparameterized”. Language modeling becomes more efficient if the words are first decomposed into tokens\\nthrough a step called tokenization. As you would expect, tokenization is highly language specific. ]\\nFor the purpose of pretraining, the idea would be to possibly extend\\ntransformer model you want to train so that you can measure the\\nconditional probability shown above and use its maximization as the\\nlearning objective during pretraining.Purdue University 83'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 83}, page_content='Transformers with Generative Pre-Training (GPT)\\nGPT for Transformers (contd.)\\nAs you would expect, the maximization of the pretraining objective\\nshown on the previous slide will make the network smarter about the\\ncontext for the tokens, for the words, for the sentences, etc.\\nThe weights that are learned during pretraining would give the\\nnetwork a good sense of what tokens, what words, what sentences,\\nand, perhaps, even what paragraphs constitute good sequences with\\nregard to how one word follows another, how one sentence follows\\nanother, or, even, how one para follows another.\\nI’ll be discussing these ideas in much greater detail in my next week’s\\nlecture on LLM (Large Language Models).\\nPurdue University 84'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 84}, page_content='The Vision Transformer Class visTransformer\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 85'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 85}, page_content='The Vision Transformer Class visTransformer\\nImage Recognition with a Transformer\\nAs mentioned in the Preamble, it was shown in the paper “ An Image is\\nWorth 16 × 16 Words: Transformers for Image Recognition at Scale” by Dosovitskiy\\net al. that transformers could also be used for solving image\\nrecognition problems.\\nThe authors referred to their contribution as ViT for “Vision\\nTransformer”.\\nThe main contribution of ViT was to demonstrate that if you\\nchopped up an image into an array of patches, with each patch of\\nsize 16 × 16, and if you then represented each patch with a learnable\\nembedding, you could literally use the same transformer architecture\\nas in language modeling for solving image recognition problems.\\nYou could think of the non-overlapping patches extracted from an\\nimage — left to right and top to bottom — as constituting a patch\\nsequence, in very much the same way you think of a sentence as\\nconsisting of a sequence of words.Purdue University 86'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 86}, page_content='The Vision Transformer Class visTransformer\\nImage Recognition with a Transformer (contd.)\\nThrough embedding vectors, you would then be able to represent an\\nimage by exactly the same sort of a tensor as you have seen earlier in\\nthis lecture.\\nI have illustrated this idea in the figure on the next slide that shows\\nus representing an image with an 5 × 5 array of nonoverlapping\\npatches. If a patch consists of p × p pixels, and assuming that we are\\ntalking about color images, we will have a total of p2 × 3 numeric\\nvalues in a patch.\\nFor transformer based processing, our goal is to learn to map the\\n3 × p2 numeric values in a patch to an embedding vector of size M. If\\nP is the total number of patches in an image, this mapping will\\nconvert an image into a tensor of shape [ P, M]. This is exactly the\\nsort of a tensor as for a sentence of words as shown in the lower half\\nof Slide 22.\\nPurdue University 87'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 87}, page_content='The Vision Transformer Class visTransformer\\nImage Recognition with a Transformer (contd.)\\nPurdue University 88'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 88}, page_content='The Vision Transformer Class visTransformer\\nImage Recognition with a Transformer (contd.)\\nSo far the idea of applying transformers to images seems straightforward.\\nBut here are two key highly novel ideas in the ViT architecture that would\\nnot be so easy to think of even in hindsight:\\nFor image recognition, you have a class label for every training image and the question is\\nhow to incorporate that in a transformer based neural network.\\n[In a regular convolutional neural network, you push the image through the neural network that makes a prediction\\nfor the image class label at the output. You compare the predicted label with the true label and thus estimate the\\nloss that is backproped through the network. Unfortunately, that does not work for transformer based networks\\nsimply because — if I could put it that way — such networks are more intensive in the extent of learning they need\\nto carry out. ]\\nThe authors of the ViT paper discovered that if they gave the transformer a “cell” in\\nwhich it could store its understanding of what was unique to all the images for the same\\nclass, that helped the neural network make a correct guess for the class label. This “cell”\\nis referred to as the class token in the ViT architecture.\\nAs you know already, transformer based learning for languages required positional\\nencoding for the words that gave the network a sense of the order in which the words\\nexisted in a sentence. As you’ll recall, I presented sinusoidal positional encoding for the\\ncase of language modeling on Slides 54-58. The question now is: How does one do that\\nfor patch sequences? Here again, the solution consisted of providing another “cell”, but\\nthis time on a per-patch basis , where the network can put away its understanding of the\\norder in which the patches relate to one another spatially. These per-patch cells are\\nreferred to as positional-encodings in ViT.\\nPurdue University 89'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 89}, page_content='The Vision Transformer Class visTransformer\\nCoding Issues for Vision Transformers\\nI’ll now now review some of the coding issues you are going to run\\ninto if writing your implementation for a vision transformer, or if you\\nare trying to understand the visTransformer class in the Transformers\\nmodule of DLStudio.\\nBut first you have to realize that the overall neural architecture for a\\nvision transformer is much simpler than what it is for language\\nmodeling. That is because you do not need the Decoder you saw\\nearlier for the case of languages.\\nFor example, for a vision transformer meant for image recognition,\\nyou feed the output of the Encoder into a couple of Fully Connected\\n(FC) layers. The number of nodes in the final output layer of the FC\\nsection equals the number of classes you your dataset.\\nActually, a vision transformer is even simpler than what would be\\nimplied by the above claim, as explained on the next slide.\\nPurdue University 90'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 90}, page_content='The Vision Transformer Class visTransformer\\nCoding Issues for Vision Transformers (contd.)\\nSince the sole job of a vision transformer (meant for image\\nrecognition) is to predict the class label of the input image and since\\nthe purpose of the class token mentioned on the previous slide is to\\nlearn what is unique about all the images that belong to the same\\nclass, you only need to retain the class token from the output of the\\ntransformer. That is, you would feed the embedding vector for just\\nthe class token into the FC section for the prediction of the class\\nlabel.\\nWhat that general introduction to the overall architecture of a vision\\ntransfer, I’ll do into the specifics of what you’re going to need in your\\ncode.\\nObviously, the very first thing you would need to do in your code\\nwould be to extract the patches from the images and, for each image,\\nconstruct a tensor of shape P × M for its representation, where P is\\nthe number of patches in an image and M the dimensionality of the\\nembedding representation of a patch.\\nOver the next few slides, I’ll mention the two ways extracting the\\npatches frown the images and mapping the patches to the embedding\\nvectors.\\nSubsequently, I’ll talk about augmenting the patch-sequence tensor\\nwith the class token and also bringing into play the position\\nencodings.\\nPurdue University 91'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 91}, page_content='The Vision Transformer Class visTransformer\\nCoding Issues for Vision Transformers (contd.)\\nAbout extracting the patches and mapping them to their embedding\\nvectors, you can use one of the following two ways for that. Although\\nthey look very different, under the hood they are the same.\\nYou invoke a convolutional layer in kernel-size and the stride equal the patch size.\\nLet’s say you patch size 16 × 16. You will construct an instance of the 2D convo\\noperator as follows:\\nconop = nn.Conv2d( 3, M, P, stride=P )\\nwhere 3 is for the three color channels of a training image, M the embedding size\\nand P the kernel size. By setting both the kernel and the stride to the same value,\\nyou will directly output the embedding vector of size M for each non-overlapping\\nP × P patch in the image. If your training dataset is CIFAR-10, your input images\\nare of size 32 × 32. If you want your patches to be of size 16 × 16, you would set\\nP = M = 16.\\nThe second approach is based on separately extracting the patches by calling\\ntorch.tensor.unfold() and then mapping them with an nn.Linear layer to the\\nembedding vectors, as shown below:\\nfor i, data in enumerate(self.train_data_loader):\\ninput_images, labels = data\\n...\\npatch_sequences = input_images.unfold(2, self.patch_size[0], self.patch_size[1]).unfold(3, \\\\\\nself.patch_size[0], self.patch_size[1])\\npatch_sequence_embeddings = patch_embedding_generator( patch_sequences )\\n...\\n...\\nPurdue University 92'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 92}, page_content='The Vision Transformer Class visTransformer\\nvisTransformer in DLStudio\\nThe visTransformer class in the Transformers module in DLStudio\\nconsists of the following inner classes and methods:\\nclass visTransformer(nn.Module)\\nclass PatchEmbeddingGenerator(nn.Module)\\nclass MasterEncoder(nn.Module)\\nclass BasicEncoder(nn.Module)\\nclass SelfAttention(nn.Module)\\nclass AttentionHead(nn.Module)\\ndef run_code_for_training_visTransformer(self, dls, vis_transformer, display_train_loss=False, checkpoint_dir=’checkpoints’)\\ndef run_code_for_evaluating_visTransformer(self, encoder_network, patch_embedding_generator)\\ndef run_code_for_evaluating_visTransformer(self, encoder_network, patch_embedding_generator)\\ndef run_code_for_evaluating_checkpoint(self, encoder_network, patch_embedding_generator, checkpoints_dir)\\nThe names I have used the main vision transformer class visTransformer\\nand its five inner classes should make them self-explanatory.\\nOf the five inner classes, you have already seen the last four. In what\\nfollows, I’ll present the definitions for the main class visTransformer and\\nits inner class PatchEmbeddingGenerator\\nPurdue University 93'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 93}, page_content='The Vision Transformer Class visTransformer\\nDefinition of the visTransformer Class\\nShown on the next slide is the the top-level class for the vision\\ntransformer in the Transformers module of DLStudio. We instantiate\\nthe transformer network in Lines (1), (2) and (3).\\nOf the data flow presented in Lines (4) through (9), the most notable\\nfact is the “expansion” of the class token to cover all the patches in a\\nsingle image and its concatenation with the Axis 1 of the batch. The\\nbatch is of shape [ B, P, M) where B is the batch size, P + 1 the\\nnumber of patches along with the class token, and M the embedding\\nsize. So the concatenation you see in Line (5) is along the patch Axis\\n— that is it is in accordance with the image representation shown in\\nSlide 88. Remember, whatever logic you place in the forward() of a\\nclass derived from nn.Module is automatically applied to every instance\\nin a batch.\\nAs you see in Line (7), we only retain the first embedding vector, the\\none that corresponds to the class token, for feeding into the\\nfully-connected section.\\nAbout the fully-connected (FC) section of the network in Line (2), the\\nnumber 10 for the number of nodes in the final layer is for the 10\\nclasses of the CIFAR10 dataset.\\nPurdue University 94'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 94}, page_content='The Vision Transformer Class visTransformer\\nDefinition of the visTransformer Class\\nclass visTransformer(nn.Module):\\ndef __init__(self, dl_studio, patch_size, embedding_size, num_basic_encoders, num_atten_heads,\\nsave_checkpoints=True, checkpoint_freq=10):\\nsuper(visTransformer, self).__init__()\\n...\\nself.checkpoint_freq = checkpoint_freq\\nself.learning_rate = dl_studio.learning_rate\\nself.num_patches_in_image = (dl_studio.image_size[0] // patch_size[0] ) *\\n(dl_studio.image_size[1] // patch_size[1] )\\nself.max_seq_length = self.num_patches_in_image + 1\\nself.patch_size = patch_size\\nself.patch_dimen = (patch_size[0] * patch_size[1]) * 3\\nself.embedding_size = embedding_size\\nself.num_basic_encoders = num_basic_encoders\\nself.num_atten_heads = num_atten_heads\\nself.master_encoder = visTransformer.MasterEncoder(dl_studio, self, num_basic_encoders,\\nnum_atten_heads) ## (1)\\nself.fc = nn.Sequential( ## (2)\\nnn.Dropout(p=0.1),\\nnn.Linear(embedding_size, 512),\\nnn.ReLU(inplace=True),\\nnn.Linear(512, 10),\\n)\\nself.class_token = nn.Parameter(torch.randn((1, 1, embedding_size))).cuda() ## (3)\\ndef forward(self, x):\\nclass_token = self.class_token.expand(x.shape[0], -1, -1) ## (4)\\nx = torch.cat((class_token, x), dim=1) ## (5)\\nx = self.master_encoder(x) ## (6)\\npredicted_class_tokens = x[:,0] ## (7)\\noutput = self.fc(predicted_class_tokens) ## (8)\\nreturn output ## (9)\\nPurdue University 95'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 95}, page_content='The Vision Transformer Class visTransformer\\nDefinition of the PatchEmbeddingGenerator Class\\nShown below is the class PatchEmbeddingGenerator. It is an inner class of\\nthe visTransformer class shown on the previous slide.\\nThe most notable part of the code how we add Positional Encodings\\nto the patches in Line (5). As mentioned earlier, As mentioned\\nearlier, Positional Encoding consists of a learning a parameter on\\nper-patch basis that is unique to that patch in the image. Taking all\\nof the training images into account, what is unique to each patch is\\nits position in the image.\\nclass PatchEmbeddingGenerator(nn.Module):\\ndef __init__(self, vis_xformer, embedding_size):\\nsuper(visTransformer.PatchEmbeddingGenerator, self).__init__()\\nself.num_patches_in_image = vis_xformer.num_patches_in_image\\nself.patch_dimen = vis_xformer.patch_dimen ## (num of pixels in patch) * 3 for color\\nself.embedding_size = embedding_size\\nself.embed = nn.Linear(self.patch_dimen, embedding_size) ## (1)\\nself.positional_encodings = nn.Parameter(torch.randn((1,\\nself.num_patches_in_image, self.embedding_size))) ## (2)\\ndef forward(self, x):\\nx = x.reshape(x.shape[0], -1, self.patch_dimen).cuda() ## (3)\\npatch_embeddings = self.embed(x) ## (4)\\nposition_coded_embeddings = patch_embeddings + self.positional_encodings ## (5)\\nreturn position_coded_embeddings\\nPurdue University 96'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 96}, page_content='The Vision Transformer Class visTransformer\\nThe Vision Transformer Examples in the ExamplesTransformer Directory\\nYou will find the following scripts in the ExamplesTransformers of\\nDLStudio:\\nimage_recog_with_visTransformer.py\\ntest_checkpoint_for_visTransformer.py\\nWhen you run the first script, it outputs checkpoints every 10 epochs\\n(by default). As the first is continuing to train further, you can test\\nthe quality of the model learned in a checkpoint by executing the\\nsecond script. See the doc section of the second script for to specify a\\nparticular checkpoint.\\nWithout any hyperparameter tuning, shown below are some results on\\nthe testing-portion of the CIFAR-10 dataset:\\nDisplaying the confusion matrix:\\nplane car bird cat deer dog frog horse ship truck\\nplane: 65.63 1.60 3.41 3.71 2.91 1.90 2.40 1.20 13.23 4.01\\ncar: 4.80 60.46 0.30 4.70 1.30 2.20 1.40 0.80 10.31 13.71\\nbird: 8.81 0.50 38.34 14.21 15.12 7.51 7.91 3.90 2.60 1.10\\ncat: 2.51 0.90 5.92 51.05 7.22 15.75 10.23 2.81 1.71 1.91\\ndeer: 3.70 0.20 7.80 10.40 53.50 6.20 10.20 3.80 2.70 1.50\\ndog: 1.81 0.30 6.02 29.29 6.42 44.53 6.42 2.91 1.50 0.80\\nfrog: 1.40 0.70 5.00 12.70 7.90 4.10 65.40 0.70 1.10 1.00\\nhorse: 4.11 1.00 2.71 12.34 9.93 11.74 2.81 52.26 1.00 2.11\\nship: 7.82 3.41 1.40 3.71 2.71 1.91 1.00 0.90 72.92 4.21\\ntruck: 5.10 11.50 0.70 7.00 2.00 3.60 2.40 2.30 7.50 57.90Purdue University 97'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 97}, page_content='The Vision Transformer Class visTransformer\\nTraining Loss on the CIFAR-10 Dataset Over 40\\nEpochs\\nPurdue University 98'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 98}, page_content='Using QKV Modeling for Inter-Pixel Attention\\nOutline\\n1 The Basic Idea of Dot-Product Attention 11\\n2 Multi-Headed Attention 20\\n3 Implementation of Attention in DLStudio’s Transformers 24\\n4 The Encoder-Decoder Architecture of a Transformer 30\\n5 The Master Encoder Class 36\\n6 The Basic Encoder Class 38\\n7 Cross Attention 41\\n8 The Basic Decoder Class 47\\n9 The Master Decoder Class 50\\n10 Positional Encoding for the Words 54\\n11 TransformerFG and TransformerPreLN Classes in DLStudio 59\\n12 Regarding the Difficulty of Training a Transformer Network 64\\n13 Results on the English-Spanish Dataset 70\\n14 Transformers with Generative Pre-Training (GPT) 81\\n15 The Vision Transformer Class visTransformer 85\\n16 Using QKV Modeling for Inter-Pixel Attention 99\\nPurdue University 99'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 99}, page_content='Using QKV Modeling for Inter-Pixel Attention\\nQKV for Inter-Pixel Attention\\nI am now going to revisit DLStudio’s GenerativeDiffusion module — more\\nspecifically, the AttentionBlock inner class in that module. Although the\\nAttention there is also based on the QKV concept as explained in this\\nlecture, there are significant (and very interesting differences) between\\nthe implementation of the concept you have seen so far in this lecture\\nand how the same concept is made to work for the case of diffusion.\\nAs you have seen in this lecture, the notion of the embedding vector\\nrepresentation of the basic units of the input data plays a\\nfundamental role in the original formulation of Attention. [As you have seen\\nalready, Single-Headed Attention consists of learning from the embedding vector for each input unit (such as a word\\nor a patch) a Query vector Q, a Key vector K, and a Value vector V . The QKV vectors for the different input units\\ninteract through dot-products for each input unit to figure out how it should attend to the other input units. And\\nthat’s what’s referred to as the Attention mechanism. Multi-headed attention does the same thing but by first\\nsegmenting the embedding vectors into P segments where P is the number of Attention Heads. Subsequently, the\\nQKV attention is calculated for each segment in exactly the same manner as for Single-headed attention. ]\\nThe same notion is used in UNetModel in GenerativeDiffusion for\\ninter-pixel attention at a couple of different levels in the UNet.\\nPurdue University 100'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 100}, page_content='Using QKV Modeling for Inter-Pixel Attention\\nQKV for Inter-Pixel Attention (contd.)\\nAs you would expect, the data that is input into the UNet is of shape\\n(B, C, H, W ). For calculating the inter-pixel attention, for each pixel\\nin the H × W array, we consider the C floating-point values along the\\nchannel axis as the embedding vector representation of that pixel.\\nSubsequently, (1) We first flatten the H × W array of pixels into a\\n1-dimensional pixel array — just to make it easier to write the\\ndot-product code later. (2) We use a 1-dimensional convolution on\\nthe 1-dimensional array of pixels to convert the C channels associated\\nwith each pixel into a 3 ∗ C channels.\\nSince the channel axis is used as the embedding vector at each pixel,\\nincreasing the number of channels gives us more latitude in dividing\\nthe channel axis into portions reserved for Q, K, and V .\\nThe next slide mentions a very interesting computationally efficient\\nway of implementing Vaswani attention for the inter-pixel case.\\nPurdue University 101'),\n",
              " Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 101}, page_content='Using QKV Modeling for Inter-Pixel Attention\\nQKV for Inter-Pixel Attention (contd.)\\nAn interesting difference between the formulation of Attention as in\\nVaswani et al. and the same mechanism for inter-pixel attention as\\nimplemented below is the absence of the matrices that multiply the\\nembedding vectors for the calculation of Q, K, and V .\\nIn the implementation code you will see in the GenerativeDiffusion\\nclass, the Q, K, V matrices are incorporated implicitly in the matrix\\noperator used for the 1-dimensional convolution carried out by the\\nself.qkv operator that is declared in the constructor of the\\nAttentionBlock class there.\\nIt is the self.qkv operator declared there that increases the the\\nnumber of output channels from C to 3 ∗ C. Since, under the hood, a\\nconvolution in PyTorch is implemented with a matrix-vector product\\n(as explained in my Week 8 slides), we can conceive of the matrix\\nbeing segmented along its row-axis into three different parts, one that\\noutputs the Q vector, the second that outputs the K vector, and\\nthird that output the V vector.Purdue University 102'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 0}, page_content='Citation: Zhang, E.Y.; Cheok, A.D.;\\nPan, Z.; Cai, J.; Yan, Y. From Turing to\\nTransformers: A Comprehensive\\nReview and Tutorial on the Evolution\\nand Applications of Generative\\nTransformer Models. Sci 2023, 5, 46.\\nhttps://doi.org/10.3390/sci5040046\\nAcademic Editors: Carlo Cattani,\\nDioneia Motta Monte-Serrat,\\nFrancesco M. Donini and Paolo\\nBellavista\\nReceived: 1 November 2023\\nRevised: 3 December 2023\\nAccepted: 11 December 2023\\nPublished: 15 December 2023\\nCopyright: © 2023 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nReview\\nFrom T uring to Transformers: A Comprehensive Review and\\nT utorial on the Evolution and Applications of Generative\\nTransformer Models\\nEmma Yann Zhang 1,*, Adrian David Cheok 2,*\\n , Zhigeng Pan 1, Jun Cai 2\\n and Ying Yan2\\n1 School of Artificial Intelligence, Nanjing University of Information Science and Technology,\\nNanjing 210044, China; zgpan@nuist.edu.cn\\n2 School of Automation, Nanjing University of Information Science and Technology, Nanjing 210044, China;\\nj.cai@nuist.edu.cn (J.C.); ying.yan@nuist.edu.cn (Y.Y.)\\n* Correspondence: 202351620003@nuist.edu.cn (E.Y.Z.); adrian@imagineeringinstitute.org (A.D.C.)\\nAbstract: In recent years, generative transformers have become increasingly prevalent in the field\\nof artificial intelligence, especially within the scope of natural language processing. This paper\\nprovides a comprehensive overview of these models, beginning with the foundational theories\\nintroduced by Alan Turing and extending to contemporary generative transformer architectures.\\nThe manuscript serves as a review, historical account, and tutorial, aiming to offer a thorough\\nunderstanding of the models’ importance, underlying principles, and wide-ranging applications.\\nThe tutorial section includes a practical guide for constructing a basic generative transformer model.\\nAdditionally, the paper addresses the challenges, ethical implications, and future directions in the\\nstudy of generative models.\\nKeywords: generative transformers; large language models; generative models; Alan Turing; artificial\\nintelligence; machine learning; neural network; natural language processing\\n1. Introduction\\n1.1. Background and Significance of Generative Models in AI\\nGenerative models serve as an essential building block in the realm of artificial intelli-\\ngence (AI). At their core, these models are designed to generate new data samples that are\\nsimilar to the input data they have been trained on. This capability has profound implica-\\ntions, enabling machines to create, imagine, and replicate complex patterns observed in the\\nreal world.\\nThe inception of generative models can be traced back to the early days of AI, where\\nthe foundational work of Alan Turing laid the groundwork for the evolution of generative\\nmodels and the broader field of AI. Following Turing’s pioneering contributions, the field\\nwitnessed the emergence of simple algorithms designed to mimic and reproduce sequential\\ndata. An exemplar of this era is the Hidden Markov Models (HMM) proposed by Leonard\\nBaum in a series of seminal papers published in the late 1960s [1–3]. These models were\\ngroundbreaking for their time, providing a probabilistic framework to understand and\\npredict sequences. The most notable application of HMMs was in the realm of speech\\nrecognition [4], where they became a foundational component, enabling systems to decode\\nand understand human speech with increasing accuracy.\\nThe introduction of Recurrent Neural Networks (RNNs) in 1982 by John Hopfield [5]\\nand Long Short-Term Memory (LSTM) networks in 1997 by Hochreiter and Schmidhuber [6]\\nmarked significant advancements in the field. RNNs brought the ability to remember\\nprevious inputs in handling sequential data, while LSTMs addressed the challenges of long-\\nterm dependencies, making them pivotal for tasks such as time series prediction, speech\\nrecognition, and natural language processing. Together, they set foundational standards\\nfor modern generative AI models handling sequences.\\nSci 2023, 5, 46. https://doi.org/10.3390/sci5040046 https://www.mdpi.com/journal/sci'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 1}, page_content='Sci 2023, 5, 46 2 of 26\\nHowever, with the advent of deep learning and the proliferation of neural networks,\\nthe potential and capabilities of generative models have expanded exponentially. Neural-\\nbased generative models, such as Variational Autoencoders (VAEs) [7,8] introduced in 2013\\nand Generative Adversarial Networks (GANs) [9,10] introduced in the following year, have\\nshowcased the ability to generate high-fidelity new data samples based on training data,\\nranging from images to text and even music.\\nThe significance of generative models in AI is multifaceted. Firstly, they play a pivotal\\nrole in unsupervised learning, where labeled data is scarce or unavailable. By learning\\nthe underlying distribution of the data, generative models can produce new samples,\\naiding in tasks such as data augmentation [ 11,12], anomaly detection [ 13], and image\\ndenoising [14,15]. Secondly, the creative potential of these models has been harnessed in\\nvarious domains, from image [16–19], video, and music generation to drug discovery [20,21]\\nand virtual reality [22–24]. The ability of machines to generate novel and coherent content\\nhas opened up avenues previously deemed exclusive to human creativity.\\nFurthermore, generative models serve as powerful tools for understanding and inter-\\npreting complex data distributions. They provide insights into the structure and relation-\\nships within the data, enabling researchers and practitioners to uncover hidden patterns,\\ncorrelations, and features [25]. This interpretative power is especially valuable in domains\\nsuch as biology [ 26], finance [ 27], and climate science [ 28], where understanding data\\nintricacies can lead to groundbreaking discoveries.\\nGenerative models stand as a testament to the advancements and possibilities within\\nAI. Their ability to create, interpret, and innovate has not only broadened the horizons of\\nmachine learning but has also reshaped our understanding of intelligence and creativity.\\n1.2. The Rise of Transformer Architectures\\nWhile Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs)\\nhave significantly advanced the field of generative AI, another monumental shift in the\\ndeep learning landscape emerged with the introduction of the transformer architecture.\\nPresented in the seminal paper “Attention is All You Need” by a team of Google researchers\\nled by Vaswani in 2017 [29], transformers have redefined the benchmarks in a multitude of\\ntasks, particularly in natural language processing (NLP).\\nThe transformer’s innovation lies in its self-attention mechanism, which allows it to\\nweigh the significance of different parts of an input sequence, be it words in a sentence or\\npixels in an image. This mechanism enables the model to capture long-range dependencies\\nand intricate relationships in the data, overcoming the limitations of previous architec-\\ntures such as Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM)\\nnetworks. RNNs and LSTMs, while effective in handling sequential data, often struggled\\nwith long sequences due to issues such as vanishing and exploding gradients [30]. Trans-\\nformers, with their parallel processing capabilities and attention mechanisms, alleviated\\nthese challenges.\\nThe success of the transformer architecture was not immediate but became evident\\nwith the introduction of large language models such as BERT (Bidirectional Encoder Rep-\\nresentations from Transformers) and GPT (Generative Pre-trained Transformer). BERT,\\ndeveloped by researchers at Google, demonstrated the power of transformers in under-\\nstanding the context of words in a sentence by considering both left and right contexts in\\nall layers [31]. This bidirectional approach led to state-of-the-art results in several NLP\\ntasks, from question answering to sentiment analysis [32]. On the other hand, OpenAI’s\\nGPT showcased the generative capabilities of transformers [33], producing human-like text\\nand achieving remarkable performance in tasks such as machine translation [34] and text\\nsummarization [35] without task-specific training data.\\nThe transformer’s versatility extends beyond NLP . Vision Transformer (ViT) [36], an\\nadaptation of the architecture for image classification tasks, has shown that transformers\\ncan rival, if not surpass, the performance of traditional convolutional neural networks'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 2}, page_content='Sci 2023, 5, 46 3 of 26\\n(CNNs) in computer vision tasks [37,38]. This cross-domain applicability underscores the\\ntransformer’s potential and its foundational role in modern AI.\\nAnother driving factor behind the rise of transformers is the ever-growing compu-\\ntational power and the availability of large-scale datasets. Training transformer models,\\nespecially large ones, require significant computational resources. The feasibility of training\\nsuch models has been made possible due to advancements in GPU and TPU technolo-\\ngies [39], coupled with the availability of vast amounts of data to train on. The combination\\nof innovative architecture and computational prowess has led to the development of mod-\\nels with billions or even trillions of parameters, pushing the boundaries of what machines\\ncan generate to new heights.\\nGenerative AI models have undergone significant transformations since their incep-\\ntion, with each milestone contributing to the capabilities we see today. From the founda-\\ntional Turing machines to the latest GPT-4 and LLaMA models, the journey of generative\\nAI has been marked by groundbreaking advancements. A detailed timeline capturing these\\nkey milestones is presented to offer a comprehensive overview of the field’s evolution\\n(Figure 1).\\n1.3. Purpose and Structure of the Paper\\nThe fast growth in artificial intelligence, especially with recent technologies such as\\ngenerative models and transformers, highlights the need for a comprehensive study that\\nspans both their historical development and current applications. The primary objective of\\nthis paper is to provide readers with a holistic understanding of the evolution, significance,\\narchitecture, and capabilities of generative transformers, contextualized within the broader\\nlandscape of AI.\\nOur motivation for this paper is informed by the existing body of work on transformer-\\nbased models and generative AI. While there are several comprehensive reviews, each\\nfocuses on specific aspects of the topic. For example, Gozalo-Brizuela and Garrido-\\nMerchan [40] concentrate on the taxonomy and industrial implications of large generative\\nmodels, providing a compilation of popular generative models organized into various\\ncategories such as text-to-text, text-to-image, and text-to-audio. Lin et al. [41] present an\\nexhaustive review of various transformer variants, their architectural modifications, and\\napplications. Additionally, there are survey papers that focus on the use of transformers\\nfor specific tasks such as natural language processing [ 42,43], computer vision [ 44–47],\\ntime series analysis and forecasting [ 48,49], among others. These existing reviews are\\ninvaluable, but our paper aims to provide a more comprehensive overview that bridges\\nthese specialized areas.\\nWhile these papers offer valuable insights, there is a gap in the literature for a resource\\nthat combines a historical review, a hands-on tutorial, and a forward-looking perspective on\\ngenerative transformer models. Our paper aims to fill this void, serving as a comprehensive\\nguide for newcomers and seasoned researchers alike. The historical review section helps\\nreaders understand how generative AI has developed and progressed in the wider context\\nof AI. Meanwhile, our practical tutorial guides readers through the foundational concepts\\nand practical implementations, equipping them to build their own generative transformer\\nmodels. We offer a unique blend of theoretical understanding and practical know-how,\\nsetting our work apart from existing reviews. Additionally, we strive to provide a unique\\nbalance between explaining the historical evolution, technical aspects, and applications\\nof transformers. This makes our paper a go-to source for researchers and professionals\\nseeking a wholesome understanding and knowledge of transformers.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 3}, page_content='Sci 2023, 5, 46 4 of 26\\n1936\\nT uring machines\\nA theoretical framework for\\nunderstanding computation\\nand algorithmic processes.\\n1950\\nT uring test\\nThe first practical measure\\nfor machine intelligence.\\n1964\\nELIZA\\nThe first chatbot that simu-\\nlates conversations with a\\nhuman.\\n1966\\nHidden Markov Model\\nAn early statistical model\\nthat predicts sequential\\ndata.\\n1982\\nRecurrent neural network\\nA popular model for han-\\ndling sequential data with\\nmemory retaining capabili-\\nties.\\n1997\\nLSTM\\nSolves vanishing gradient\\nproblem of RNN, allow-\\ning it to process longer se-\\nquences of data.\\n2014\\nGenerative adversarial net-\\nwork\\nA framework that can gen-\\nerate new data based on\\ntraining dataset.\\n2017\\nTransformers\\nBased on the attention\\nmechanism, a scalable and\\nefficient architecture for\\nlarge language models.\\n2018\\nGPT-1, BERT\\nOpenAI introduces GPT-1\\nwith 117 million parameters.\\nGoogle introduces BERT.\\n2019\\nGPT-2\\nImproved text generation\\nwith 1.5 billion parameters.\\n2020\\nGPT-3\\nAn updated model with 175\\nbillion parameters, capable\\nof translating languages,\\nwriting essays, and generat-\\ning code.\\n2021\\nDALL-E\\nGenerates high-quality im-\\nages from textual descrip-\\ntions.\\n2022\\nChatGPT\\nSets new standards for nat-\\nural, coherent, and context-\\naware interactions in gener-\\native models.\\n2023\\nGPT-4, LLaMA\\nOpenAI releases GPT-4\\nwith 1.76 trillion parame-\\nters. Meta introduces the\\nLLaMA and LLaMA 2 mod-\\nels.\\nFigure 1. A timeline illustrating key milestones in the development of generative AI, from Turing\\nMachines to GPT-4.\\nThe structure of the paper, which is designed to guide the reader through a logical\\nprogression, is as follows:'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 4}, page_content='Sci 2023, 5, 46 5 of 26\\n• Historical Evolution: We embark on a journey tracing the roots of computational\\ntheory, starting with the foundational concepts introduced by Alan Turing. This\\nsection provides a backdrop, setting the stage for the emergence of neural networks,\\nthe challenges they faced, and the eventual rise of transformer architectures.\\n• Tutorial on Generative Transformers: Transitioning from theory to practice, this section\\noffers a practical approach to understanding the intricacies of generative transformers.\\nReaders will gain insights into the architecture, training methodologies, and best\\npractices, supplemented with code snippets and practical examples.\\n• Applications and Challenges: Building upon the foundational knowledge, we delve\\ninto the myriad applications of generative transformers, highlighting their impact\\nacross various domains. Concurrently, we address the challenges and ethical consider-\\nations associated with their use, fostering a balanced perspective.\\n• Conclusion and Future Directions: The paper concludes with a reflection on the\\ncurrent state of generative transformers, their potential trajectory, and the exciting\\npossibilities they hold for the future of AI.\\nIn essence, this paper endeavors to be more than just a review or a tutorial, it aspires\\nto be a comprehensive guide, weaving together history, theory, practice, and prospects,\\nproviding readers with a panoramic view of the world of generative transformers.\\n2. Historical Evolution\\nThe development of computational theory and artificial intelligence has been shaped\\nby pioneering figures, innovative ideas, and transformative discoveries. Central to this\\nnarrative is Alan Turing, whose unparalleled contributions laid the foundations for modern\\ncomputation and the subsequent emergence of AI. This section delves deeper into Turing’s\\ngroundbreaking work, and the lasting legacy that continues to shape the digital age.\\n2.1. Turing Machines and the Foundations of Computation\\nOne of Turing’s major contributions was the idea of the Turing machine proposed\\nin his 1936 paper titled “On Computable Numbers, with an Application to the Entschei-\\ndungsproblem” [50]. This abstract machine was a simple but powerful theoretical construct\\nthat was designed to perform computations by manipulating symbols on an infinite tape\\nbased on a set of rules. The infinite tape is divided into discrete cells, each cell can contain a\\nsymbol from a finite alphabet, and the machine itself has a “head” that can read and write\\nsymbols on the tape and move left or right. The machine’s behavior is dictated by a set\\nof transition rules, which determine its actions based on the current state and the symbol\\nbeing read. In essence, the Turing machine is a rule-based system that manipulates symbols\\non a tape, embodying the fundamental operations of reading, writing, and transitioning\\nbetween states.\\nWhile the concept might seem rudimentary, the implications of the Turing machine are\\nprofound. Turing demonstrated that this simple device, with its set of rules and operations,\\ncould compute any function that is computable, given enough time and tape. This assertion,\\nknown as the Church–Turing thesis [51] (independently proposed by Alonzo Church in\\nhis paper titled “An Unsolvable Problem of Elementary Number Theory” also published\\nin 1936 [52]), posits that any function computable by an algorithm can be computed by\\na Turing machine. This thesis, although not proven, has stood the test of time, with no\\nevidence to the contrary. It serves as a foundational pillar in computer science, defining the\\nboundaries of what is computable.\\nWorld War II saw Turing’s theoretical concept manifest in tangible, real-world applica-\\ntions. Stationed at Bletchley Park, Britain’s cryptographic hub, Turing played a key role\\nin deciphering the Enigma code used by the German military. Turing helped develop a\\nmachine called the Bombe, which expedited the decryption process of Enigma-encrypted\\nmessages [53]. This secret work was crucial for the Allies’ success and showed how com-\\nputer science could have a major impact on real-world events.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 5}, page_content='Sci 2023, 5, 46 6 of 26\\nAfter World War II, Turing turned his attention to the development of electronic\\ncomputers. He was instrumental in the design of the Automatic Computing Engine\\n(ACE) [54], one of the earliest computer models capable of storing programs. This showed\\nTuring’s forward-thinking approach to the digital age. Beyond computing, he also delved\\ninto the nature of intelligence and how it could be replicated in machines.\\nThe Turing machine’s significance transcended its immediate mathematical implica-\\ntions. The true brilliance of Turing’s insight, however, lies in the concept of universal compu-\\ntation. Turing’s subsequent proposition of a Universal Turing Machine (UTM)—a machine\\ncapable of simulating any other Turing machine given the right input and rules—was a rev-\\nolutionary idea [ 50]. Given a description of a Turing machine and its input encoded on the\\ntape, the UTM could replicate the behavior of that machine. This meta-level of computation\\nwas groundbreaking. It suggested that a single, general-purpose machine could be de-\\nsigned to perform any computational task, eliminating the need for task-specific machines.\\nThe UTM was a harbinger of modern computers, devices that can be reprogrammed to\\nexecute a wide array of tasks.\\nThe implications of universal computation extend beyond mere hardware. It chal-\\nlenges our understanding of intelligence and consciousness. If the human brain, with its\\nintricate neural networks and synaptic connections, operates on computational principles,\\nthen could it be simulated by a Turing machine? This question, which blurs the lines be-\\ntween philosophy, neuroscience, and computer science, remains one of the most intriguing\\nand debated topics in the field of artificial intelligence.\\n2.1.1. Turing’s Impact on Artificial Intelligence and Machine Learning\\nAlan Turing’s influence on the fields of artificial intelligence (AI) and machine learning\\n(ML) is both profound and pervasive. While Turing is often lauded for his foundational\\ncontributions to computational theory, his vision and insights into the realm of machine\\nintelligence have played a pivotal role in shaping the trajectory of AI and ML.\\nHis 1950 paper, “Computing Machinery and Intelligence”, Ref. [55] introduced the\\nfamous Turing Test as a practical measure of machine intelligence. Alan Turing introduced\\nthe Turing Test within the context of an “Imitation Game”, involving a man, a woman, and\\na judge as players. They communicate electronically from separate rooms, and the goal of\\nthe judge is to identify who is the woman. The man aims to deceive the judge into thinking\\nhe is the woman, while the woman assists the judge. Turing then adapts this game into\\nhis famous test by replacing the man with a machine, aiming to deceive the questioner in\\nthe same way. Although the original game focused on gender identification, this aspect is\\noften overlooked in later discussions of the Turing Test.\\nIn this work, Turing posed the provocative question: “Can machines think?” Rather\\nthan delving into the philosophical intricacies of defining “thinking”, Turing proposed a\\npragmatic criterion for machine intelligence: if a machine could engage in a conversation\\nwith a human, indistinguishably from another human, it would be deemed intelligent.\\nThis criterion, while straightforward, sparked widespread debate and research, laying the\\nfoundation for the field of artificial intelligence.\\nThe Turing Test, in many ways, encapsulated the essence of AI—the quest to create\\nmachines that can mimic, replicate, or even surpass human cognitive abilities. It set a\\nbenchmark, a gold standard for machine intelligence, challenging researchers and scientists\\nto build systems that could “think” and “reason” like humans. While the test itself has\\nbeen critiqued and refined over the years, its underlying philosophy remains central to AI:\\nthe aspiration to understand and emulate human intelligence.\\nBeyond the Turing Test, Turing’s insights into neural networks and the potential of\\nmachine learning were visionary. In a lesser-known report written in 1948, titled “Intelligent\\nMachinery” [56], Turing delved into the idea of machines learning from experience. He\\nenvisioned a scenario where machines could be trained, much like a human child, through\\na process of education. Turing postulated the use of what he termed “B-type unorganized\\nmachines”, which bear a striking resemblance to modern neural networks. These machines,'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 6}, page_content='Sci 2023, 5, 46 7 of 26\\nas Turing described, would be trained, rather than explicitly programmed, to perform tasks.\\nAlthough in its infancy at the time, this idea signaled the rise of machine learning, where\\nalgorithms learn from data rather than being explicitly programmed.\\nTuring’s exploration of morphogenesis, the biological process that causes organisms\\nto develop their shape, further showcased his interdisciplinary genius [57]. In his work on\\nreaction-diffusion systems, Turing demonstrated how simple mathematical models could\\ngive rise to complex patterns observed in nature. This work, while primarily biological in\\nits focus, has profound implications for AI and ML. It underscores the potential of simple\\nalgorithms to generate complex, emergent behavior, a principle central to neural networks\\nand deep learning.\\nAlan Turing’s impact on artificial intelligence and machine learning is immeasurable.\\nHis vision of machine intelligence, his pioneering insights into neural networks, and his\\ninterdisciplinary approach to problem-solving have left an indelible mark on the field. As\\nwe navigate the intricate landscape of modern AI, with its deep neural networks, generative\\nmodels, and transformers, it is imperative to recognize and honor Turing’s legacy. His\\nwork serves as a beacon, illuminating the path forward, reminding us of the possibilities,\\nchallenges, and the profound potential of machines that can “think”.\\n2.1.2. From Turing’s Foundations to Generative Transformers\\nThe journey from Alan Turing’s foundational concepts to the sophisticated realm\\nof generative transformers is a testament to the evolution of computational theory and\\nits application in artificial intelligence. While at first glance Turing’s work and genera-\\ntive transformers might seem worlds apart, a closer examination reveals a direct lineage\\nand influence.\\nAlan Turing’s conceptualization of the Turing machine provided the bedrock for\\nunderstanding computation. His idea of a machine that could simulate any algorithm, given\\nthe right set of instructions, laid the groundwork for the concept of universal computation.\\nThis idea, that a single machine could be reprogrammed to perform a myriad of tasks, is\\nthe precursor to the modern notion of general-purpose computing systems.\\nFast forward to the advent of neural networks, which Turing had touched upon in\\nhis lesser-known works. These networks, inspired by the human brain’s interconnected\\nneurons, were designed to learn from data. The foundational idea was that, rather than\\nbeing explicitly programmed to perform a task, these networks would “learn” by adjusting\\ntheir internal parameters based on the data they were exposed to. Turing’s vision of\\nmachines learning from experience resonates deeply with the principles of neural networks.\\nGenerative transformers, a cutting-edge development in the AI landscape, are an\\nextension of these neural networks. Transformers, with their self-attention mechanisms,\\nare designed to weigh the significance of different parts of an input sequence, capturing\\nintricate relationships within the data. The “generative” aspect of these models allows\\nthem to produce new, previously unseen data samples based on their training.\\nDrawing a direct link, Turing’s Universal Turing Machine can be seen as an early,\\nabstract representation of what generative transformers aim to achieve in a more specialized\\ndomain. Just as the Universal Turing Machine could simulate any other Turing machine,\\ngiven the right input and set of rules, generative transformers aim to generate any plausible\\ndata sample, given the right training and context. The universality of Turing’s machine\\nfinds its parallel in the versatility of generative transformers.\\nFurthermore, Turing’s exploration into machine learning, the idea of machines learning\\nfrom data rather than explicit programming, is the very essence of generative transformers.\\nThese models are trained on vast datasets, learning patterns, structures, and nuances, which\\nthey then use to generate new content. The bridge between Turing’s early insights into\\nmachine learning and the capabilities of generative transformers is a direct one, showcasing\\nthe evolution of a concept from its theoretical inception to its practical application.\\nWhile Alan Turing might not have directly worked on generative transformers, his\\nfoundational concepts, vision of machine learning, and the principles he laid down have'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 7}, page_content='Sci 2023, 5, 46 8 of 26\\ndirectly influenced and shaped their development. The journey from Turing machines to\\ngenerative transformers is a testament to the enduring legacy of Turing’s genius and the\\ncontinual evolution of artificial intelligence.\\n2.2. Early Neural Networks and Language Models\\nThe realm of artificial intelligence has witnessed a plethora of innovations and ad-\\nvancements, with neural networks standing at the forefront of this revolution. These\\ncomputational models, inspired by the intricate web of neurons in the human brain, have\\npaved the way for sophisticated language models that can understand, generate, and\\nmanipulate human language with unprecedented accuracy.\\n2.2.1. Introduction to Neural Networks\\nNeural networks [58,59], at their core, are a set of algorithms designed to recognize\\npatterns. They interpret sensory data through a kind of machine perception, labeling, and\\nclustering of raw input. These algorithms loosely mirror the way a human brain operates,\\nthus the nomenclature “neural networks”.\\nA basic neural network consists of layers of interconnected nodes or “neurons”. Each\\nconnection between neurons has an associated weight, which is adjusted during training.\\nThe fundamental equation governing the output y of a neuron is given by:\\ny = f\\n \\n∑\\ni\\nwixi + b\\n!\\n(1)\\nwhere xi are the input values,wi are the weights,b is a bias term, andf is an activation function.\\nThe activation function introduces non-linearity into the model, allowing it to learn\\nfrom error and make adjustments, which is essential for learning complex patterns. One of\\nthe commonly used activation functions is the sigmoid function, defined as:\\nf (z) = 1\\n1 + e−z (2)\\nNeural networks typically consist of an input layer, one or more hidden layers, and an\\noutput layer. The depth and complexity of a network, often referred to as its “architecture”,\\ndetermine its capacity to learn from data.\\n2.2.2. Evolution of Recurrent Neural Networks (RNNs)\\nWhile traditional neural networks have proven effective for a wide range of tasks, they\\npossess inherent limitations when dealing with sequential data. This is where Recurrent\\nNeural Networks (RNNs) come into play. RNNs are designed to recognize patterns in\\nsequences of data, such as time series or natural language.\\nThe fundamental difference between RNNs and traditional neural networks lies in the\\nformer’s ability to retain memory of previous inputs in its internal state. This is achieved\\nby introducing loops in the network, allowing information to persist.\\nThe output of an RNN at time t, denoted ht, is computed as:\\nht = f (Whh ht−1 + Wxh xt + b) (3)\\nwhere Whh and Wxh are weight matrices, xt is the input at time t, and ht−1 is the output\\nfrom the previous timestep.\\nWhile RNNs are powerful, they suffer from challenges such as the vanishing and\\nexploding gradient problems, especially when dealing with long sequences [ 30]. This\\nmakes them less effective in capturing long-term dependencies in the data.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 8}, page_content='Sci 2023, 5, 46 9 of 26\\n2.2.3. Long Short-Term Memory (LSTM) Networks\\nTo address the vanishing gradient problem of RNNs, Long Short-Term Memory\\n(LSTM) networks were introduced. LSTMs, a special kind of RNN, are designed to remem-\\nber information for extended periods [60].\\nThe core idea behind LSTMs is the cell state, a horizontal line running through the\\nentire chain of repeating modules in the LSTM. The cell state can carry information from\\nearlier time steps to later ones, mitigating the memory issues faced by traditional RNNs.\\nLSTMs introduce three gates:\\n1. Forget Gate: It decides what information from the cell state should be thrown away\\nor kept. Mathematically, the forget gate ft is given by:\\nft = σ(Wf · [ht−1, xt] +bf ) (4)\\n2. Input Gate: It updates the cell state with new information. The input gate it and the\\ncandidate values ˜Ct are computed as:\\nit = σ(Wi · [ht−1, xt] +bi) (5)\\n˜Ct = tanh(WC · [ht−1, xt] +bC) (6)\\n3. Output Gate: It determines the output based on the cell state and the input. The\\noutput ht is given by:\\nht = ot × tanh(Ct) (7)\\nwhere ot is the output gate, defined as:\\not = σ(Wo · [ht−1, xt] +bo) (8)\\nLSTMs, with their ability to capture long-term dependencies and mitigate the chal-\\nlenges faced by traditional RNNs, have paved the way for advancements in sequence\\nmodeling, particularly in the domain of natural language processing.\\n2.3. The Advent of Transformers\\nIn the ever-evolving landscape of artificial intelligence and machine learning, the\\ntransformer architecture stands out as a significant leap forward, especially in the domain\\nof natural language processing. Introduced in the seminal paper “Attention Is All You\\nNeed” by Vaswani et al. [ 29], transformers have revolutionized the way we approach\\nsequence-to-sequence tasks. This section aims to demystify the transformer architecture,\\nbreaking it down into its core components and principles.\\n2.3.1. Introduction to the Transformer Architecture\\nAt a high level, the transformer is a type of neural network architecture designed\\nto handle sequential data, making it particularly well-suited for tasks such as language\\ntranslation, text generation, and more. Unlike its predecessors, such as RNNs and LSTMs,\\nwhich process data in order, transformers leverage a mechanism called “attention” to draw\\nglobal dependencies between input and output.\\nThe heart of the transformer architecture is the attention mechanism. In essence,\\nattention allows the model to focus on different parts of the input sequence when pro-\\nducing an output sequence, much like how humans pay attention to specific words when\\nunderstanding a sentence.\\nMathematically, the attention score for a given query q and key k is computed as:\\nAttention(q, k) = exp(score(q, k))\\n∑k′ exp(score(q, k′)) (9)'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 9}, page_content='Sci 2023, 5, 46 10 of 26\\nwhere score is a function that calculates the relevance of the key k to the query q. The\\noutput of the attention mechanism is a weighted sum of values, where the weights are the\\nattention scores.\\nThe transformer model consists of an encoder and a decoder. Each of these is composed\\nof multiple layers of attention and feed-forward neural networks.\\nThe encoder takes in a sequence of embeddings (representations of input tokens)\\nand processes them through its layers. The decoder then generates the output sequence,\\nleveraging both its internal layers and the encoder’s output.\\nOne of the distinguishing features of transformers is the use of “multi-head attention”,\\nwhich allows the model to focus on different parts of the input simultaneously, capturing\\nvarious aspects of the information.\\n2.3.2. Advantages of Transformers\\nTransformers have brought significant advancements in the processing of sequential\\ndata, characterized by several key advantages. One notable feature of transformers is\\nparallelization. Unlike RNNs, which process sequences step-by-step, transformers can\\nprocess all tokens in parallel, leading to faster training times.\\nTransformers are also known for their adeptness at handling long-range dependencies.\\nThe attention mechanism enables transformers to capture relationships between tokens,\\nregardless of their distance in the sequence. This capability is particularly beneficial for\\ncomplex tasks where context and relationships between distant elements are crucial for\\naccurate interpretation and response.\\nScalability is another advantage of transformer models. Transformers are highly\\nscalable, making them well-suited for dealing with large datasets and intricate tasks. This\\nscalability ensures that transformers remain effective and efficient even as the size and\\ncomplexity of the data or the task increase.\\n2.4. Attention Mechanism: The Heart of Transformers\\nThe attention mechanism, a pivotal innovation in the realm of deep learning, has\\ntransformed the way we approach sequence-to-sequence tasks in natural language process-\\ning. Serving as the cornerstone of the transformer architecture, attention allows models to\\ndynamically focus on different parts of the input data, capturing intricate relationships and\\ndependencies. This section aims to elucidate the principles and mathematics behind the\\nattention mechanism, shedding light on its significance in the transformer architecture.\\n2.4.1. Conceptual Overview of Attention\\nIn traditional sequence-to-sequence models, such as RNNs and LSTMs, information\\nfrom the entire input sequence is compressed into a fixed-size context vector, which is then\\nused to generate the output sequence. This approach, while effective for short sequences,\\nstruggles with longer sequences as the context vector becomes a bottleneck, unable to\\ncapture all the nuances of the input data.\\nThe attention mechanism addresses this challenge by allowing the model to “attend”\\nto different parts of the input sequence dynamically, based on the current context. Instead\\nof relying on a single context vector, the model computes a weighted sum of all input\\nvectors, where the weights represent the “attention scores”.\\n2.4.2. Mathematics of Attention\\nThe core of the attention mechanism is the computation of attention scores. Given\\na query q and a set of key-value pairs (k, v), the attention score for a specific key k is\\ncomputed as:\\nscore(q, k) =qTk (10)\\nThe attention weights, which determine how much focus should be given to each\\nkey-value pair, are computed using a softmax function:'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 10}, page_content='Sci 2023, 5, 46 11 of 26\\nAttention(q, k) = exp(score(q, k))\\n∑k′ exp(score(q, k′)) (11)\\nThe output of the attention mechanism is a weighted sum of the values:\\noutput = ∑\\ni\\nAttention(q, ki)vi (12)\\nAs depicted in Figure 2, the attention mechanism computes scores based on the query\\nand keys, derives attention weights, and produces an output based on a weighted sum\\nof values.\\nQuery\\nKey\\nValue\\nscore Attention weights\\nWeighted sum\\nOutput\\nFigure 2. Schematic representation of the attention mechanism.\\n2.4.3. Significance in Transformers\\nIn the transformer architecture, attention is not just a supplementary feature; it is the\\ncore component. Transformers employ a variant called “multi-head attention”, which runs\\nmultiple attention mechanisms in parallel, capturing different types of relationships in\\nthe data.\\nThe attention mechanism’s ability to focus on different parts of the input sequence,\\nirrespective of their position, empowers transformers to handle long-range dependencies,\\nmaking them particularly effective for tasks like language translation, text summarization,\\nand more.\\nFurthermore, the self-attention mechanism, a special case where the query, key, and\\nvalue are all derived from the same input, enables transformers to weigh the significance of\\ndifferent parts of the input relative to a specific position. This is crucial for understanding\\ncontext and semantics in natural language processing tasks.\\n2.5. Generative Transformers and Their Significance\\nGenerative transformers have emerged as a groundbreaking advancement in the\\ndomain of artificial intelligence, particularly in natural language processing and generation.\\nThese models, characterized by their ability to generate coherent and contextually relevant\\nsequences of text, have set new benchmarks in various tasks, from text completion to story\\ngeneration. This section introduces the notable generative models available, including the\\nGPT series and other significant contributions in this domain.\\n2.5.1. GPT (Generative Pre-Trained Transformer) Series\\nThe GPT series, developed by OpenAI, fully demonstrates the power and potential of\\ngenerative transformers. Built upon the transformer architecture, the GPT models leverage\\nthe attention mechanism to understand and generate human-like text. The GPT series has\\nseen rapid evolution, with each iteration bringing enhanced capabilities and performance.\\nGPT-1. The first in the series, GPT-1 [61], was released in 2018. It laid the foundation for\\nsubsequent models. With 117 million parameters, it showcased the potential of transformers\\nin generating coherent paragraphs of text.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 11}, page_content='Sci 2023, 5, 46 12 of 26\\nGPT-2. Released in 2019, GPT-2 [62] increased its parameters to 1.5 billion. Its ability\\nto generate entire articles, answer questions, and even write poetry garnered significant\\nattention from the research community and the public alike.\\nGPT-3. GPT-3 [63] has 175 billion parameters. Its capabilities extend beyond mere text\\ngeneration; it can translate languages, write essays, create poetry, and even generate code.\\nGPT-4. The most recent model from OpenAI, GPT-4 [ 64], consists a staggering\\n1.76 trillion parameters, positioning it among the most advanced language models currently\\navailable. Leveraging advanced deep learning methodologies, it surpasses the capabilities\\nof its forerunner, GPT-3. Remarkably, GPT-4 can handle up to 25,000 words simultaneously,\\na capacity 8-fold greater than GPT-3. Furthermore, GPT-4 is versatile in accepting both text\\nand image prompts, allowing users to define tasks across vision and language domains. A\\nnotable improvement in GPT-4 is its reduced propensity for hallucinations compared to\\nearlier versions.\\n2.5.2. Other Notable Generative Transformer Models\\nBeyond the GPT series, the landscape of generative transformers is rich and diverse,\\nwith several models making significant contributions to the field.\\nBERT (Bidirectional Encoder Representations from Transformers).Developed by Google,\\nBERT [31] revolutionized the way we approach natural language understanding tasks.\\nUnlike GPT, which is generative, BERT is discriminative, designed to predict missing\\nwords in a sentence. Its bidirectional nature allows it to capture context from both the left\\nand the right of a word, leading to superior performance in tasks like question-answering\\nand sentiment analysis.\\nLLaMA. LLaMA [65] is an auto-regressive language model built on the transformer\\narchitecture, introduced by Meta. In February 2023, Meta unveiled the initial version\\nof LLaMA, boasting 65 billion parameters and adept at numerous generative AI func-\\ntions. By July 2023, LLaMA 2 was launched with 3 distinct model sizes: 7, 13, and\\n70 billion parameters.\\nLaMDA. LaMDA [66] is a specialized family of transformer-based neural language\\nmodels for dialog applications developed by Google in 2022. With up to 137 billion pa-\\nrameters and pre-training on 1.56 trillion words of public dialog and web text, LaMDA\\naims to address two key challenges: safety and factual grounding. The model incorporates\\nfine-tuning and external knowledge consultation to improve its safety metrics, ensuring\\nresponses align with human values and avoid harmful or biased suggestions. For factual\\ngrounding, LaMDA employs external knowledge sources like information retrieval sys-\\ntems and calculators to generate responses that are not just plausible but also factually\\naccurate. The model shows promise in various domains, including education and content\\nrecommendations, offering a balanced blend of quality, safety, and factual integrity.\\n3. T utorial on Generative Transformers\\nIn this section, we delve into a hands-on tutorial on generative transformers, guiding\\nreaders through the foundational concepts and practical implementations. By the end of\\nthis tutorial, readers should have a clear understanding of the transformer architecture and\\nbe equipped to build their own generative transformer models.\\n3.1. Basics of the Transformer Architecture\\nThe transformer architecture, introduced by Vaswani et al. in their seminal paper\\n“Attention Is All You Need” [29], has become the backbone of many state-of-the-art models\\nin natural language processing. We will now break down its core components.\\n3.1.1. Overview\\nAs depicted in Figure 3, the transformer consists of an encoder and a decoder. The\\nencoder processes the input sequence, and the decoder generates the output sequence.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 12}, page_content='Sci 2023, 5, 46 13 of 26\\nBoth the encoder and decoder are composed of multiple layers of attention mechanisms\\nand feed-forward neural networks.\\nEncoder DecoderFeatures\\nInput Output\\nFigure 3. Expanded schematic representation of the transformer architecture with a smaller\\nFeatures block.\\n3.1.2. Attention Mechanism\\nAs previously discussed, the attention mechanism allows the model to focus on\\ndifferent parts of the input sequence when producing an output. The mechanism computes\\nattention scores based on queries, keys, and values.\\nMathematical Representation:\\nGiven a query q, key k, and value v, the attention output is computed as:\\nAttention(q, k, v) =softmax\\n\\x12q · kT\\n√dk\\n\\x13\\nv (13)\\nwhere dk is the dimension of the key.\\nCode Snippet: The following Python code snippet demonstrates how to implement this\\nattention mechanism using PyTorch:\\nimport torch\\nimport torch.nn.functional as F\\ndef scaled_dot_product_attention(q, k, v):\\nmatmul_qk = torch.matmul(q, k.transpose(-2, -1))\\nd_k = q.size(-1) ** 0.5\\nscaled_attention_logits = matmul_qk / d_k\\nattention_weights = F.softmax(scaled_attention_logits, dim=-1)\\noutput = torch.matmul(attention_weights, v)\\nreturn output, attention_weights\\nIn this code snippet, q, k, and v are the query, key, and value tensors, respectively. The\\nfunction scaled_dot_product_attention computes the attention output according to\\nEquation (13).\\n3.1.3. Multi-Head Attention\\nInstead of using a single set of attention weights, the transformer uses multiple sets,\\nallowing it to focus on different parts of the input simultaneously. This is known as\\nmulti-head attention.\\nCode Snippet:\\nclass MultiHeadAttention(nn.Module):\\ndef __init__(self, d_model, num_heads):\\nsuper(MultiHeadAttention, self).__init__()\\nself.num_heads = num_heads\\n# Dimension of the model\\nself.d_model = d_model\\n# Depth of each attention head\\nself.depth = d_model\\n# Linear layer for creating query, key and value matrix\\nself.wq = nn.Linear(d_model, d_model)'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 13}, page_content='Sci 2023, 5, 46 14 of 26\\nself.wk = nn.Linear(d_model, d_model)\\nself.wv = nn.Linear(d_model, d_model)\\n# Final linear layer to produce the output\\nself.dense = nn.Linear(d_model, d_model)\\n3.1.4. Feed-Forward Neural Networks\\nEach transformer layer contains a feed-forward neural network, applied independently\\nto each position.\\nCode Snippet:\\nclass PointWiseFeedForwardNetwork(nn.Module):\\ndef __init__(self, d_model, dff):\\nsuper(PointWiseFeedForwardNetwork, self).__init__()\\nself.fc1 = nn.Linear(d_model, dff)\\nself.fc2 = nn.Linear(dff, d_model)\\n...\\nEach method and its body are indented with a tab or four spaces, which is the standard\\nPython indentation. This makes the code easier to read and understand.\\n3.1.5. Self-Attention Mechanism\\nThe self-attention mechanism is a variant of the attention mechanism where the input\\nsequence itself serves as the queries, keys, and values. This allows the transformer to weigh\\nthe significance of different parts of the input relative to a specific position, crucial for\\nunderstanding context and semantics.\\nMathematical Representation:\\nGiven an input sequence X, the queries Q, keys K, and values V are derived as:\\nQ = XWQ, K = XWK, V = XWV (14)\\nwhere WQ, WK, and WV are weight matrices. The self-attention output is then computed\\nusing the attention formula:\\nSelfAttention(Q, K, V) =softmax\\n\\x12QKT\\n√dk\\n\\x13\\nV (15)\\n3.1.6. Positional Encoding\\nTransformers, by design, do not have a built-in notion of sequence order. To provide\\nthe model with positional information, we inject positional encodings to the input embed-\\ndings. These encodings are added to the embeddings to ensure the model can make use of\\nthe sequence’s order.\\nMathematical Representation:\\nThe positional encodings are computed using sine and cosine functions:\\nPE(pos,2i) = sin\\n\\x12 pos\\n100002i/dmodel\\n\\x13\\n(16)\\nPE(pos,2i+1) = cos\\n\\x12 pos\\n100002i/dmodel\\n\\x13\\n(17)\\nwhere pos is the position and i is the dimension.\\n3.1.7. Multi-Head Attention\\nMulti-head attention is an extension of the attention mechanism, allowing the model\\nto focus on different parts of the input simultaneously. By running multiple attention\\nmechanisms in parallel, the model can capture various types of relationships in the data.\\nMathematical Representation:'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 14}, page_content='Sci 2023, 5, 46 15 of 26\\nGiven queries Q, keys K, and valuesV, the multi-head attention output iscomputed as:\\nMultiHead(Q, K, V) =Concat(head1, . . . , headh)WO (18)\\nwhere each head is computed as:\\nheadi = Attention(QWQi, KWKi, VWVi) (19)\\nand WQi, WKi, WVi, and WO are weight matrices.\\nFigure 4 showcases the multi-head attention mechanism, where multiple attention\\nheads operate in parallel, and their outputs are concatenated and passed through a dense\\nlayer to produce the final output.\\nHead 1\\nHead 2\\nHead h\\nConcat Dense Layer\\nOutput\\nFigure 4. Schematic representation of multi-head attention.\\nUnderstanding the intricacies of the transformer architecture, from the self-attention\\nmechanism to multi-head attention, is crucial for harnessing its full potential. By delving\\ninto the mathematical foundations and practical implementations, one can build powerful\\nmodels capable of handling a wide range of tasks in natural language processing.\\n3.1.8. Encoder and Decoder Modules\\nThe Transformer architecture consists of an encoder and a decoder, each made up of\\nmultiple layers. Here, we’ll walk through the implementation of these modules.\\nEncoder Module. The encoder module consists of multiple encoder layers, each contain-\\ning multi-head attention and feed-forward neural networks.\\nCode Snippet:\\nimport torch.nn as nn\\nclass EncoderLayer(nn.Module):\\ndef __init__(self, d_model, num_heads):\\nsuper(EncoderLayer, self).__init__()\\nself.mha = MultiHeadAttention(d_model, num_heads)\\nself.ffn = PointWiseFeedForwardNetwork(d_model, dff)\\n# Layer normalization and dropout layers can be added here\\ndef forward(self, x):'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 15}, page_content='Sci 2023, 5, 46 16 of 26\\nattn_output = self.mha(x, x, x)\\nout1 = x + attn_output # Add & Norm\\nffn_output = self.ffn(out1)\\nout2 = out1 + ffn_output # Add & Norm\\nreturn out2\\nDecoder Module. The decoder module is similar to the encoder but has an additional\\nmulti-head attention layer to attend to the encoder’s output.\\nCode Snippet:\\nclass DecoderLayer(nn.Module):\\ndef __init__(self, d_model, num_heads):\\nsuper(DecoderLayer, self).__init__()\\nself.mha1 = MultiHeadAttention(d_model, num_heads)\\nself.mha2 = MultiHeadAttention(d_model, num_heads)\\nself.ffn = PointWiseFeedForwardNetwork(d_model, dff)\\n# Layer normalization and dropout layers can be added here\\ndef forward(self, x, enc_output):\\nattn1 = self.mha1(x, x, x)\\nout1 = x + attn1 # Add & Norm\\nattn2 = self.mha2(out1, enc_output, enc_output)\\nout2 = out1 + attn2 # Add & Norm\\nffn_output = self.ffn(out2)\\nout3 = out2 + ffn_output # Add & Norm\\nreturn out3\\nIn these code snippets, ‘MultiHeadAttention’ and ‘PointWiseFeedForwardNetwork’\\nare custom classes that you would define based on your specific needs for multi-head\\nattention and point-wise feed-forward networks, respectively.\\n3.2. Building a Simple Generative Transformer\\nBuilding a generative transformer from scratch involves several steps, from data\\npreprocessing to model training and text generation. In this section, we’ll walk through\\neach of these steps, providing a comprehensive guide to constructing your own\\ngenerative transformer.\\n3.2.1. Data Preprocessing and Tokenization\\nBefore feeding data into the model, it is essential to preprocess and tokenize it. To-\\nkenization involves converting raw text into a sequence of tokens, which can be words,\\nsubwords, or characters.\\nUsing popular libraries like the HuggingFace’s ‘transformers‘, tokenization can be\\nachieved as:\\nfrom transformers import GPT2Tokenizer\\ntokenizer = GPT2Tokenizer.from_pretrained(’gpt2-medium’)\\ntokens = tokenizer.encode(\"Hello, world!\")\\n3.2.2. Defining the Transformer Model\\nAssuming one has already defined the EncoderLayer and DecoderLayer classes, one\\ncan define the complete Transformer model as follows:'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 16}, page_content='Sci 2023, 5, 46 17 of 26\\nclass Transformer(nn.Module):\\ndef __init__(self, d_model, num_heads, num_layers):\\nsuper(Transformer, self).__init__()\\nself.encoder = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\\nself.decoder = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\\ndef forward(self, src, tgt):\\nenc_output = src\\nfor layer in self.encoder:\\nenc_output = layer(enc_output)\\ndec_output = tgt\\nfor layer in self.decoder:\\ndec_output = layer(dec_output, enc_output)\\nreturn dec_output\\nBuilding a generative transformer, while complex, is made accessible with modern\\nlibraries and tools. By understanding the steps involved, from data preprocessing to model\\ntraining and generation, one can harness the power of transformers for a wide range\\nof applications.\\n3.3. Advanced Techniques and Best Practices\\nWhile the foundational concepts and basic implementations provide a solid starting\\npoint, mastering generative transformers requires a deeper understanding of advanced\\ntechniques and best practices. This section offers insights into improving generation\\nquality, handling long sequences, memory issues, and leveraging fine-tuning and transfer\\nlearning [67].\\n3.3.1. Techniques for Improving Generation Quality\\nAchieving high-quality text generation necessitates a combination of model architec-\\nture tweaks, training strategies, and post-processing methods.\\nTemperature Sampling. By adjusting the temperature during sampling, one can control\\nthe randomness of the generated text [68]. A lower temperature makes the output more\\ndeterministic, while a higher value introduces randomness.\\npi = e\\nzi\\nT\\n∑j e\\nzj\\nT\\n(20)\\nwhere pi is the adjusted probability, zi is the original probability, and T is the temperature.\\nTop-k and Top-p Sampling. Instead of sampling from the entire distribution, one can\\nrestrict the sampling pool to the top-k tokens or those tokens that have a cumulative\\nprobability greater than a threshold p [69].\\nGradient Clipping. To prevent exploding gradients during training, gradient clipping\\ncan be employed, ensuring that the gradients remain within a defined range [70]. Gradient\\nclipping can be implemented in PyTorch as follows:\\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\\n3.3.2. Handling Long Sequences and Memory Issues\\nTransformers, by design, have quadratic complexity with respect to sequence length.\\nThis can lead to memory issues for long sequences.\\nGradient Accumulation. Instead of updating the model weights after every batch,\\ngradients can be accumulated over multiple batches, effectively simulating a larger batch\\nsize without the memory overhead [71].\\nModel Parallelism. For models with billions of parameters, distributing the model\\nacross multiple GPUs can alleviate memory constraints [72].'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 17}, page_content='Sci 2023, 5, 46 18 of 26\\nGradient Checkpointing. This technique involves storing intermediate activations during\\nthe forward pass and recomputing them during the backward pass, reducing memory\\nusage at the cost of increased computation.\\n3.3.3. Fine-Tuning and Transfer Learning\\nTransfer learning, the practice of leveraging pre-trained models on new tasks, has\\nproven highly effective in the NLP domain.\\nFine-tuning. Once a model is pre-trained on a large corpus, it can be fine-tuned on\\na smaller, task-specific dataset. This approach often yields superior results compared to\\ntraining from scratch [73,74].\\nAdapters. Instead of fine-tuning the entire model, adapters allow for training only\\na small portion of the model, introducing task-specific parameters without altering the\\npre-trained weights [75].\\nMastering generative transformers goes beyond understanding the basics. By incorpo-\\nrating advanced techniques and best practices, one can achieve state-of-the-art performance,\\nhandle large models and sequences efficiently, and adapt pre-trained models to new tasks\\nwith ease. As the field of NLP continues to evolve, staying abreast of these practices ensures\\nrobust and high-quality model deployments.\\n4. Applications and Use Cases\\nGenerative transformers, with their unparalleled capability to understand and gener-\\nate human-like text, have found applications across a myriad of domains [40]. This section\\nprovides an in-depth exploration of some of the most prominent applications, shedding\\nlight on the transformative impact of these models on various industries.\\n4.1. Text Generation for Creative Writing\\nThe realm of creative writing, traditionally seen as the bastion of human creativity, has\\nwitnessed significant advancements with the advent of generative transformers [76]. These\\nmodels, trained on vast corpora of literature, can produce text that mirrors the style, tone,\\nand complexity of human authors.\\nNovel and Short Story Generation. AI-powered applications based on GPT-3 and other\\nlarge language models have been employed to generate entire novels or assist authors by\\nsuggesting plot twists, character developments, and dialogues [77]. The generated content,\\nwhile sometimes requiring human oversight, exhibits creativity and coherence.\\nPoetry and Song Lyrics. The nuanced and abstract nature of poetry and song lyrics\\nposes a significant challenge for traditional models. However, the advent of generative\\ntransformers has enabled these models to produce verses that resonate with human emo-\\ntions and experiences. A recent study demonstrated that AI-generated poems were often\\nindistinguishable from those written by humans [ 78], showcasing the success of these\\nalgorithms in replicating human-like poetic expressions.\\n4.2. Chatbots and Conversational Agents\\nThe rise of digital communication has spurred the demand for intelligent chatbots and\\nconversational agents. Generative transformers, with their ability to generate contextually\\nrelevant and coherent responses, stand at the forefront of this revolution. One of the most\\nprominent examples of a conversational agent built on generative transformer architecture\\nis ChatGPT, developed by OpenAI. ChatGPT reached 100 million monthly active users just\\n2 months after launching, making it the fastest-growing application in history.\\nCustomer Support. Businesses employ transformer-based chatbots to handle customer\\nqueries, complaints, and feedback [ 79,80]. These chatbots can understand the context,\\nprovide accurate information, and even escalate issues when necessary.\\nPersonal Assistants. Digital personal assistants, such as Siri and Alexa, are integrating\\ntransformer models to enhance their conversational capabilities, making interactions more\\nnatural and context-aware.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 18}, page_content='Sci 2023, 5, 46 19 of 26\\n4.3. Code Generation and Programming Assistance\\nSoftware development is undergoing a significant transformation with the introduc-\\ntion of transformer models capable of understanding and generating code. One such model\\nthat transforms natural language instructions to code is the Codex model developed by\\nOpenAI [81]. These models assist developers by suggesting code snippets, detecting bugs,\\nand even generating entire functions or modules.\\nCode Completion.Integrated Development Environments (IDEs) are incorporating trans-\\nformers to provide real-time code completion suggestions, enhancing developer productivity .\\nBug Detection and Fixing. Transformers can be trained to detect anomalies in code and\\nsuggest potential fixes, reducing debugging time and ensuring more robust software.\\n4.4. Other Notable Applications\\nBeyond the aforementioned domains, generative transformers have found applications\\nin diverse areas:\\nTranslation. While traditional machine translation models have limitations, transform-\\ners can produce translations that consider the broader context, resulting in more accurate\\nand idiomatic outputs [34].\\nSummarization. Generative transformers can read lengthy articles or documents and\\nproduce concise summaries, retaining the core information and intent [35].\\nGaming. In the gaming industry, transformers are used to generate dialogues, plotlines,\\nand even assist in game design by suggesting scenarios or character backstories [82].\\nThe applications of generative transformers are vast and continually expanding. As\\nresearch progresses and models become more sophisticated, it is anticipated that their\\nintegration into various domains will become even more profound.\\n5. Challenges and Limitations\\nWhile generative transformers have showcased remarkable capabilities, they are not\\ndevoid of challenges and limitations. This section delves into some of the most pressing\\nconcerns surrounding these models, from interpretability issues to ethical dilemmas and\\ncomputational constraints.\\n5.1. Model Interpretability\\nDeep learning models, especially those with millions or billions of parameters such as\\ngenerative transformers, are often criticized for being “black boxes”. Understanding why a\\nmodel made a particular decision can be elusive [83].\\nAttention Maps. One approach to interpretability is visualizing attention maps [29,84].\\nThese maps show which parts of the input the model focused on when producing an\\noutput. Attention maps are generated by the attention mechanism that computes a set of\\nattention scores, which can be visualized as a heatmap.\\nAttention maps serve as a tool for interpreting transformer models in NLP by pro-\\nviding insights into various aspects of text processing. They help in analyzing the roles\\nof words in sentences, identifying key topics, evaluating text quality, and detecting errors\\nor biases. However, while attention maps provide insights, they do not offer a complete\\nunderstanding of the model’s decision-making process.\\nMathematical Analysis. Efforts are being made to develop mathematical tools and\\nframeworks to dissect the inner workings of transformers [85,86]. Yet, a comprehensive\\nunderstanding remains a research frontier.\\n5.2. Hallucination in Text Generation\\nGenerative transformers are sometimes susceptible to generating text that, while\\ncoherent and grammatically correct, is factually incorrect or nonsensical. This phenomenon\\nis commonly referred to as a hallucination. Ji et al. conducted a comprehensive survey of\\nthe issue of hallucination in natural language generation (NLG) [87].'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 19}, page_content='Sci 2023, 5, 46 20 of 26\\nThe causes of hallucination are multifaceted and can vary. They may include in-\\nadequate training data, which limits the model’s understanding of the subject matter.\\nOverfitting to the training set is another common issue, where the model learns the noise\\nin the data rather than the actual pattern. Additionally, high model complexity leading to\\nover-parameterization can also contribute to hallucination.\\nAddressing the issue of hallucination involves multiple strategies. One approach is\\nto fine-tune the model on a more specific dataset that is closely aligned with the task at\\nhand. Another strategy involves incorporating external knowledge bases that can fact-\\ncheck the generated text in real-time. Ensemble methods, which combine the outputs of\\nmultiple models, can also be used to validate the generated text and reduce the likelihood\\nof hallucination.\\nEfforts are underway to quantify the degree of hallucination in generated text. Al-\\nthough a standard measure has yet to be established, one simplistic way to quantify it is\\nthrough the Hallucination Score, defined as the ratio of the number of hallucinated tokens\\nto the total number of generated tokens, as shown in Equation (21).\\nHallucination Score = Number of hallucinated tokens\\nTotal number of generated tokens (21)\\n5.3. Ethical Considerations in Text Generation\\nGenerative transformers, with their ability to produce human-like text, raise several\\nethical concerns [88].\\nMisinformation and Fake News. There is potential for these models to generate mislead-\\ning or false information, which can be weaponized to spread misinformation.\\nBias and Fairness. Transformers, being trained on vast internet datasets, can inherit and\\nperpetuate biases present in the data [89]. Addressing this requires careful dataset curation\\nand post-hoc bias mitigation techniques.\\nBias = ∑n\\ni=1(Pmodel(xi) − Ptrue(xi))\\nn (22)\\nwhere Pmodel is the model’s prediction, Ptrue is the true distribution, and n is the number\\nof samples.\\n5.4. Computational Requirements and Environmental Impact\\nTraining a large language model demands significant computational resources. For ex-\\nample, the GPT-3 model, which has 175 billion parameters, would require3.14 × 1023 FLOPS\\nfor training, translating to 355 GPU-years and a cost of USD 4.6 million on a V100 GPU [90].\\nMemory is another bottleneck; the model’s 175 billion parameters would need 700 GB of\\nmemory, far exceeding the capacity of a single GPU. To manage these challenges, OpenAI\\nused model parallelism techniques and trained the models on a high-bandwidth cluster.\\nAs language models grow in size, model parallelism is becoming increasingly essential\\nfor research.\\nEnergy Consumption. The energy required to train state-of-the-art models can be equiv-\\nalent to the carbon footprint of multiple car lifetimes. This raises environmental concerns.\\nExclusivity. The computational demands mean that only well-funded organizations\\ncan train the most advanced models, leading to concerns about the democratization of AI.\\nWhile generative transformers offer immense potential, it is crucial to address their\\nchallenges and limitations. Balancing the pursuit of state-of-the-art performance with\\nethical, environmental, and computational considerations is paramount for the sustainable\\nand responsible advancement of the field.\\n6. The Future of Generative Transformers\\nGenerative transformers, evolving from early models such as the Recurrent Neural\\nNetworks (RNNs) to the sophisticated Generative Adversarial Networks (GANs) and now\\nthe powerful transformers, have revolutionized numerous domains. With advancements'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 20}, page_content='Sci 2023, 5, 46 21 of 26\\nin model architectures, training techniques, and hardware capabilities, we can anticipate\\nmodels that not only understand and generate human-like text but also exhibit enhanced\\ncreativity, reasoning, and a form of artificial consciousness.\\nThe way forward is full of opportunities for exploration and innovation. As the field\\nof generative transformers continues to evolve, there are numerous avenues for research\\nand development that remain unexplored or underexplored. The evolution from rules-\\nbased systems to advanced LLMs has dramatically improved performance and training\\nefficiency. These improvements are not confined to text and language processing but extend\\nto computer vision and other modalities, creating avenues for interdisciplinary research.\\n6.1. Multimodal Models\\nThe future sees generative models that seamlessly integrate multiple modalities—text,\\nimage, sound, video, and more—offering a holistic understanding of the world and gener-\\nating content that overcomes the limitations of current models. Recent advancements have\\nalready led to transformers capable of generating not just text, but also image, audio, and\\nvideo [91]. These multimodal models are expected to evolve into sophisticated systems\\ncapable of processing and understanding inputs from various modalities simultaneously.\\nIn the future, we anticipate the emergence of single applications and more advanced\\nmultimodal models. These systems would not only understand inputs from different sen-\\nsory channels—such as visual, auditory, and textual—but also generate outputs in various\\nforms, moving well beyond mere text generation. The integration of these modalities in a\\nsingle model offers a more comprehensive approach to understanding complex real-world\\nscenarios and creating more nuanced and contextually relevant outputs.\\n6.2. Domain-Specific Models\\nThe development of domain-specific GPT models is becoming increasingly crucial\\nacross various applications [ 92]. While current large language models are adept at un-\\nderstanding natural language and generating content, their effectiveness and accuracy\\ncan vary significantly when applied to specialized domains such as medicine, law, and\\nfinance [93]. A big challenge in tailoring these models to a specific domain lies in the acqui-\\nsition of high-quality, domain-specific data. Another significant challenge is the fine-tuning\\nprocess, which involves adapting the model to the unique characteristics and vocabulary\\nof the domain.\\nDespite these obstacles, there has been progress in the development and implemen-\\ntation of domain-specific GPT models. The emergence of these models marks a future\\ntowards more tailored AI solutions. Companies with unique large datasets stand to gain\\ncompetitive advantages by training their own bespoke models. This trend is exemplified by\\nBloomberg’s development of a specialized LLM for financial tasks [94]. Other companies\\nsuch as Hugging Face and Databricks are also playing pivotal roles in providing the neces-\\nsary resources and platforms for developing and fine-tuning these customized models.\\nIn the future, we can expect these domain-specific GPT models to offer enhanced\\nefficiency, improved interpretability, and better domain generability compared to existing\\nlarge language models. However, the development of these models must also focus on\\noptimizing energy consumption and addressing the challenges of knowledge retention\\nduring the fine-tuning process.\\n6.3. Model Efficiency\\nThe growing size of models necessitates research in computational efficiency and\\nenergy consumption. This includes efforts to develop more sustainable AI infrastruc-\\nture and predictive infrastructure, essential for the data-intensive nature of enterprise\\nAI applications.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 21}, page_content='Sci 2023, 5, 46 22 of 26\\n6.4. Ethical AI\\nWith the widespread implementation of generative AI across various sectors, ensuring\\nethical use becomes paramount. This involves research into bias mitigation, fairness,\\ntransparency, and the development of guidelines for responsible AI usage [95], especially\\nas AI begins to automate complex tasks like legal work and medical fields like drug design\\nand medical diagnosis.\\n6.5. Interdisciplinary Integration\\nThe future of generative AI involves its fusion with other fields such as neuroscience\\nand cognitive science. This integration could lead to breakthroughs in understanding\\nboth artificial and natural intelligence, with generative AI applications expanding beyond\\ntechnical fields to impact popular culture and everyday life, such as in the creation of\\nhigh-resolution images and user-friendly AI applications for enhancing productivity.\\n7. Conclusions\\nAs we reflect upon the evolution of generative transformers, from their foundational\\nroots with Alan Turing to their current state-of-the-art capabilities, it becomes clear that\\nwe are at a turning point in the development of artificial intelligence. In the words of Alan\\nTuring, “We can only see a short distance ahead, but we can see plenty there that needs to\\nbe done”.\\nAs we reflect upon the evolution of generative transformers, from their foundational\\nroots with Alan Turing to their current state-of-the-art capabilities, it becomes clear that\\nwe are at a turning point in the development of artificial intelligence. In the words of Alan\\nTuring, “We can only see a short distance ahead, but we can see plenty there that needs\\nto be done”. This foresight aptly describes the current state of AI. The advancements in\\ngenerative transformers have not only redefined what machines are capable of doing but\\nalso opened up a myriad of possibilities for future exploration and innovation. As we\\nadvance and develop new technologies, it is crucial to navigate the ethical implications,\\nenvironmental and societal impacts of these technologies. The goal is not just to push\\nthe boundaries of what AI can achieve but to do so responsibly, ensuring that these\\nadvancements benefit society at large.\\nAuthor Contributions: Conceptualization, investigation, methodology , formal analysis,writing—original\\ndraft: E.Y.Z. and A.D.C.; Supervision: Z.P . and J.C.; Writing—review & editing: E.Y.Z., A.D.C., Z.P .,\\nJ.C. and Y.Y. All authors have read and agreed to the published version of the manuscript.\\nFunding: This research was funded by Research on quality Assurance and Evaluation of higher\\nEducation in Jiangsu Province under Grant No. 2023JSETKT032.\\nInstitutional Review Board Statement: Not applicable.\\nInformed Consent Statement: Not applicable.\\nData Availability Statement: Not applicable.\\nConflicts of Interest: The authors declare no conflict of interest. The funders had no role in the design\\nof the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or\\nin the decision to publish the results.\\nReferences\\n1. Baum, L.E.; Petrie, T. Statistical inference for probabilistic functions of finite state Markov chains. Ann. Math. Stat. 1966,\\n37, 1554–1563. [CrossRef]\\n2. Baum, L.E.; Eagon, J.A. An Inequality with Applications to Statistical Estimation for Probabilistic Functions of Markov Processes\\nand to a Model for Ecology. 1967. Available online: https://community.ams.org/journals/bull/1967-73-03/S0002-9904-1967-11\\n751-8/S0002-9904-1967-11751-8.pdf (accessed on 10 November 2023).\\n3. Baum, L.E.; Petrie, T.; Soules, G.; Weiss, N. A maximization technique occurring in the statistical analysis of probabilistic functions\\nof Markov chains. Ann. Math. Stat. 1970, 41, 164–171. [CrossRef]'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 22}, page_content='Sci 2023, 5, 46 23 of 26\\n4. Rabiner, L.R. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE 1989, 77, 257–286.\\n[CrossRef]\\n5. Hopfield, J.J. Neural networks and physical systems with emergent collective computational abilities. Proc. Natl. Acad. Sci. USA\\n1982, 79, 2554–2558. [CrossRef] [PubMed]\\n6. Hochreiter, S.; Schmidhuber, J. Long short-term memory. Neural Comput. 1997, 9, 1735–1780. [CrossRef] [PubMed]\\n7. Kingma, D.P .; Welling, M. An introduction to variational autoencoders.Found. Trends Mach. Learn. 2019, 12, 307–392. [CrossRef]\\n8. Kingma, D.P .; Welling, M. Auto-encoding variational bayes. arXiv 2013, arXiv:1312.6114.\\n9. Creswell, A.; White, T.; Dumoulin, V .; Arulkumaran, K.; Sengupta, B.; Bharath, A.A. Generative adversarial networks: An\\noverview. IEEE Signal Process. Mag. 2018, 35, 53–65. [CrossRef]\\n10. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative adversarial\\nnets. Adv. Neural Inf. Process. Syst. 2014, 27.\\n11. Antoniou, A.; Storkey, A.; Edwards, H. Data augmentation generative adversarial networks. arXiv 2017, arXiv:1711.04340.\\n12. Shorten, C.; Khoshgoftaar, T.M. A survey on image data augmentation for deep learning. J. Big Data 2019, 6, 1–48. [CrossRef]\\n13. Deecke, L.; Vandermeulen, R.; Ruff, L.; Mandt, S.; Kloft, M. Image anomaly detection with generative adversarial networks. In\\nProceedings of the Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2018, Dublin,\\nIreland, 10–14 September 2018; Proceedings, Part I 18; Springer: Berlin/Heidelberg, Germany, 2019; pp. 3–17.\\n14. Yang, Q.; Yan, P .; Zhang, Y.; Yu, H.; Shi, Y.; Mou, X.; Kalra, M.K.; Zhang, Y.; Sun, L.; Wang, G. Low-dose CT image denoising using\\na generative adversarial network with Wasserstein distance and perceptual loss. IEEE Trans. Med. Imaging 2018, 37, 1348–1357.\\n[CrossRef] [PubMed]\\n15. Zhang, H.; Sindagi, V .; Patel, V .M. Image de-raining using a conditional generative adversarial network.IEEE Trans. Circuits Syst.\\nVideo Technol.2019, 30, 3943–3956. [CrossRef]\\n16. Oord, A.V .D.; Dieleman, S.; Zen, H.; Simonyan, K.; Vinyals, O.; Graves, A.; Kalchbrenner, N.; Senior, A.; Kavukcuoglu, K.\\nWavenet: A generative model for raw audio. arXiv 2016, arXiv:1609.03499.\\n17. Ramesh, A.; Pavlov, M.; Goh, G.; Gray, S.; Voss, C.; Radford, A.; Chen, M.; Sutskever, I. Zero-shot text-to-image generation. In\\nProceedings of the International Conference on Machine Learning, PMLR, Virtual, 18–24 July 2021; pp. 8821–8831.\\n18. Dhariwal, P .; Jun, H.; Payne, C.; Kim, J.W.; Radford, A.; Sutskever, I. Jukebox: A generative model for music. arXiv 2020,\\narXiv:2005.00341.\\n19. Cetinic, E.; She, J. Understanding and creating art with AI: Review and outlook. ACM Trans. Multimed. Comput. Commun. Appl.\\n(TOMM) 2022, 18, 1–22. [CrossRef]\\n20. Bian, Y.; Xie, X.Q. Generative chemistry: Drug discovery with deep learning generative models. J. Mol. Model. 2021, 27, 71.\\n[CrossRef]\\n21. Stephenson, N.; Shane, E.; Chase, J.; Rowland, J.; Ries, D.; Justice, N.; Zhang, J.; Chan, L.; Cao, R. Survey of machine learning\\ntechniques in drug discovery. Curr. Drug Metab. 2019, 20, 185–193. [CrossRef]\\n22. Martin, D.; Serrano, A.; Bergman, A.W.; Wetzstein, G.; Masia, B. Scangan360: A generative model of realistic scanpaths for\\n360 images. IEEE Trans. Vis. Comput. Graph. 2022, 28, 2003–2013. [CrossRef]\\n23. Achlioptas, P .; Diamanti, O.; Mitliagkas, I.; Guibas, L. Learning representations and generative models for 3D point clouds. In\\nProceedings of the International Conference on Machine Learning, PMLR, Stockholm, Sweden, 10–15 July 2018; pp. 40–49.\\n24. Khoo, E.T.; Lee, S.P .; Cheok, A.D.; Kodagoda, S.; Zhou, Y.; Toh, G.S. Age invaders: Social and physical inter-generational family\\nentertainment. In Proceedings of the CHI’06 Extended Abstracts on Human Factors in Computing Systems, Montreal, QU,\\nCanada, 22–27 April 2006; pp. 243–246.\\n25. Radford, A.; Metz, L.; Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks.\\narXiv 2015, arXiv:1511.06434.\\n26. Way, G.P .; Greene, C.S. Extracting a biologically relevant latent space from cancer transcriptomes with variational autoencoders.\\nIn Proceedings of the Pacific Symposium on Biocomputing 2018, Hawaii, HI, USA, 3–7 January 2018; World Scientific: Singapore,\\n2018; pp. 80–91.\\n27. Sirignano, J.; Cont, R. Universal features of price formation in financial markets: Perspectives from deep learning. Quant. Financ.\\n2019, 19, 1449–1459. [CrossRef]\\n28. Reichstein, M.; Camps-Valls, G.; Stevens, B.; Jung, M.; Denzler, J.; Carvalhais, N.; Prabhat, F. Deep learning and process\\nunderstanding for data-driven Earth system science. Nature 2019, 566, 195–204. [CrossRef]\\n29. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, Ł.; Polosukhin, I. Attention is all you need.\\nAdv. Neural Inf. Process. Syst. 2017, 30. Available online: https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243\\n547dee91fbd053c1c4a845aa-Paper.pdf (accessed on 10 November 2023).\\n30. Pascanu, R.; Mikolov, T.; Bengio, Y. On the difficulty of training recurrent neural networks. In Proceedings of the International\\nConference on Machine Learning, PMLR, Atlanta, GA, USA, 17–19 June 2013; pp. 1310–1318.\\n31. Devlin, J.; Chang, M.W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding.\\narXiv 2018, arXiv:1810.04805.\\n32. Rogers, A.; Kovaleva, O.; Rumshisky, A. A primer in BERTology: What we know about how BERT works. Trans. Assoc. Comput.\\nLinguist. 2021, 8, 842–866. [CrossRef]'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 23}, page_content='Sci 2023, 5, 46 24 of 26\\n33. Bubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P .; Lee, Y.T.; Li, Y.; Lundberg, S.; et al. Sparks of\\nartificial general intelligence: Early experiments with gpt-4. arXiv 2023, arXiv:2303.12712.\\n34. Jiao, W.; Wang, W.; Huang, J.T.; Wang, X.; Tu, Z. Is ChatGPT a good translator? A preliminary study.arXiv 2023, arXiv:2301.08745.\\n35. Gao, M.; Ruan, J.; Sun, R.; Yin, X.; Yang, S.; Wan, X. Human-like summarization evaluation with chatgpt. arXiv 2023,\\narXiv:2304.02554.\\n36. Dosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn, D.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.; Heigold, G.;\\nGelly, S.; et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv 2020, arXiv:2010.11929.\\n37. Raghu, M.; Unterthiner, T.; Kornblith, S.; Zhang, C.; Dosovitskiy, A. Do Vision Transformers See Like Convolutional Neural\\nNetworks? arXiv 2021, arXiv:2108.08810.\\n38. Paul, S.; Chen, P .Y. Vision transformers are robust learners. In Proceedings of the AAAI Conference on Artificial Intelligence,\\nWashington, DC, USA, 7–14 February 2022; Volume 36, pp. 2071–2081.\\n39. Nikoli´ c, G.S.; Dimitrijevi´ c, B.R.; Nikoli´ c, T.R.; Stojcev, M.K. A survey of three types of processing units: CPU, GPU and TPU.\\nIn Proceedings of the 2022 57th International Scientific Conference on Information, Communication and Energy Systems and\\nTechnologies (ICEST), Ohrid, Macedonia, 16–18 June 2022; pp. 1–6.\\n40. Gozalo-Brizuela, R.; Garrido-Merchan, E.C. ChatGPT is not all you need. A State of the Art Review of large Generative AI models.\\narXiv 2023, arXiv:2301.04655.\\n41. Lin, T.; Wang, Y.; Liu, X.; Qiu, X. A survey of transformers. arXiv 2022, arXiv:2106.04554.\\n42. Kalyan, K.S.; Rajasekharan, A.; Sangeetha, S. Ammus: A survey of transformer-based pretrained models in natural language\\nprocessing. arXiv 2021, arXiv:2108.05542.\\n43. Acheampong, F.A.; Nunoo-Mensah, H.; Chen, W. Transformer models for text-based emotion detection: A review of BERT-based\\napproaches. Artif. Intell. Rev. 2021, 54, 5789–5829. [CrossRef]\\n44. Han, K.; Wang, Y.; Chen, H.; Chen, X.; Guo, J.; Liu, Z.; Tang, Y.; Xiao, A.; Xu, C.; Xu, Y.; et al. A survey on vision transformer.\\nIEEE Trans. Pattern Anal. Mach. Intell. 2022, 45, 87–110. [CrossRef]\\n45. Khan, S.; Naseer, M.; Hayat, M.; Zamir, S.W.; Khan, F.S.; Shah, M. Transformers in vision: A survey.ACM Comput. Surv. (CSUR)\\n2022, 54, 1–41. [CrossRef]\\n46. Shamshad, F.; Khan, S.; Zamir, S.W.; Khan, M.H.; Hayat, M.; Khan, F.S.; Fu, H. Transformers in medical imaging: A survey.Med.\\nImage Anal. 2023, 88, 102802. [CrossRef]\\n47. Aleissaee, A.A.; Kumar, A.; Anwer, R.M.; Khan, S.; Cholakkal, H.; Xia, G.S.; Khan, F.S. Transformers in remote sensing: A survey.\\nRemote Sens. 2023, 15, 1860. [CrossRef]\\n48. Wen, Q.; Zhou, T.; Zhang, C.; Chen, W.; Ma, Z.; Yan, J.; Sun, L. Transformers in time series: A survey.arXiv 2022, arXiv:2202.07125.\\n49. Ahmed, S.; Nielsen, I.E.; Tripathi, A.; Siddiqui, S.; Ramachandran, R.P .; Rasool, G. Transformers in time-series analysis: A tutorial.\\nCircuits Syst. Signal Process. 2023, 42, 7433–7466. [CrossRef]\\n50. Turing, A.M. On computable numbers, with an application to the Entscheidungsproblem. J. Math 1936, 58, 5.\\n51. Copeland, B.J. The Church-Turing Thesis. 1997. Available online: https://plato.stanford.edu/ENTRIES/church-turing/ (accessed\\non 10 November 2023).\\n52. Bernays, P . Alonzo Church. An unsolvable problem of elementary number theory. Am. J. Math. 1936, 58, 345–363.\\n53. Hodges, A. Alan Turing: The Enigma: The Book That Inspired the Film “The Imitation Game”; Princeton University Press: Princeton,\\nNJ, USA, 2014.\\n54. Turing, A.M. Proposed Electronic Calculator; National Physical Laboratory: London, UK, 1946.\\n55. Machinery, C. Computing machinery and intelligence-AM Turing. Mind 1950, 59, 433.\\n56. Turing, A. Intelligent machinery (1948). In The Essential Turing; Copeland, B.J., Ed.; Oxford Academic: Oxford, UK, 2004;\\npp. 395–432.\\n57. Turing, A.M. The chemical basis of morphogenesis. Philos. Trans. R. Soc. London Ser. Biol. Sci. 1952, 237, 37–72.\\n58. Goodfellow, I.; Bengio, Y.; Courville, A. Deep Learning; MIT Press: Cambridge, MA, USA, 2016.\\n59. Bishop, C.M.; Nasrabadi, N.M. Pattern Recognition and Machine Learning; Springer: Berlin/Heidelberg, Germany, 2006; Volume 4.\\n60. Yu, Y.; Si, X.; Hu, C.; Zhang, J. A review of recurrent neural networks: LSTM cells and network architectures. Neural Comput.\\n2019, 31, 1235–1270. [CrossRef]\\n61. Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I. Improving Language Understanding by Generative Pre-Training. 2018.\\nAvailable online: https://www.mikecaptain.com/resources/pdf/GPT-1.pdf (accessed on 10 November 2023).\\n62. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language models are unsupervised multitask learners. OpenAI\\nBlog 2019, 1, 9.\\n63. Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.D.; Dhariwal, P .; Neelakantan, A.; Shyam, P .; Sastry, G.; Askell, A.; et al.\\nLanguage models are few-shot learners. Adv. Neural Inf. Process. Syst. 2020, 33, 1877–1901.\\n64. OpenAI. GPT-4 Technical Report. 2023. Available online: http://xxx.lanl.gov/abs/2303.08774 (accessed on 10 November 2023).\\n65. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al.\\nLlama: Open and efficient foundation language models. arXiv 2023, arXiv:2302.13971.\\n66. Thoppilan, R.; De Freitas, D.; Hall, J.; Shazeer, N.; Kulshreshtha, A.; Cheng, H.T.; Jin, A.; Bos, T.; Baker, L.; Du, Y.; et al. Lamda:\\nLanguage models for dialog applications. arXiv 2022, arXiv:2201.08239.\\n67. Zhuang, B.; Liu, J.; Pan, Z.; He, H.; Weng, Y.; Shen, C. A survey on efficient training of transformers. arXiv 2023, arXiv:2302.01107.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 24}, page_content='Sci 2023, 5, 46 25 of 26\\n68. Xu, F.F.; Alon, U.; Neubig, G.; Hellendoorn, V .J. A systematic evaluation of large language models of code. In Proceedings of the\\n6th ACM SIGPLAN International Symposium on Machine Programming, New York, NY, USA, 13 June 2022; pp. 1–10.\\n69. Hewitt, J.; Manning, C.D.; Liang, P . Truncation sampling as language model desmoothing. arXiv 2022, arXiv:2210.15191.\\n70. Zhang, J.; He, T.; Sra, S.; Jadbabaie, A. Why gradient clipping accelerates training: A theoretical justification for adaptivity. arXiv\\n2019, arXiv:1905.11881.\\n71. Lin, Y.; Han, S.; Mao, H.; Wang, Y.; Dally, W.J. Deep gradient compression: Reducing the communication bandwidth for\\ndistributed training. arXiv 2017, arXiv:1712.01887.\\n72. Shoeybi, M.; Patwary, M.; Puri, R.; LeGresley, P .; Casper, J.; Catanzaro, B. Megatron-lm: Training multi-billion parameter language\\nmodels using model parallelism. arXiv 2019, arXiv:1909.08053.\\n73. Ziegler, D.M.; Stiennon, N.; Wu, J.; Brown, T.B.; Radford, A.; Amodei, D.; Christiano, P .; Irving, G. Fine-tuning language models\\nfrom human preferences. arXiv 2019, arXiv:1909.08593.\\n74. Dodge, J.; Ilharco, G.; Schwartz, R.; Farhadi, A.; Hajishirzi, H.; Smith, N. Fine-tuning pretrained language models: Weight\\ninitializations, data orders, and early stopping. arXiv 2020, arXiv:2002.06305.\\n75. He, R.; Liu, L.; Ye, H.; Tan, Q.; Ding, B.; Cheng, L.; Low, J.W.; Bing, L.; Si, L. On the effectiveness of adapter-based tuning for\\npretrained language model adaptation. arXiv 2021, arXiv:2106.03164.\\n76. Shidiq, M. The use of artificial intelligence-based chat-gpt and its challenges for the world of education; from the viewpoint of\\nthe development of creative writing skills. In Proceedings of the International Conference on Education, Society and Humanity,\\nTaipei, Taiwan, 28–30 June 2023; Volume 1, pp. 353–357.\\n77. Ippolito, D.; Yuan, A.; Coenen, A.; Burnam, S. Creative writing with an ai-powered writing assistant: Perspectives from\\nprofessional writers. arXiv 2022, arXiv:2211.05030.\\n78. Köbis, N.; Mossink, L.D. Artificial intelligence versus Maya Angelou: Experimental evidence that people cannot differentiate\\nAI-generated from human-written poetry. Comput. Hum. Behav. 2021, 114, 106553. [CrossRef]\\n79. Hardalov, M.; Koychev, I.; Nakov, P . Towards automated customer support. InArtificial Intelligence: Methodology, Systems, and\\nApplications, Proceedings of the 18th International Conference, AIMSA 2018, Varna, Bulgaria, 12–14 September 2018; Proceedings 18;\\nSpringer: Berlin/Heidelberg, Germany, 2018; pp. 48–59.\\n80. Følstad, A.; Skjuve, M. Chatbots for customer service: User experience and motivation. In Proceedings of the 1st International\\nConference on Conversational User Interfaces, Dublin, Ireland, 22–23 August 2019; pp. 1–9.\\n81. Finnie-Ansley, J.; Denny, P .; Becker, B.A.; Luxton-Reilly, A.; Prather, J. The robots are coming: Exploring the implications of openai\\ncodex on introductory programming. In Proceedings of the 24th Australasian Computing Education Conference, Melbourne,\\nVIC, Australia, 14–18 February 2022; pp. 10–19.\\n82. Värtinen, S.; Hämäläinen, P .; Guckelsberger, C. Generating role-playing game quests with gpt language models. IEEE Trans.\\nGames 2022, 1–12. . [CrossRef]\\n83. Doshi-Velez, F.; Kim, B. Towards a rigorous science of interpretable machine learning. arXiv 2017, arXiv:1702.08608.\\n84. Xu, K.; Ba, J.; Kiros, R.; Cho, K.; Courville, A.; Salakhudinov, R.; Zemel, R.; Bengio, Y. Show, attend and tell: Neural image caption\\ngeneration with visual attention. In Proceedings of the International Conference on Machine Learning, PMLR, Lille, France, 6–11\\nJuly 2015; pp. 2048–2057.\\n85. Chefer, H.; Gur, S.; Wolf, L. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 20–25 June 2021; pp. 782–791.\\n86. Elhage, N.; Nanda, N.; Olsson, C.; Henighan, T.; Joseph, N.; Mann, B.; Askell, A.; Bai, Y.; Chen, A.; Conerly, T.; et al. A\\nmathematical framework for transformer circuits. Transform. Circuits Thread 2021, 1. Available online: https://transformer-\\ncircuits.pub/2021/framework/index.html (accessed on 10 November 2023).\\n87. Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y.; Ishii, E.; Bang, Y.J.; Madotto, A.; Fung, P . Survey of hallucination in natural\\nlanguage generation. ACM Comput. Surv. 2023, 55, 1–38. [CrossRef]\\n88. Ganguli, D.; Hernandez, D.; Lovitt, L.; Askell, A.; Bai, Y.; Chen, A.; Conerly, T.; Dassarma, N.; Drain, D.; Elhage, N.; et al.\\nPredictability and surprise in large generative models. In Proceedings of the 2022 ACM Conference on Fairness, Accountability,\\nand Transparency, Seoul, Republic of Korea, 21–24 June 2022; pp. 1747–1764.\\n89. Silva, A.; Tambwekar, P .; Gombolay, M. Towards a comprehensive understanding and accurate evaluation of societal biases\\nin pre-trained transformers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies, Online, 6–11 June 2021; pp. 2383–2389.\\n90. Li, C. OpenAI’s GPT-3 Language Model: A Technical Overview. Lambda Labs Blog 2020. Available online: https://lambdalabs.\\ncom/blog/demystifying-gpt-3 (accessed on 10 November 2023).\\n91. Xu, P .; Zhu, X.; Clifton, D.A. Multimodal learning with transformers: A survey.IEEE Trans. Pattern Anal. Mach. Intell. 2023, 45,\\n12113–12132. [CrossRef] [PubMed]\\n92. Pal, S.; Bhattacharya, M.; Lee, S.S.; Chakraborty, C. A Domain-Specific Next-Generation Large Language Model (LLM) or\\nChatGPT is Required for Biomedical Engineering and Research. Ann. Biomed. Eng. 2023, 1–4. . [CrossRef]\\n93. Wang, C.; Liu, X.; Yue, Y.; Tang, X.; Zhang, T.; Jiayang, C.; Yao, Y.; Gao, W.; Hu, X.; Qi, Z.; et al. Survey on factuality in large\\nlanguage models: Knowledge, retrieval and domain-specificity. arXiv 2023, arXiv:2310.07521.'),\n",
              " Document(metadata={'source': 'research_papers/From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf', 'page': 25}, page_content='Sci 2023, 5, 46 26 of 26\\n94. Wu, S.; Irsoy, O.; Lu, S.; Dabravolski, V .; Dredze, M.; Gehrmann, S.; Kambadur, P .; Rosenberg, D.; Mann, G. Bloomberggpt: A\\nlarge language model for finance. arXiv 2023, arXiv:2303.17564.\\n95. Floridi, L.; Cowls, J.; Beltrametti, M.; Chatila, R.; Chazerand, P .; Dignum, V .; Luetge, C.; Madelin, R.; Pagallo, U.; Rossi, F.; et al.\\nAn ethical framework for a good AI society: Opportunities, risks, principles, and recommendations. Ethics Gov. Policies Artif.\\nIntell. 2021, 144, 19–39.\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 0}, page_content='An Introduction to Transformers\\nRichard E. Turner\\nDepartment of Engineering, University of Cambridge, UK\\nMicrosoft Research, Cambridge, UK\\nret26@cam.ac.uk\\nAbstract. The transformer is a neural network component that can be used to learn useful represen-\\ntations of sequences or sets of data-points [Vaswani et al., 2017]. The transformer has driven recent\\nadvances in natural language processing [Devlin et al., 2019], computer vision [Dosovitskiy et al., 2021],\\nand spatio-temporal modelling [Bi et al., 2022]. There are many introductions to transformers, but most\\ndo not contain precise mathematical descriptions of the architecture and the intuitions behind the design\\nchoices are often also missing.1 Moreover, as research takes a winding path, the explanations for the\\ncomponents of the transformer can be idiosyncratic. In this note we aim for a mathematically precise,\\nintuitive, and clean description of the transformer architecture. We will not discuss training as this is\\nrather standard. We assume that the reader is familiar with fundamental topics in machine learning\\nincluding multi-layer perceptrons, linear transformations, softmax functions and basic probability.\\n1See Phuong and Hutter [2022] for an exception to this.\\narXiv:2304.10557v5  [cs.LG]  8 Feb 2024'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 1}, page_content='Figure 1: The input to a transformer isN vectors\\nx(0)\\nn which are eachD dimensional. These can\\nbe collected together into an arrayX(0).\\n1 Strictly speaking, the collection of tokens does\\nnot need to have an order and the transformer\\ncan handle them as a set (where order does not\\nmatter), rather than a sequence. See section 3.\\n2 Note that much of the literature uses the trans-\\nposednotationwherebythedatamatrixis N×D,\\nbut I want sequences to run across the page and\\nfeatures down it in the schematics (a convention\\nI use in other lecture notes).\\nFigure 2: Encoding an image: an example [Doso-\\nvitskiy et al., 2021]. An image is split into N\\npatches. Each patch is reshaped into a vector by\\nthe vec operator. This vector is acted upon by a\\nmatrix W which maps the patch to aD dimen-\\nsional vector x(0)\\nn . These vectors are collected\\ntogether into the input X(0). The matrix W\\ncan be learned with the rest of the transformer’s\\nparameters.\\n3 The idea of interleaving processing across the\\nsequence and across features is a common motif\\nof many machine learning architectures includ-\\ning graph neural networks (interleaves processing\\nacross nodes and across features), Fourier neu-\\nral operators (interleaves processing across space\\nand across features), and bottleneck blocks in\\nResNets (interleaves processing across pixels and\\nacross features).\\n1 Preliminaries\\nLet’s start by talking about the form of the data that is input into a transformer,\\nthe goal of the transformer, and the form of its output.\\n1.1 Input data format: sets or sequences of tokens\\nIn order to apply a transformer, data must be converted into a set or sequence1\\nof N tokens x(0)\\nn of dimension D (see figure 1). The tokens can be collected\\ninto a matrixX(0) which isD×N.2 To give two concrete examples\\n1. a passage of text can be broken up into a sequence of words or sub-words,\\nwith each word being represented by a single unique vector,\\n2. an image can be broken up into a set of patches and each patch can be\\nmapped into a vector.\\nThe embeddings can be fixed or they can be learned with the rest of the pa-\\nrameters of the model e.g. the vectors representing words can be optimised or\\na learned linear transform can be used to embed image patches (see figure 2).\\nA sequence of tokens is a generic representation to use as an input – many\\ndifferent types of data can be “tokenised” and transformers are then immediately\\napplicable rather than requiring a bespoke architectures for each modality as\\nwas previously the case (CNNs for images, RNNs for sequences, deepsets for\\nsets etc.). Moreover, this means that you don’t need bespoke handcrafted\\narchitectures for mixing data of different modalities — you can just throw them\\nall into a big set of tokens.\\n1.2 Goal: representations of sequences\\nThe transformer will ingest the input dataX(0) and return a representation of\\nthe sequence in terms of another matrixX(M) which is also of sizeD×N.\\nThe slicexn = X(M)\\n:,n will be a vector of features representing the sequence at\\nthe location of tokenn. These representations can be used for auto-regressive\\nprediction of the next (n+1)th token, global classification of the entire sequence\\n(by pooling across the whole representation), sequence-to-sequence or image-\\nto-image prediction problems, etc. HereM denotes the number of layers in the\\ntransformer.\\n2 The transformer block\\nThe representation of the input sequence will be produced by iteratively applying\\na transformer block\\nX(m) = transformer-block(X(m−1)).\\nTheblockitselfcomprisestwostages: oneoperatingacrossthesequenceandone\\noperating across the features. The first stage refines each feature independently\\naccording to relationships between tokens across the sequence e.g. how much\\na word in a sequence at positionn depends on previous words at positionn′,\\nor how much two different patches from an image are related to one another.\\nThis stage acts horizontally across rows ofX(m−1). The second stage refines\\nthe features representing each token. This stage acts vertically across a column\\nof X(m−1). By repeatedly applying the transformer block the representation at\\ntoken nand featuredcan be shaped by information at tokenn′and featured′.3\\n1'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 2}, page_content='4 Relationship to Convolutional Neural Net-\\nworks (CNNs). The attention mechanism can\\nrecover convolutional filtering as a special case\\ne.g. ifx(0)\\nn is a 1D regularly sampled time-series\\nand A(m)\\nn′,n = A(m)\\nn′−n then the attention mecha-\\nnism in eq. 1 becomes a convolution. Unlike nor-\\nmal CNNs, these filters have full temporal sup-\\nport. Later we will see that the filters themselves\\ndynamically depend on the input, another differ-\\nence from standard CNNs. We will also see a\\nsimilarity: transformers will use multiple atten-\\ntion maps in each layer in the same way that\\nCNNs use multiple filters (though typically trans-\\nformers have fewer attention maps than CNNs\\nhave channels).\\n5 The need for transformers to store and com-\\npute N ×N attention arrays can be a major com-\\nputational bottleneck, which makes processing of\\nlong sequences challenging.\\n6 When training transformers to perform auto-\\nregressive prediction, e.g. predicting the next\\nword in a sequence based on the previous ones, a\\nclever modification to the model can be used to\\naccelerate training and inference. This involves\\napplying the transformer to the whole sequence,\\nand using masking in the attention mechanism\\n(A(m) becomes an upper triangular matrix) to\\npreventfuturetokensaffectingtherepresentation\\nat earlier tokens. Causal predictions can then be\\nmade for the entire sequence in one forward pass\\nthrough the transformer. See section 4 for more\\ninformation.\\n7 We temporarily suppress the superscripts here\\nto ease the notation so A(m)\\nn,n′ becomes An,n′\\nand similarlyx(m)\\nn becomes xn.\\n2.1 Stage 1: self-attention across the sequence\\nThe output of the first stage of the transformer block is anotherD×N array,\\nY(m). The output is produced by aggregating information across the sequence\\nindependently for each feature using an operation calledattention.\\nAttention. Specifically, the output vector at locationn, denotedy(m)\\nn , is pro-\\nduced by a simple weighted average of the input features at locationn′ =\\n1 ...N , denotedx(m−1)\\nn′ , that is4\\ny(m)\\nn =\\nN∑\\nn′=1\\nx(m−1)\\nn′ A(m)\\nn′,n. (1)\\nHere the weighting is given by a so-calledattention matrix A(m)\\nn′,n which is of\\nsize5 N ×N and normalises over its columns∑N\\nn′=1 A(m)\\nn′,n = 1 . Intuitively\\nspeaking A(m)\\nn′,n will take a high value for locations in the sequencen′which are\\nof high relevance for locationn. For irrelevant locations, it will take the value\\n0. For example, all patches of a visual scene coming from a single object might\\nhave high corresponding attention values.\\nWe can compactly write the relationship as a matrix multiplication,\\nY(m) = X(m−1)A(m), (2)\\nand we illustrate it below in figure 3.6\\nFigure 3: The output of an element of the attention mechanism, Y(m)\\nd,n , is\\nproduced by the dot product of the input horizontally sliced through timeX(m)\\nd,:\\nwith a vertical slice from the attention matrixA(m)\\n:,n . Here the shading in the\\nattention matrix represent the elements with a high value in white and those\\nwith a low value, near to 0, in black.\\nSelf-attention. So far, so simple. But where does the attention matrix come\\nfrom? The neat idea in the first stage of the transformer is that the attention\\nmatrix is generated from the input sequence itself – so-calledself-attention.\\nA simple way of generating the attention matrix from the input would be to\\nmeasure the similarity between two locations by the dot product between the\\nfeatures at those two locations and then use a softmax function to handle the\\nnormalisation i.e.7\\nAn,n′ = exp(x⊤\\nnxn′ )∑N\\nn′′=1 exp(x⊤\\nn′′ xn′ )\\n.\\n2'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 3}, page_content='8 Often you will see attention parameterised as\\nAn,n′ = exp(x⊤\\nn U⊤Uxn′ /\\n√\\nD)\\n∑N\\nn′′=1 exp(x⊤\\nn′′ U⊤Uxn′ /\\n√\\nD)\\n.\\nDividing the exponents by the square-root of the\\ndimensionality of the projected vector helps nu-\\nmerical stability, but in this presentation we ab-\\nsorb this term intoU to improve clarity.\\n9 Some of this effect could be handled by the\\nnormalisation in the denominator, but asymmet-\\nric similarity allows more flexibility. However, I\\ndo not know of experimental evidence to support\\nusing Uq ̸= Uk.\\n10 Relationship to Recurrent Neural Net-\\nworks (RNNs). It is illuminating to compare\\nthe temporal processing in the transformer to\\nthat of RNNs which recursively update a hid-\\nden state feature representation ( x(1)\\nn ) based\\non the current observation (x(0)\\nn ) and the pre-\\nvious hidden state x(1)\\nn = f(x(1)\\nn−1; x(0)\\nn ) =\\nf(f(x(1)\\nn−2; x(0)\\nn−1); x(0)\\nn ). Here we’ve unrolled\\nthe RNN one step to show that observations\\nwhich are nearby to the hidden state (e.g.x(0)\\nn )\\nare treated differently from observations that\\nare further away (e.g.x(0)\\nn−1), as information is\\npropagated by recurrent application of the func-\\ntion f(·). In contrast, in the transformer, self-\\nattentiontreatsallobservationsatalltime-points\\nin an identical manner, no matter how far away\\nthey are. This is one reason why they find it\\nsimpler to learn long-range relationships.\\n11 If attention matrices are viewed as a data-\\ndriven version of filters in a CNN, then the need\\nfor more filters / channels is clear. Typical\\nchoices for the number of headsH is 8 or 16,\\nlowerthantypicalnumbersofchannelsinaCNN.\\n12 The computational cost of multi-head self-\\nattentionisusuallydominatedbythematrixmul-\\ntiplication involving the attention matrix and is\\ntherefore O(HDN 2).\\n13 The product of the matrices V (m)\\nh X(m−1)\\nis related to the so-calledvalues which are nor-\\nmally introduced in descriptions of self-attention\\nalong side queries and keys. In the usual presen-\\ntation, there is a redundancy between the linear\\ntransformusedtocomputethevaluesandthelin-\\near projection at the end of the multi-head self-\\nattention, so we have not explicitly introduced\\nthem here. The standard presentation can be re-\\ncovered by setting Vh to be a low-rank matrix\\nVh = UhUv,h where Uh is DxK and Uv,h is\\nKxD. Typically K is set toK = D/H so that\\nchanging the number of heads leads to models\\nwith similar numbers of parameters and compu-\\ntational demands.\\nHowever, this naïve approach entangles information about the similarity between\\nlocations in the sequence with the content of the sequence itself.\\nAn alternative is to perform the same operation on a linear transformation of\\nthe sequence,Uxn, so that8\\nAn,n′ = exp(x⊤\\nnU⊤Uxn′ )∑N\\nn′′=1 exp(x⊤\\nn′′ U⊤Uxn′ )\\nTypically,U will project to a lower dimensional space i.e.U isK×Ddimensional\\nwith K <D. In this way only some of the features in the input sequence need\\nbe used to compute the similarity, the others being projected out, thereby de-\\ncoupling the attention computation from the content. However, the numerator\\nin this construction is symmetric. This could be a disadvantage. For example,\\nwe might want the word ‘caulking iron’ to be strongly associated with the word\\n‘tool’ (as it is a type of tool), but have the word ‘tool’ more weakly associated\\nwith the word ‘caulking iron’ (because most of us rarely encounter it).9\\nFortunately, itissimpletogeneralisetheattentionmechanismabovetobeasym-\\nmetric by applying two different linear transformations to the original sequence,\\nAn,n′ = exp\\n(\\nx⊤\\nnU⊤\\nk Uqxn′\\n)\\n∑N\\nn′′=1 exp\\n(\\nx⊤\\nn′′ U⊤\\nk Uqxn′\\n). (3)\\nThe two quantities that are dot-producted together hereqn = Uqxn and kn =\\nUkxn are typically known as thequeries and thekeys, respectively.\\nTogether equations 2 and 3 define the self-attention mechanism. Notice that\\nthe K×D matrices Uq and Uk are the only parameters of this mechanism.10\\nMulti-head self-attention (MHSA). In the self-attention mechanisms de-\\nscribed above, there is one attention matrix which describes the similarity of\\ntwo locations within the sequence. This can act as a bottleneck in the architec-\\nture – it would be useful for pairs of points to be similar in some ‘dimensions’\\nand different in others.11\\nIn order to increase capacity of the first self-attention stage, the transformer\\nblock applies H sets of self-attention in parallel12 (termed H heads) and then\\nlinearly projects the results down to theD×N array required for further pro-\\ncessing. This slight generalisation is calledmulti-head self-attention.\\nY(m) = MHSAθ(X(m−1)) =\\nH∑\\nh=1\\nV(m)\\nh X(m−1)A(m)\\nh , where (4)\\n[A(m)\\nh ]n,n′ =\\nexp\\n((\\nk(m)\\nh,n\\n)⊤\\nq(m)\\nh,n′\\n)\\n∑N\\nn′′=1 exp\\n((\\nk(m)\\nh,n′′\\n)⊤\\nq(m)\\nh,n′\\n) (5)\\nq(m)\\nh,n = U(m)\\nq,h x(m−1)\\nn and k(m)\\nh,n = U(m)\\nk,h x(m−1)\\nn . (6)\\nHere theH matrices V(m)\\nh which areD×Dproject theH self-attention stages\\ndown to the required output dimensionalityD.13\\nThe addition of the matricesV(m)\\nh , and the fact that retaining just the diagonal\\nelements of the attention matrixA(m) will interact the signal instantaneously\\nwith itself, does mean there is some cross-feature processing in multi-head self-\\nattention, as opposed to it containing purely cross-sequence processing. How-\\never, the stage has limited capacity for this type of processing and it is the job\\nof the second stage to address this.\\n3'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 4}, page_content='14 The MLPs used typically have one or two\\nhidden-layers with dimension equal to the num-\\nber of featuresD (or larger). The computational\\ncostofthisstepisthereforeroughly N×D×D. If\\nthefeatureembeddingsizeapproachesthelength\\nof the sequenceD ≈N, the MLPs can start to\\ndominatethecomputationalcomplexity(e.g.this\\ncan be the case for vision transformers which em-\\nbed large patches).\\n15 Relationship to Graph Neural Networks\\n(GNNs). At a high level, graph neural networks\\ninterleave two steps. First, a message passing\\nstep where each node receives messages from its\\nneighbours which are then aggregated together.\\nSecond, a feature processing step where the in-\\ncoming aggregated messages are used to update\\neach node’s features. Through this lens, the\\ntransformer can be viewed as an unrolled GNN\\nwith each token corresponding to an edge of a\\nfully connected graph. MHSA forms the mes-\\nsage passing step, and the MLPs forming the\\nfeature update step. Each transformer block cor-\\nresponds to one update of the GNN. Moreover,\\nmany methods for scaling transformers introduce\\nsparse forms of attention where each token at-\\ntends to only a restricted set of other tokens,\\nthat is they specify a sparse graph connectivity\\nstructure. Arguably, in this way transformers are\\nmore general as they can use different graphs at\\ndifferent layers in the transformer.\\n16 This is also known as z-scoring in some fields\\nand is related to whitening.\\nFigure 4 shows multi-head self-attention schematically. Multi-head attention\\ncomprises the following parametersθ = {Uq,h,Uk,h,Vh}H\\nh=1 i.e. 3H matrices\\nof sizeK×D, K×D, andD×D respectively.\\nFigure 4:Multi-head self-attention appliesH self-\\nattention operations in parallel and then linearly\\nprojects theHD ×N dimensional output down to\\nD ×N by applying a linear transform, implemented\\nhere by theH matrices Vh.\\n2.2 Stage 2: multi-layer perceptron across features\\nThe second stage of processing in the transformer block operates across features,\\nrefining the representation using a non-linear transform. To do this, we simply\\napply a multi-layer perceptron (MLP) to the vector of features at each location\\nn in the sequence,\\nx(m)\\nn = MLPθ(y(m)\\nn ).\\nNotice that the parameters of the MLP,θ, are the same for each locationn.14\\n15\\n2.3 The transformer block: Putting it all together with residual con-\\nnections and layer normalisation\\nWe can now stack MHSA and MLP layers to produce the transformer block.\\nRather than doing this directly, we make use of two ubiquitous transformations\\nto produce a more stable model that trains more easily: residual connections\\nand normalisation.\\nResidual connections. The use of residual connections is widespread across\\nmachinelearningastheymakeinitialisationsimple, haveasensibleinductivebias\\ntowards simple functions, and stabilise learning [Szegedy et al., 2017]. Instead\\nof directly specifying a functionx(m) = fθ(x(m−1)), the idea is to parameterise\\nit in terms of an identity mapping and a residual term\\nx(m) = x(m−1) + resθ(x(m−1)).\\nEquivalently, this can be viewed as modelling the differences between the repre-\\nsentation x(m) −x(m−1) = resθ(x(m−1)) and will work well when the function\\nthat is being modelled is close to identity. This type of parameterisation is used\\nfor both the MHSA and MLP stages in the transformer, with the idea that each\\napplies a mild non-linear transformation to the representation. Over many layers,\\nthese mild non-linear transformations compose to form large transformations.\\nToken normalisation.The use of normalisation, such as LayerNorm and Batch-\\nNorm, is also widespread across the deep learning community as a means to\\nstabilise learning. There are many potential choices for how to compute nor-\\nmalisation statistics (see figure 5 for a discussion), but the standard approach\\nis use LayerNorm [Ba et al., 2016] which normalises each token separately, re-\\nmoving the mean and dividing by the standard deviation,16\\n¯xd,n = 1√\\nvar(xn)\\n(xd,n −mean(xn)) γd + βd = LayerNorm(X)d,n\\nwhere mean(xn) = 1\\nD\\n∑D\\nd=1 xd,n and var(xn) = 1\\nD\\n∑D\\nd=1(xd,n −mean(xn))2.\\nThe two parametersγd and βd are a learned scale and shift.\\n4'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 5}, page_content='batch\\nfeature\\nsequenced=D\\nd=1\\nn=Nn=1 batch\\nfeature\\nsequenced=D\\nd=1\\nn=Nn=1\\nLayerNorm for transformers(TokenNorm) BatchNorm for transformers\\nFigure 5: Transformers perform layer normali-\\nsation (left hand schematic) which normalises the\\nmean and standard deviation of each individual to-\\nken in each sequence in the batch. Batch normal-\\nisation (right hand schematic), which normalises\\nover the featureand batch dimension together, is\\nfound to be far less stable [Shen et al., 2020].\\nOther flavours of normalisation are possible and po-\\ntentially under-explored e.g. instance normalisation\\nwould normalise across the sequence dimension in-\\nstead.\\n17 Whilst it is possible to control the non-\\nlinearities and weights in neural networks to pre-\\nvent explosion of the representation, the con-\\nstraints this places on the activation functions\\ncan adversely affect learning. The LayerNorm\\napproach is arguably simpler and simpler to train.\\nbatch\\nfeature\\nimage height\\nand width\\nd=D\\nd=1\\nbatch\\nfeature\\nsequence\\nd=D\\nd=1\\nn=Nn=1\\nLayerNorm  for CNNs BatchNorm  for CNNs\\nFigure 6: In CNNs LayerNorm is conventionally\\napplied to both the featuresand across the fea-\\nture maps (i.e. across the height and width of the\\nimages) (left hand schematic). As the height and\\nwidth dimension in CNNs corresponds to the se-\\nquence dimension, 1 . . . Nof transformers, the\\nterm ’LayerNorm’ is arguably used inconsistently\\n(compare to figure 5). I would prefer to call the\\nnormalisation used in transformers ’token nor-\\nmalisation’ instead to avoid confusion. Batch\\nnormalisation (right hand schematic) is consis-\\ntently defined.\\n18 The exact configuration of the normalisation\\nand residual layers can differ, but here we show\\na standard setup [Xiong et al., 2020].\\nAs this transform normalises each token individually and as LayerNorm is ap-\\nplied differently in CNNs, see figure 6, I would prefer to call this normalisation\\nTokenNorm.\\nThis transform stops feature representations blowing up in magnitude as non-\\nlinearities are repeatedly applied through neural networks.17 In transformers,\\nLayerNorm is usually applied in the residual terms of both the MHSA and MLP\\nstages.\\nPuttingthisalltogether, wehavethestandardtransformerblockshownschemat-\\nically in figure 7.18\\nFigure 7: The transformer block. Residual connections are added to the multi-\\nhead self-attention (MHSA) stage and the multi-layer perceptron (MLP) stage.\\nLayer normalisation is also applied to the inputs of both the MHSA and the\\nMLP. They are then stacked. This block can then be repeatedM times.\\n3 Position encoding\\nThe transformer treats the data as a set — if you permute the columns ofX(0)\\n(i.e. re-order the tokens in the input sequence) you permute all the represen-\\ntations throughout the networkX(m) in the same way. This is key for many\\napplications since there may not be a natural way to order the original data into\\na sequence of tokens. For example, there is no single ‘correct’ order to map\\n5'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 6}, page_content='19 Vision transformers [Dosovitskiy et al., 2021]\\nuse x(0)\\nn = Wpn + en where pn is the nth\\nvectorised patch,en is the learned position em-\\nbedding, andW is the patch embedding matrix.\\nArguably it would be more intuitive to append\\nthe position embedding to the patch embedding.\\nHowever, if we use the concatenation approach\\nandconsiderwhathappensafterapplyingalinear\\ntransform,\\nV\\n[Wpn\\nen\\n]\\n=\\n[V11 V12\\nV21 V22\\n][Wpn\\nen\\n]\\n=\\n[V11Wpn + V12en\\nV21Wpn + V22en\\n]\\n= W′pn + e′\\nn\\nwe recover the additive construction, which is\\none hint as to why the additive construction\\nworks.\\n20 Note that I’m overloading the notation here:\\npreviously superscripts denoted layers in the\\ntransformer, but here I’m using them to denote\\nthe number of items in the input sequence.\\nimage patches into a one dimensional sequence.\\nHowever, this presents a problem since positional information is key in many\\nproblems and the transformer has thrown it out. The sequence ‘herbivores\\neat plants’ should not have the same representation (up to permutation) as\\n‘plants eat herbivores’. Nor should an image have the same representation as\\none comprising the same patches randomly permuted. Thankfully, there is a\\nsimple fix for this: the location of each token within the original dataset should\\nbe included in the token itself, or through the way it is processed. There are\\nseveral options how to do this, one is to include this information directly into the\\nembedding X(0). E.g.bysimplyaddingthepositionembedding(surprisinglythis\\nworks19) or concatenating. The position information can be fixed e.g. adding a\\nvector of sinusoids of different frequencies and phases to encode position of a\\nword in a sentence [Vaswani et al., 2017], or it can be a free parameter which is\\nlearned [Devlin et al., 2019], as it often done in image transformers. There are\\nalso approaches to include relative distance information between pairs of tokens\\nby modifying the self-attention mechanism [Wu et al., 2021] which connects to\\nequivariant transformers.\\n4 Application specific transformer variants\\nFor completeness we will give some simple examples for how the standard trans-\\nformer architecture above is used and modified for specific applications. This\\nincludes adding a head to the transformer blocks to carry out the desired pre-\\ndiction task, but also modifications to the standard construction of the body.\\n4.1 Auto-regressive language modelling\\nIn auto-regressive language modelling the goal is to predict the next wordwn\\nin the sequence given the previous wordsw1:n−1, that is to return p(wn =\\nw|w1:n−1). Two modifications are required to use the transformer for this task\\n— a change to the body to make the architecture efficient and the addition of\\na head to make the predictions for the next word.\\nModification to the body: auto-regressive masking.Applying the version\\nof the transformer we have covered so far to auto-regressive prediction is compu-\\ntationally expensive, both during training and testing. To see this, note that AR\\nprediction requires making a sequence of predictions: you start by predicting the\\nfirst wordp(w1 = w), then you predict the second given the firstp(w2 = w|w1),\\nthen the third word given the first twop(w2 = w|w1,w2), and so on until you\\npredict the last item in the sequencep(wN = w|w1:N−1). This requires apply-\\ning the transformerN −1 times with input sequences that grow by one word\\neach time: w1,w1:2,...,w 1:N−1. This is very costly at both training-time and\\ntest-time.\\nFortunately, there is a neat way around this by enabling the transformer to\\nsupport incremental updates whereby if you add a new token to an existing\\nsequence, you do not change the representation for the old tokens. To make this\\nproperty clear, I will define it mathematically: let the output of the incremental\\ntransformer applied to the firstn words be denoted20\\nX(n) = transformer-incremental(w1:n).\\nThen the output of the incremental transformer when applied ton+ 1 words is\\nX(n+1) = transformer-incremental(w1:n+1).\\nIn the incremental transformerX(n) = X(n+1)\\n1:D,1:n i.e. the representation of the\\nold tokens has not changed by adding the new one. If we have this property\\n6'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 7}, page_content='21 Notice that this masking operation also en-\\ncodes position information since you can infer\\nthe order of the tokens from the mask.\\n22 This restriction to the attention will cause a\\nloss of representational power. It’s an open ques-\\ntion as to how significant this is and whether in-\\ncreasing the capacity of the model can mitigate\\nit e.g. by using higher dimensional tokens, i.e. in-\\ncreasing D.\\nthen 1. at test-time auto-regressive generation can use incremental updates to\\ncompute the new representation efficiently, 2. at training time we can make\\nthe N auto-regressive predictions for the whole sequencep(w1 = w)p(w2 =\\nw|w1)p(w2 = w|w1,w2) ...p (wN = w|w1:N−1) in a single forwards pass.\\nUnfortunately, the standard transformer introduced above does not have this\\nproperty due to the form of the attention used. Every token attends to every\\nother token, so if we add a new token to the sequence then the representation\\nfor every token changes throughout the transformer. However, if we mask the\\nattention matrix so that it is upper-triangularAn,n′ = 0 when n > n′ then\\nthe representation of each wordonly depends on the previous words.21 This\\nthen gives us the incremental property as none of the other operations in the\\ntransformer operate across the sequence.22\\nAdding a head. We’re now almost set to perform auto-regressive language\\nmodelling. We apply the masked transformer blockM times to the input se-\\nquence of words. We then take the representation at tokenn−1, that isx(M)\\nn−1\\nwhich captures causal information in the sequence at this point, and generate\\nthe probability of the next word through a softmax operation\\np(wn = w|w1:n−1) = p(wn = w|x(M)\\nn−1) = exp(g⊤\\nwx(M)\\nn−1)\\n∑W\\nw=1 exp(g⊤wx(M)\\nn−1)\\n.\\nHere W is the vocabulary size, the wth word isw and {gw}W\\nw=1 are softmax\\nweights that will be learned.\\n4.2 Image classification\\nFor image classification the goal is to predict the labely given the input image\\nwhich has been tokenised into the sequenceX(0), that isp(y|X(0)). One way\\nof computing this distribution would be to apply the standard transformer body\\nM times to the tokenised image patches before aggregating the final layer of the\\ntransformer, X(M), across the sequence e.g. by spatial poolingh = ∑N\\nn=1 x(M)\\nn\\nin order to form a feature representation for the entire image. The representation\\nh could then be used to perform softmax classification. An alternative approach\\nis found to perform better [Dosovitskiy et al., 2021]. Instead we introduce a\\nnew fixed (learned) token at the startn = 0 of the input sequencex(0)\\n0 . At\\nthe head we use then= 0 vector, x(M)\\n0 , to perform the softmax classification.\\nThis approach has the advantage that the transformer maintains and refines a\\nglobal representation of the sequence at each layermof the transformer that is\\nappropriate for classification.\\n4.3 More complex uses\\nThe transformer block can also be used as part of more complicated systems\\ne.g. in encoder-decoder architectures for sequence-to-sequence modelling for\\ntranslation [Devlin et al., 2019, Vaswani et al., 2017] or in masked auto-encoders\\nfor self-supervised vision systems [He et al., 2021].\\n5 Conclusion\\nThis concludes this basic introduction to transformers which aspired to be math-\\nematically precise and to provide intuitions behind the design decisions.\\nWe have not talked about loss functions or training in any detail, but this is\\nbecause rather standard deep learning approaches are used for these. Briefly,\\n7'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 8}, page_content='transformers are typically trained using the Adam optimiser. They are often\\nslow to train compared to other architectures and typically get more unstable\\nas training progresses. Gradient clipping, decaying learning rate schedules, and\\nincreasing batch sizes through training help to mitigate these instabilities, but\\noften they still persist.\\nAcknowledgements. We thank Dr. Max Patacchiola, Sasha Shysheya, John\\nBronskill, Runa Eschenhagen and Jess Riedel for feedback on previous versions\\nof this note. Richard E. Turner is supported by Microsoft, Google, Amazon,\\nARM, Improbable and EPSRC grant EP/T005386/1.\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.\\narXiv preprint arXiv:1607.06450, 2016.\\nKaifeng Bi, Lingxi Xie, Hengheng Zhang, Xin Chen, Xiaotao Gu, and Qi Tian.\\nPangu-weather: A 3d high-resolution model for fast and accurate global\\nweather forecast, 2022.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT:\\nPre-training of deep bidirectional transformers for language understanding.\\nIn Proceedings of the 2019 Conference of the North American Chapter\\nof the Association for Computational Linguistics: Human Language Tech-\\nnologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapo-\\nlis, Minnesota, June 2019. Association for Computational Linguistics. doi:\\n10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\\nage is worth 16x16 words: Transformers for image recognition at scale. In\\n9th International Conference on Learning Representations, ICLR 2021, Vir-\\ntual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https:\\n//openreview.net/forum?id=YicbFdNTTy.\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Gir-\\nshick. Masked autoencoders are scalable vision learners, 2021.\\nMary Phuong and Marcus Hutter. Formal algorithms for transformers.arXiv\\npreprint arXiv:2207.09238, 2022.\\nSheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer.\\nPowerNorm: Rethinking batch normalization in transformers. In Hal Daumé\\nIII and Aarti Singh, editors, Proceedings of the 37th International Confer-\\nence on Machine Learning, volume 119 of Proceedings of Machine Learn-\\ning Research, pages 8741–8751. PMLR, 13–18 Jul 2020. URL https:\\n//proceedings.mlr.press/v119/shen20e.html.\\nChristian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi.\\nInception-v4, inception-resnetandtheimpactofresidualconnectionsonlearn-\\ning. In Proceedings of the AAAI conference on artificial intelligence, vol-\\nume 31, 2017.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all\\nyou need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach,\\n8'),\n",
              " Document(metadata={'source': 'research_papers/2304.10557v5.pdf', 'page': 9}, page_content='R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neu-\\nral Information Processing Systems, volume 30. Curran Associates, Inc.,\\n2017. URL https://proceedings.neurips.cc/paper_files/paper/\\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\\nK. Wu, H. Peng, M. Chen, J. Fu, and H. Chao. Rethinking and improv-\\ning relative position encoding for vision transformer. In 2021 IEEE/CVF\\nInternational Conference on Computer Vision (ICCV), pages 10013–10021,\\nLos Alamitos, CA, USA, oct 2021. IEEE Computer Society. doi:10.1109/\\nICCV48922.2021.00988. URL https://doi.ieeecomputersociety.org/\\n10.1109/ICCV48922.2021.00988.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,\\nHuishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normal-\\nization in the transformer architecture. In Hal Daumé III and Aarti Singh,\\neditors, Proceedings of the 37th International Conference on Machine Learn-\\ning, volume 119 ofProceedings of Machine Learning Research, pages 10524–\\n10533. PMLR, 13–18 Jul 2020. URLhttps://proceedings.mlr.press/\\nv119/xiong20b.html.\\n9'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 0}, page_content='TRANSFORMER MODELS : AN INTRODUCTION AND CATALOG\\nXavier Amatriain\\nLos Gatos, CA 95032\\nxavier@amatriain.net\\nFebruary 16, 2023\\nABSTRACT\\nI have a terrible memory for names. In the past few years we have seen the meteoric appearance of\\ndozens of models of the Transformer family, all of which have funny, but not self-explanatory, names.\\nThe goal of this post is to offer a short and simple catalog and classiﬁcation of the most popular\\nTransformer models. In other words, I needed a Transformers cheat-sheet and could not ﬁnd a good\\nenough one online, so I thought I’d write my own. I hope it can be useful to you too.\\nContents\\n1 Introduction: What are Transformers 3\\n1.1 Encoder/Decoder architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n1.3 What are Transformers used for and why are they so popular . . . . . . . . . . . . . . . . . . . . . . 5\\n1.4 RLHF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n1.5 Diffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2 The Transformers catalog 8\\n2.1 Features of a Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.1.1 Pretraining Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.1.2 Pretraining Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n2.1.3 Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.2 Catalog table . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3 Family Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.4 Chronological timeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.5 Catalog List . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.5.1 ALBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.5.2 AlphaFold . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.5.3 Anthropic Assistant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n2.5.4 BART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5.5 BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\narXiv:2302.07730v1  [cs.CL]  12 Feb 2023'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 1}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.6 Big Bird . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2.5.7 BlenderBot3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.5.8 BLOOM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.5.9 ChatGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n2.5.10 Chinchilla . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.5.11 CLIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.5.12 CM3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n2.5.13 CTRL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.5.14 DALL-E . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.5.15 DALL-E 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n2.5.16 Decision Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.5.17 DialoGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.5.18 DistilBERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n2.5.19 DQ-BART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.5.20 ELECTRA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.5.21 ERNIE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n2.5.22 Flamingo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5.23 Gato . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5.24 GLaM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n2.5.25 GLIDE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.5.26 Global Context ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.5.27 Gopher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n2.5.28 GopherCite . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n2.5.29 GPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n2.5.30 GPT-2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n2.5.31 GPT-3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.5.32 GPT-3.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.5.33 InstructGPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n2.5.34 GPT-Neo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.5.35 GPT-NeoX-20B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.5.36 HTML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.5.37 Imagen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n2.5.38 Jurassic-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.5.39 LAMDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.5.40 mBART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\\n2.5.41 Megatron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2.5.42 Minerva . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2.5.43 MT-NLG (Megatron TouringNLG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\n2.5.44 OPT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n2'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 2}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.45 PalM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n2.5.46 Pegasus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\n2.5.47 RoBERTa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.5.48 SeeKer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.5.49 Sparrow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.5.50 StableDiffusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n2.5.51 Swin Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n2.5.52 Switch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n2.5.53 T5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n2.5.54 Trajectory Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n2.5.55 Transformer XL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n2.5.56 Turing-NLG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.5.57 ViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.5.58 Wu Dao 2.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n2.5.59 XLM-RoBERTa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n2.5.60 XLNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\\n3 Further reading 31\\n1 Introduction: What are Transformers\\nTransformers are a class of deep learning models that are deﬁned by some architectural traits. They were ﬁrst introduced\\nin the now famous \"Attention is All you Need\" paper by Google researchers in 2017 [1] (the paper has accumulated a\\nwhooping 38k citations in only 5 years) and associated blog post1.\\nThe Transformer architecture is a speciﬁc instance of the encoder-decoder models[2]2 that had become popular just\\nover the 2–3 years prior. Up until that point however, attention was just one of the mechanisms used by these models,\\nwhich were mostly based on LSTM (Long Short Term Memory)[3] and other RNN (Recurrent Neural Networks)[4]\\nvariations. The key insight of the Transformers paper was that, as the title implies, attention could be used as the only\\nmechanism to derive dependencies between input and output.\\nIt is beyond the scope of this blog to go into all the details of the Transformer architecture. For that, I will refer you\\nto the original paper above or to the wonderful The Illustrated Transformer 3 post. That being said, we will brieﬂy\\ndescribe the most important aspects since we will be referring to them in the catalog below. Let’s start with the basic\\narchitectural diagram from the original paper, and describe some of the components.\\n1.1 Encoder/Decoder architecture\\nA generic encoder/decoder architecture (see Figure 1) is made up of two models. The encoder takes the input and\\nencodes it into a ﬁxed-length vector. The decoder takes that vector and decodes it into the output sequence. The\\nencoder and decoder are jointly trained to minimize the conditional log-likelihood. Once trained the encoder/decoder\\ncan generate an output given an input sequence or can score a pair of input/output sequences.\\nIn the case of the original Transformer architecture, both encoder and decoder had 6 identical layers. In each of those 6\\nlayers the Encoder has two sub layers: a multi-head attention layer, and a simple feed forward network. Each sublayer\\nhas a residual connection and a layer normalization. The output size of the Encoder is 512. The Decoder adds a third\\nsublayer, which is another multi-head attention layer over the output of the Encoder. Besides, the other multi-head layer\\nin the decoder is masked to prevent attention to subsequent positions.\\n1https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html\\n2https://machinelearningmastery.com/encoder-decoder-long-short-term-memory-networks/\\n3https://jalammar.github.io/illustrated-transformer/\\n3'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 3}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 1: Transformer Architecture from [1]\\n4'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 4}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 2: The Attention Mechanism\\n1.2 Attention\\nIt is clear from the description above that the only “exotic” elements of the model architecture are the multi-headed\\nattention, but, as described above, that is where the whole power of the model lies! So, what is attention anyway? An\\nattention function is a mapping between a query and a set of key-value pairs to an output. The output is computed as\\na weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of\\nthe query with the corresponding key. Transformers use multi-headed attention, which is a parallel computation of a\\nspeciﬁc attention function called scaled dot-product attention. I will refer you again to the The Illustrated Transformer 4\\npost for many more details on how the attention mechanism works, but will reproduce the diagram from the original\\npaper in Figure 2 so you get the main idea\\nThere are several advantages of attention layers over recurrent and convolutional networks, the two most important\\nbeing their lower computational complexity and their higher connectivity, especially useful for learning long-term\\ndependencies in sequences.\\n1.3 What are Transformers used for and why are they so popular\\nThe original transformer was designed for language translation, particularly from English to German. But, already\\nthe original paper showed that the architecture generalized well to other language tasks. This particular trend became\\nquickly noticed by the research community. Over the next few months most of the leaderboards for any language-related\\nML task became completely dominated by some version of the transformer architecture (see for example the well\\nknown SQUAD leaderboard5 for question answer where all models at the top are ensembles of Transformers).\\nOne of the key reasons Transformers were able to so quickly take over most NLP leaderboards is their ability to quickly\\nadapt to other tasks, a.k.a. Transfer learning. Pretrained Transformer models can adapt extremely easily and quickly to\\ntasks they have not been trained on, and that has huge advantages. As an ML practitioner, you no longer need to train\\na large model on a huge dataset. All you need to do is re-use the pretrained model on your task, maybe just slightly\\n4https://jalammar.github.io/illustrated-transformer/\\n5https://rajpurkar.github.io/SQuAD-explorer/\\n5'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 5}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nadapting it with a much smaller data set. A speciﬁc technique used to adapt pretrained models to a different task is the\\nso-called ﬁne tuning6.\\nIt turns out that the capability of Transformers to adapt to other tasks is so great, that, while they were initially developed\\nfor language related tasks, they quickly became useful for other tasks ranging from vision[ 5] or audio and music 7\\napplications all the way to playing chess[6] or doing math[7].\\nOf course all these applications would have not been possible if it wasn’t because of the myriad of tools that made\\nthem readily available to anyone that could write a few lines of code. Not only were Transformers quickly integrated\\ninto the main AI frameworks (namely Pytorch 8 and TF9), but they even enabled the creation of an entire company\\naround them. Huggingface10, a startup that has raised over $ 60M to this day, is almost entirely built around the idea of\\ncommercializing their open source Transformers library11.\\nLast but not least, I would be remiss if I did not mention the impact of GPT-3[8] on the popularization of Transformers.\\nGPT-3 is a Transformer model introduced by OpenAI in May 2020 as a follow up to their earlier GPT and GPT-2. The\\ncompany made a big splash by introducing the model in a preprint[ 8] in which they claimed that the model was so\\npowerful that they were not in a position to release it to the world. Since then, the model has not only been released,\\nbut also commercialized through a very large partnership12 between OpenAI and Microsoft. GPT-3 powers over 300\\ndifferent applications13, and is the foundation for OpenAI’s commercial strategy (which is a lot to say for a company\\nthat has received over $ 1B in funding).\\n1.4 RLHF\\nReinforcement Learning from Human Feedback (or Preferences) aka RLHF (or RLHP) has become a huge addition to\\nthe AI toolkit as of lately. The concept was introduced already in 2017 in the paper [“Deep reinforcement learning\\nfrom human preferences”](https://arxiv.org/abs/1706.03741). More recently though, it has been applied to ChatGPT\\nand similar dialog agents like BlenderBot3 or Sparrow. The idea is pretty simple though: Once a language model is\\npretrained, we can generate different responses to a dialog and have Humans rank the results. We can use those ranking\\n(aka preferences or feedback) to train a reward, in the reinforcement learning context (see Figure 3). You can read much\\nmore in these two wonderful posts by Huggingface]14 or Weights and Bias15.\\n1.5 Diffusion\\nDiffusion models have become the new SOTA in image generation, clearly pushing aside the previous approaches such\\nas GANs (Generative Adversarial Networks). What are diffusion models? They are a class of latent variable models\\ntrained variational inference. What this means in practice is that we train a deep neural network to denoise images\\nblurred with some sort of noise function. Networks that are trained this way are in fact learning the latent space of what\\nthose images represent (see ﬁgure 4.\\nDiffusion models have relation with other generative models like the famous [Generative Adversarial Networks\\n(GAN)]16, which they have mostly replaced in many applications and, particularly with (denoising) Autoencoders.\\nSome authors17 will go as far as saying that Diffusion models are just a speciﬁc instance of autoencoders. However,\\nthey also admit that the small differences do transform their application, from the latent representation of autoconders\\nto the pure generative nature of Diffusion models.\\n6https://huggingface.co/docs/transformers/training\\n7https://magenta.tensorflow.org/music-transformer\\n8https://pytorch.org/tutorials/beginner/transformer_tutorial.html\\n9https://www.tensorflow.org/text/tutorials/transformer\\n10https://huggingface.co/docs\\n11https://github.com/huggingface/transformers\\n12https://openai.com/blog/openai-licenses-gpt-3-technology-to-microsoft/\\n13https://openai.com/blog/gpt-3-apps/\\n14https://huggingface.co/blog/rlhf\\n15https://wandb.ai/ayush-thakur/RLHF/reports/Understanding-Reinforcement-Learning-from-Human-Feedback-RLHF-Part-1--VmlldzoyODk5MTIx\\n16https://en.wikipedia.org/wiki/Generative_adversarial_network\\n17https://benanne.github.io/2022/01/31/diffusion.html\\n6'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 6}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 3: Reinforcement Learning with Human Feedback. From HuggingFace’s RLHF blog post at https://\\nhuggingface.co/blog/rlhf.\\nFigure 4: Probabilistic diffusion model architecture from “Diffusion Models: A Comprehensive Survey of Methods and\\nApplications” [9]\\n7'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 7}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2 The Transformers catalog\\nNote: For all the models available in Huggingface, I decided to directly link to the page in the documentation since\\nthey do a fantastic job of offering a consistent format and links to everything else you might need, including the original\\npapers. Only a few of the models are not included in Huggingface. For those, I try to include a link to their github if\\navailable or blog post if not. For all, I also include bibliographic reference.\\n2.1 Features of a Transformer\\nSo hopefully by now you understand what Transformer models are, and why they are so popular and impactful. In this\\nsection I will introduce a catalog of the most important Transformer models that have been developed to this day. I will\\ncategorize each model according to the following properties: Pretraining Architecture, Pretraining Task, Compression,\\nApplication, Year, and Number of Parameters. Let’s brieﬂy deﬁne each of those:\\n2.1.1 Pretraining Architecture\\nWe described the Transformer architecture as being made up of an Encoder and a Decoder, and that is true for the\\noriginal Transformer. However, since then, different advances have been made that have revealed that in some cases it\\nis beneﬁcial to use only the encoder, only the decoder, or both.\\nEncoder Pretraining These models, which are also called bi-directional or auto-encoding, only use the encoder\\nduring pretraining, which is usually accomplished by masking words in the input sentence and training the model to\\nreconstruct. At each stage during pretraining, attention layers can access all the input words. This family of models\\nare most useful for tasks that require understanding complete sentences such as sentence classiﬁcation or extractive\\nquestion answering.\\nDecoder Pretraining Decoder models, often called auto-regressive, use only the decoder during a pretraining that\\nis usually designed so the model is forced to predict the next word. The attention layers can only access the words\\npositioned before a given word in the sentence. They are best suited for tasks involving text generation.\\nTransformer (Encoder-Decoder) Pretraining Encoder-decoder models, also called sequence-to-sequence, use both\\nparts of the Transformer architecture. Attention layers of the encoder can access all the words in the input, while those\\nof the decoder can only access the words positioned before a given word in the input. The pretraining can be done using\\nthe objectives of encoder or decoder models, but usually involves something a bit more complex. These models are\\nbest suited for tasks revolving around generating new sentences depending on a given input, such as summarization,\\ntranslation, or generative question answering.\\n2.1.2 Pretraining Task\\nWhen training a model we need to deﬁne a task for the model to learn on. Some of the typical tasks, such as predicting\\nthe next word or learning to reconstruct masked words were already mentioned above. “Pre-trained Models for Natural\\nLanguage Processing: A Survey”[10] includes a pretty comprehensive taxonomy of pretraining tasks, all of which can\\nbe considered self-supervised:\\n1. Language Modeling (LM): Predict next token (in the case of unidirectional LM) or previous and next token\\n(in the case of bidirectional LM)\\n2. Masked Language Modeling (MLM): mask out some tokens from the input sentences and then trains the\\nmodel to predict the masked tokens by the rest of the tokens\\n3. Permuted Language Modeling (PLM): same as LM but on a random permutation of input sequences. A\\npermutation is randomly sampled from all possible permutations. Then some of the tokens are chosen as the\\ntarget, and the model is trained to predict these targets.\\n4. Denoising Autoencoder (DAE): take a partially corrupted input (e.g. Randomly sampling tokens from the\\ninput and replacing them with \"[MASK]\" elements. randomly deleting tokens from the input, or shufﬂing\\nsentences in random order) and aim to recover the original undistorted input.\\n5. Contrastive Learning (CTL): A score function for text pairs is learned by assuming some observed pairs of\\ntext that are more semantically similar than randomly sampled text. It includes:\\n• Deep InfoMax (DIM): maximize mutual information between an image representation and local regions\\nof the image;\\n8'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 8}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n• Replaced Token Detection (RTD):predict whether a token is replaced given its surroundings;\\n• Next Sentence Prediction (NSP): train the model to distinguish whether two input sentences are contin-\\nuous segments from the training corpus; and\\n• Sentence Order Prediction (SOP): Similar to NSP, but uses two consecutive segments as positive\\nexamples, and the same segments but with their order swapped as negative examples\\n2.1.3 Application\\nHere we will note what are the main practical applications of the Transformer model. Most of these applications will be\\nin the language domain (e.g. question answering, sentiment analysis, or entity recognition). However, as mentioned\\nbefore, some Transformer models have also found applications well beyond NLP and are also included in the catalog.\\n2.2 Catalog table\\nFigure 5 is a screenshot of a large table where I have tabulated all the models. If you are interested in the table, access it\\ndirectly in 18\\n2.3 Family Tree\\nThe diagram in ﬁgure 6 is a simple view that highlights the different families of transformers and how they relate to\\neach other.\\n2.4 Chronological timeline\\nAnother interesting perspective of the catalog is to see it as a chronological timeline. In Figure 7 you will ﬁnd all the\\ntransformers in the catalog sorted by their date of publication. In this ﬁrst visualization, the Y-axis is only used to\\ncluster transformers of related heritage/family.\\nIn Figure 8, the Y-axis represents model size in millions of parameters. You won’t be able to see all the models in the\\ncatalog since many fall right on the same time and size, so please refer to the previous image for that.\\n2.5 Catalog List\\nFinally, here is the full list view that might be easier to follow along in some cases:\\n2.5.1 ALBERT\\n• Reference:19[11]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM/NSP\\n• Extension: Compressed version of BERT using parameter sharing, which is much more efﬁcient given the\\nsame number of parameters\\n• Application: Same as BERT\\n• Date (of ﬁrst known publication): 09/2019\\n• Num. Params:b12M, Large = 18M, XLarge = 60M*\\n• Corpus:Same as BERT\\n• Lab: Google\\n2.5.2 AlphaFold\\n• Reference:20[12]\\n18https://docs.google.com/spreadsheets/d/1ltyrAB6BL29cOv2fSpNQnnq2vbX8UrHl47d7FkIf6t4\\n19https://huggingface.co/docs/transformers/model_doc/albert\\n20https://www.deepmind.com/publications/highly-accurate-protein-structure-prediction-with-alphafold\\n9'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 9}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 5: You can access the original table at https://docs.google.com/spreadsheets/d/\\n1ltyrAB6BL29cOv2fSpNQnnq2vbX8UrHl47d7FkIf6t4 for easier browsing across the different model features.\\n10'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 10}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 6:\\nFigure 7: Transformer timeline. Colors describe Transformer family.\\n11'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 11}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 8: Transformer timeline. On the vertical axis, number of parameters. Colors describe Transformer family.\\n• Family: SE(3) Transformer21\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: Protein folding prediction*ion of BERT using parameter sharing, which is much more\\nefﬁcient given the same number of parameters\\n• Extension:The original Alphafold used a BERT-style transformer. The details of Alphafold’s Transformer are\\nnot known, but it is believed it is an extension of the SE(3)-Tranformer, a 3-D equivariant Transformer (see\\nthis blog post22)\\n• Application: Same as BERT\\n• Date (of ﬁrst known publication): 09/2019\\n• Num. Params:b12M, Large = 18M, XLarge = 60M*\\n• Corpus:Same as BERT\\n• Lab:Google\\n2.5.3 Anthropic Assistant\\n• Reference:23 see also24 [13, 14]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: Protein folding prediction*ion of BERT using parameter sharing, which is much more\\nefﬁcient given the same number of parameters\\n• Extension:These models do not introduce novelties at the architecture/pretraining level and they are\\nbased on GPT-3 but rather focuses on how to improve alignment through ﬁne-tuning and prompting.\\nNote that the Anthropic Assistant includes several models optimized for different tasks. Latest versions\\nof this work focus on the beneﬁts of RLHF\\n21https://arxiv.org/abs/2006.10503\\n22https://fabianfuchsml.github.io/alphafold2/\\n23https://arxiv.org/abs/2112.00861\\n24https://arxiv.org/abs/2204.05862\\n12'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 12}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n• Application: Different models with different applications from general dialog to code assistant.\\n• Date (of ﬁrst known publication): 12/2021\\n• Num. Params:10M to 52B\\n• Corpus:400B tokens from ﬁltered Common Crawl and Books. They also create several Dialogue Preference\\ndatasets for the RLHF training.\\n• Lab:Anthropic\\n2.5.4 BART\\n• Reference:25[15]\\n• Family: BERT for encoder, GPT for Decoder\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE\\n• Extension:It can be seen as a generalization of BERT and GPT in that it combines ideas from both in\\nthe encoder and decoder\\n• Application: Mostly text generation but also some text understanding tasks*\\n• Date (of ﬁrst known publication): 10/2019*\\n• Num. Params:10 % more than BERT\\n• Corpus:Same as RoBERTa (160Gb of news, books, stories\\n• Lab:Facebook\\n2.5.5 BERT\\n• Reference:26[16]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM/NSP\\n• Extension:It can be seen as a generalization of BERT and GPT in that it combines ideas from both in\\nthe encoder and decoder\\n• Application:General Language Understanding and Question Answering. Many other language applications\\nfollowed\\n• Date (of ﬁrst known publication): 10/2018\\n• Num. Params:Base = 110M, Large = 340MT\\n• Corpus:Toronto Book Corpus and Wikipedia (3.3B Tokens)\\n• Lab:Google\\n2.5.6 Big Bird\\n• Reference:27[17]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM\\n• Extension: Big Bird can extend other architectures such as BERT, Pegasus, or RoBERTa by using a sparse\\nattention mechanism that elminates the quadratic dependency thus making it more suitable for longer sequences\\n• Application:Particularly well suited for longer sequences, not only in text but also e.g. in genomics\\n• Date (of ﬁrst known publication): 07/2020\\n25https://huggingface.co/docs/transformers/model_doc/bart\\n26https://huggingface.co/docs/transformers/model_doc/bert\\n27https://huggingface.co/docs/transformers/model_doc/big_bird\\n13'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 13}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n• Num. Params:Depends on the overall architecture\\n• Corpus:Books, CC-News, Stories and Wikipedia)\\n• Lab:Google\\n2.5.7 BlenderBot3\\n• Reference:28[18]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: BlenderBot 3 is based on a pre-trained OPT. It adds features needed for a dialog agent such as\\nlong-term memory or the ability to search the internet. It is also ﬁne-tuned for some speciﬁc tasks given human\\nfeedback on them.\\n• Application: Same as GPT-3\\n• Date (of ﬁrst known publication): 08/2022\\n• Num. Params:175B\\n• Corpus: 180B tokens = RoBERTa + the Pile + PushShift.io Reddit\\n• Lab:Facebook\\n2.5.8 BLOOM\\n• Reference:29\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Main difference to GPT-3 is that it uses full attention instead of sparse attention\\n• Application: Same as GPT-3\\n• Date (of ﬁrst known publication): 07/2022\\n• Num. Params:176B\\n• Corpus: 366B tokens (1.5 TB of text data) multilingual dataset\\n• Lab: Big Science/Huggingface\\n2.5.9 ChatGPT\\n• Reference:30\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: ChatGPT takes a GPT3.5 (aka GPT3 Davinci-003) pretrained model and uses RLHF to ﬁnetune\\nthe model mostly like described in InstructGPT but with slight differences in the data collection. ChatGPT is\\nalso more than a model since it includes extensions for Memory Store and retrieval similar to BlenderBot3\\n• Application: Dialog agents\\n• Date (of ﬁrst known publication): 10/2022\\n• Num. Params:Same as GPT3\\n• Corpus: Same as GPT3 + datasets generated for RLHF\\n• Lab: OpenAI\\n28https://arxiv.org/abs/2208.03188\\n29https://huggingface.co/docs/transformers/model_doc/bloom\\n30https://openai.com/blog/chatgpt/\\n14'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 14}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.10 Chinchilla\\n• Reference:31[19]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Same as Gopher but with optimizations to reduce model size and therefore training/inference time\\nwith equal or superior performance\\n• Application: Same as Gopher/GPT3\\n• Date (of ﬁrst known publication): 03/2022\\n• Num. Params:70B\\n• Corpus: Massive Text\\n• Lab: Deepmind\\n2.5.11 CLIP\\n• Reference:32[20]\\n• Family: CLIP (Also using Resnet, ViT, and vanilla transformer for text)\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: predict which of the N × N possible (image, text) pairings across a batch actually occurred\\n• Extension: Combines Resnet and ViT for the visual encoding with Transformer for the Textual encoder\\n• Application: Image/object classiﬁcation\\n• Date (of ﬁrst known publication): 02/2021\\n• Num. Params:?\\n• Corpus: WIT (WebImageText) - 400 million text,image pairs\\n• Lab: OpenAI\\n2.5.12 CM3\\n• Reference:33[21]\\n• Family: HTML\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: Causality-masked LM\\n• Extension: This is somewhat similar to HTML in its use of structured training data. However, it is a different\\narchitecture and uses causal masking\\n• Application: Multimodal language model with the ability to do structured prompting\\n• Date (of ﬁrst known publication): 01/2022\\n• Num. Params:13B (largest)\\n• Corpus: CC-News, English Wikipedia\\n• Lab: Facebook\\n31https://arxiv.org/abs/2203.15556\\n32https://huggingface.co/docs/transformers/model_doc/clip\\n33https://arxiv.org/abs/2201.07520\\n15'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 15}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.13 CTRL\\n• Reference:34[22]\\n• Family:\\n• Pretraining Architecture: Decoder\\n• Pretraining Task:\\n• Extension: model can generate text conditioned on control codes that specify domain, style, topics, dates,\\nentities, relationships between entities, plot points, and task-related behavior\\n• Application: Controllable text generation\\n• Date (of ﬁrst known publication): xx\\n• Num. Params:1.63B\\n• Corpus: 140 GB of text including: Wikipedia (En, De, Es, Fr), Project Gutenberg, 45 subreddits, OpenWeb-\\nText2, Amazon Reviews, Europarl and UN data from WMT, question-answer pairs from ELI5, and the MRQA\\nshared task3, which includes the Stanford Question Answering Dataset, NewsQA, TriviaQA, SearchQA,\\nHotpotQA , and Natural Questions\\n• Lab: Salesforce\\n2.5.14 DALL-E\\n• Reference:35[23]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: Caption prediction\\n• Extension: A differential variational auto-encoder is used to learn the visual codebook. The transformer is a\\nvariation of GPT-3\\n• Application: Text to image\\n• Date (of ﬁrst known publication): 01/2021\\n• Num. Params:12B\\n• Corpus: 250 million text-images pairs from the internet\\n• Lab: OpenAI\\n2.5.15 DALL-E 2\\n• Reference:36[24]\\n• Family: CLIP, GLIDE\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: Caption prediction\\n• Extension: Combines CLIP encoder and Diffusion decoder similar to GLIDE\\n• Application: Text to image\\n• Date (of ﬁrst known publication): 04/2022\\n• Num. Params:3.5B\\n• Corpus: Combination of the DALL-E and CLIP datasets\\n• Lab: OpenAI\\n34https://huggingface.co/docs/transformers/model_doc/ctrl\\n35https://openai.com/blog/dall-e/\\n36https://openai.com/dall-e-2/\\n16'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 16}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.16 Decision Transformers\\n• Reference:37[25]\\n• Family: GPT, Control Transformers” (not per se a family, but grouping here those transformers that try to\\nmodel more general control, RL-like, tasks)\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: Next action prediction\\n• Extension: Decision transformers use a GPT architecture and extend it by encoding trajectories in a way that\\nthey can be learned by an auto-regressive task\\n• Application: General RL (reinforcement learning tasks)\\n• Date (of ﬁrst known publication): 06/2021\\n• Num. Params:Same as GPT\\n• Corpus: Different corpus for different experiments\\n• Lab: Google/UC Berkeley/Facebook\\n2.5.17 DialoGPT\\n• Reference:38[26]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: GPT-2 architecture trained on dialog data\\n• Application: Text generation in dialog settings\\n• Date (of ﬁrst known publication): 10/2019\\n• Num. Params:1.5B\\n• Corpus: 140M Reddit conversations\\n• Lab: Microsoft\\n2.5.18 DistilBERT\\n• Reference:39[27]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM/NSP\\n• Extension: Compressed version of BERT using distillation, which is much more efﬁcient given the same\\nnumber of parameters\\n• Application: Same as BERT\\n• Date (of ﬁrst known publication): 10/2019\\n• Num. Params:66M\\n• Corpus: Same as BERT\\n• Lab: Huggingface\\n37https://arxiv.org/abs/2106.01345\\n38https://huggingface.co/docs/transformers/model_doc/dialogpt\\n39https://huggingface.co/docs/transformers/model_doc/distilbert\\n17'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 17}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.19 DQ-BART\\n• Reference:40[28]\\n• Family: BART\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE\\n• Extension: Adds quantization and distillation to a BART model to improve performance and model size\\n• Application: Text generation and understanding\\n• Date (of ﬁrst known publication): 03/2022\\n• Num. Params:Up to 30x reduction in parameters compared to standard BART\\n• Corpus: CNN/DM, XSUM, ELI5, WMT16 En-Ro ( 1M tokens)\\n• Lab: Amazon\\n2.5.20 ELECTRA\\n• Reference:41[29]\\n• Family:\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: RTD\\n• Extension: Same as BERT\\n• Application: 03/2020\\n• Date (of ﬁrst known publication): xx\\n• Num. Params:Base = 110M, Large = 330M\\n• Corpus: Same as BERT except for Large with is same as XLNet\\n• Lab: Stanford/Google\\n2.5.21 ERNIE\\n• Reference:42[30]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM\\n• Extension: Uses BERT for Encoder architecture, but stacks and aggregates two of them for text and entities.\\nThis architecture could be understood as BERT for text + knowledge graphs\\n• Application: Knowledge intensive related tasks that might beneﬁt from knowledge graphs or entities such as\\nentity recognition\\n• Date (of ﬁrst known publication): 05/2019\\n• Num. Params:114M\\n• Corpus: English Wikipedia + Wikidata for entitites (note that they initialize model to original BERT parameter\\nvalues\\n• Lab: Various Chinese institutions\\n40https://arxiv.org/abs/2203.11239\\n41https://huggingface.co/docs/transformers/model_doc/electra\\n42https://arxiv.org/abs/1905.07129\\n18'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 18}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.22 Flamingo\\n• Reference:43[31]\\n• Family: Chinchilla\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: Log likelihood of text given some visual input\\n• Extension: It uses a frozen textual language model (like Chinchilla) conditioned on the visual representation,\\nwhich is encoded from a Normalizer-Free ResNet\\n• Application: Text to image\\n• Date (of ﬁrst known publication): 04/2022\\n• Num. Params:80B (largest)\\n• Corpus: MultiModal MassiveWeb (M3W): 185 million images and 182 GB text + a number of text paired\\nwith image datasets: ALIGN + LTIP (Long Text & Image Pairs) = 312 million images, and VTP (Video &\\nText Pairs) = 27 million short videos (approximately 22 seconds on average)\\n• Lab: Deepmind\\n2.5.23 Gato\\n• Reference:44[32]\\n• Family: “Control Transformers” (not per se a family, but grouping here those transformers that try to model\\nmore general control, RL-like, tasks)\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: MLM (where tokens are either text or agent actions)\\n• Extension: The standard decoder-only transformer architecture is preceded by an embedding layer that can\\nembed text and images, plus add position encodings to add spatial information when applicable.\\n• Application: Gato presents a generalizable agent that can be used beyond text to tasks such as playing Atari\\nor controlling a robot arm.\\n• Date (of ﬁrst known publication): 05/2022\\n• Num. Params:1.2B\\n• Corpus: 1.5T tokens including standard text (e.g. MassiveText), vision (e.g. ALIGN), and simulation\\nenvironments (e.g. ALE Atari, or RGB Stacking Real Robot)\\n• Lab: Deepmind\\n2.5.24 GLaM\\n• Reference:45[33]\\n• Family: Transformer\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: GLaM introduces a Mixture of 64 Experts to increase parameter count and generalization properties\\nin a somewhat standard decoder-only. Transformer architecture. Only two experts get activated at a time per\\ntoken, which makes the model also more efﬁcient in training and inference.\\n• Application: General language modeling\\n• Date (of ﬁrst known publication): 12/2021\\n• Num. Params:1.2T across 64 experts, but only 96B get activated for inference\\n• Corpus: 1.6T tokens including web pages ﬁltered by Wikipedia and books for quality\\n• Lab: Google\\n43https://arxiv.org/abs/2204.14198\\n44https://www.deepmind.com/publications/a-generalist-agent\\n45https://ai.googleblog.com/2021/12/more-efficient-in-context-learning-with.html\\n19'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 19}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.25 GLIDE\\n• Reference:46[34]\\n• Family: Diffusion models\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: Caption prediction\\n• Extension: GLIDE can be seen as an extension of the ADM (Ablated Diffusion Model) by the same authors.\\nHowever, ADM is not per se a transformer architecture although it does resemble one in some of the\\nconﬁgurations the authors use. Given that ADM is by the same authors and was quickly followed up by\\nGLIDE, I think it is fair to consider GLIDE as the ﬁrst of its kind.\\n• Application: Text to image\\n• Date (of ﬁrst known publication): 12/2021\\n• Num. Params: 3.5B diffusion model (2.3B for visual encoding, 1.2B for textual) + 1.5B for model for\\nupsampling\\n• Corpus: Same as DALL-E\\n• Lab: OpenAI\\n2.5.26 Global Context ViT\\n• Reference:47[35]\\n• Family: ViT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: Image classiﬁcation\\n• Extension: hierarchical ViT architecture consisting of local and global self-attention modules\\n• Application: Image generation\\n• Date (of ﬁrst known publication): 06/2022\\n• Num. Params:90M\\n• Corpus: Imagenet-1K and other task dependent dataasets\\n• Lab: NVidia\\n2.5.27 Gopher\\n• Reference:48[36]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Same as GPT-2 but use RSNorm instead of LayerNorm and relative positional encoding rather\\nthan absolute\\n• Application: Mostly Language Modeling and NLU, but also extensible like GPT\\n• Date (of ﬁrst known publication): 12/2021\\n• Num. Params:280B\\n• Corpus: Massive Text (2.35 billion documents, or about 10.5 TB of text including Massive Web, Books,\\nGithub, News, C4, and Wikipedia.\\n• Lab: Deepmind\\n46https://arxiv.org/abs/2112.10741\\n47https://arxiv.org/abs/2206.09959\\n48https://www.deepmind.com/blog/language-modelling-at-scale-gopher-ethical-considerations-and-retrieval\\n20'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 20}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.28 GopherCite\\n• Reference:49[37]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: GopherCite is based on Gopher but adds a step using RLHP (Reinforcement Learning from\\nHuman Preferences) to learn whether not only a response is plausible but also supported\\n• Application: Dialog systems, Q&A, general language generation tasks\\n• Date (of ﬁrst known publication): 03/2022\\n• Num. Params:280B\\n• Corpus: Same as Gopher plus speciﬁc dataset generated in the RLHP process\\n• Lab: Deepmind\\n2.5.29 GPT\\n• Reference:50[38]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension:\\n• Application: Text generation, but adaptable to many other NLP tasks when ﬁne tuned.\\n• Date (of ﬁrst known publication): 06/2018\\n• Num. Params:117M\\n• Corpus: Unsupervised Pretraining on BookCorpus dataset. Supervised Finetuning on several task-speciﬁc\\ndatasets including SNLI, RACE, Quora. . .\\n• Lab: OpenAI\\n2.5.30 GPT-2\\n• Reference:51[39]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Minor extensions to the GPT architecture (e.g. layer normalization moved to the input of each\\nsub-layer, or increased context size from 512 to 1024)\\n• Application: Text generation, but adaptable to many other NLP tasks when ﬁne tuned.\\n• Date (of ﬁrst known publication): 02/2019\\n• Num. Params:1.5B\\n• Corpus: 8 million web pages (40 GB). 10X GPT . WebText dataset is created by crawling all links at Reddit\\nwith at least 3 Karma points.\\n• Lab: OpenAI\\n49https://arxiv.org/abs/2203.11147\\n50https://huggingface.co/docs/transformers/model_doc/openai-gpt\\n51https://huggingface.co/docs/transformers/model_doc/gpt2\\n21'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 21}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.31 GPT-3\\n• Reference:52[8]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Same as GPT-2 with the only addition of alternating dense and locally banded sparse attention\\npatterns, inspired by the Sparse Transformer\\n• Application: Initially text generation, but has over time been used for a large range of applications in areas\\nsuch as code generation, but also image and audio generation\\n• Date (of ﬁrst known publication): 05/2020\\n• Num. Params:175 B\\n• Corpus: 500B tokens including CommonCrawl (410B), WebText2 (19B), Books1 (12B), Books2 (55B), and\\nWikipedia (3B)\\n• Lab: OpenAI\\n2.5.32 GPT-3.5\\n• Reference:53\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: The GPT3.5 series includes a number of models like Davinci-003. They are basically versions of\\nthe InstructGPT model. See [here](https://scale.com/blog/gpt-3-davinci-003-comparison) for details on the\\ncomparison of the performance to older GPT3 models.\\n• Application: Dialog and general language, but there is a code speciﬁc model too\\n• Date (of ﬁrst known publication): 10/2022\\n• Num. Params:175B\\n• Corpus: Same as InstructGPT\\n• Lab: OpenAI\\n2.5.33 InstructGPT\\n• Reference:54[40]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: GPTInstruct starts off with a pretrained GPT3 model and adds reward modeling through reinforce-\\nment learning after a supervised ﬁnetuning\\n• Application: Knowledge-intensive dialog or language tasks\\n• Date (of ﬁrst known publication): 01/2022\\n• Num. Params:Same as GPT3\\n• Corpus: Same as GPT3 for pretraining, but ﬁnetuned and optimized using labeler data and prompts\\n• Lab: OpenAI\\n52https://github.com/openai/gpt-3\\n53https://beta.openai.com/docs/model-index-for-researchers\\n54https://openai.com/blog/instruction-following/\\n22'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 22}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.34 GPT-Neo\\n• Reference:55\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Similar to GPT-2 but uses local attention in every other layer with a window size of 256 tokens\\n• Application: Text generation, but adaptable to many other NLP tasks when ﬁne tuned\\n• Date (of ﬁrst known publication): 03/2021\\n• Num. Params: 5B, 2.7B (XL)\\n• Corpus: Pile — 840 GB open source text dataset that combines 22 pre existing datasets\\n• Lab: EleutherAI\\n2.5.35 GPT-NeoX-20B\\n• Reference:56[41]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Similar to GPT-3 with rotary encoders instead of positional, parallel attention and feed forward\\nlayers, different initialization, and all dense layers instead of alternate dense/sparse\\n• Application: same as GPT-3\\n• Date (of ﬁrst known publication): 04/2022\\n• Num. Params:20B\\n• Corpus: Pile — 840 GB open source text dataset that combines 22 pre existing datasets\\n• Lab: EleutherAI\\n2.5.36 HTML\\n• Reference:57[42]\\n• Family: BART\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE\\n• Extension: As opposed to BART, they don’t do sentence shufﬂing\\n• Application: General purpose language model that allows structured HTML prompting\\n• Date (of ﬁrst known publication): 07/2021\\n• Num. Params:400M\\n• Corpus: 23TB of simpliﬁed HTML extracted from CommonCrawl\\n• Lab: Facebook\\n2.5.37 Imagen\\n• Reference:58[43]\\n• Family: T5, CLIP, Diffusion models\\n• Pretraining Architecture: T5 (or CLIP or BERT) for frozen text encoder + U-net architecture for cascaded\\ndiffusion models for text to image\\n55https://huggingface.co/docs/transformers/model_doc/gpt_neo\\n56https://arxiv.org/abs/2204.06745\\n57https://arxiv.org/abs/2107.06955\\n58https://imagen.research.google/\\n23'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 23}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n• Pretraining Task: image/text pair prediction\\n• Extension: Imagen adds a few extensions to the U-net diffusion architecture (pooled embedding vector, cross\\nattention over text embeddings, and Layer Normalizations)\\n• Application: Text to image\\n• Date (of ﬁrst known publication): 06/2022\\n• Num. Params:2B\\n• Corpus: a combination of internal datasets, with 460M image-text pairs, and the publicly available Laion\\ndataset, with 400M image-text pairs\\n• Lab: Google\\n2.5.38 Jurassic-1\\n• Reference:59[44]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Very similar to GPT-3, but far more parameters and improved training efﬁciency mostly because\\nof the improved tokenizer. Also, different ratio of depth to breadth\\n• Application: Similar to GPT-3\\n• Date (of ﬁrst known publication): 09/2021\\n• Num. Params:178B (Jumbo), 7.5B (Large)\\n• Corpus: 300B tokens (same as GPT-3)\\n• Lab: AI21\\n2.5.39 LAMDA\\n• Reference:60[45]\\n• Family: Transformer\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: LAMDA focuses on how to improve safety, quality, and groundeness using different ﬁne-tuning\\nstrategies\\n• Application: General language modeling\\n• Date (of ﬁrst known publication): 01/2022\\n• Num. Params:137B\\n• Corpus: 1.56T words from public dialog data and other public web documents\\n• Lab: Google\\n2.5.40 mBART\\n• Reference:61[46]\\n• Family: BART\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE\\n• Extension:\\n59https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_\\npaper.pdf\\n60https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html\\n61https://huggingface.co/docs/transformers/model_doc/mbart\\n24'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 24}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n• Application: Translation\\n• Date (of ﬁrst known publication): 01/2020\\n• Num. Params: Same as BART\\n• Corpus:\\n• Lab: CC25 Corpus includes 25 monolingual corpuses in different languages. Largest corpuses are English\\n(300 GB) and Russian (280GB)\\n2.5.41 Megatron\\n• Reference:62[47]\\n• Family: GPT/BERT/T5\\n• Pretraining Architecture: Encoder or Decorder, depending on the base model\\n• Pretraining Task: Same as base model\\n• Extension: Megatron is a family of models that extend previously known architectures (namely GPT-2 and\\nBERT originally, but also T5 more recently) by introducing model parallelism primitives. In the case of BERT,\\nthe authors also replace the next sentence prediction head with sentence order prediction and use whole word\\nn-gram masking.\\n• Application: Same as base model\\n• Date (of ﬁrst known publication): 03/2020\\n• Num. Params: 8.3B (GPT-like), 3.9B (BERT-like)\\n• Corpus: Original paper uses an aggregate dataset consisting of Wikipedia), CC-Stories), RealNews, and\\nOpenWebtext\\n• Lab: NVidia\\n2.5.42 Minerva\\n• Reference:63[48]\\n• Family: PaLM\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Extends PaLM by ﬁne-tuning on the mathematical dataset\\n• Application: Mathematical reasoning\\n• Date (of ﬁrst known publication): 06/2022\\n• Num. Params:540B\\n• Corpus: Same as PaLM + 118GB dataset of scientiﬁc papers from the arXiv preprint server and web pages\\nthat contain mathematical expressions using LaTeX, MathJax, or other mathematical typesetting formats\\n• Lab: Google\\n2.5.43 MT-NLG (Megatron TouringNLG)\\n• Reference:64[49]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Uses parallelization similar to Megatron to train a LM double the size of GPT-3\\n• Application: Language generation and others (similar to GPT-3)\\n• Date (of ﬁrst known publication): 10/2021\\n62https://github.com/NVIDIA/Megatron-LM\\n63https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\\n64https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/\\n25'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 25}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n• Num. Params:530B\\n• Corpus: The Pile65 (800GB dataset) + 2 Common Crawl snapshots\\n• Lab: NVidia\\n2.5.44 OPT\\n• Reference:66[50]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Basically same architecture as GPT-3 but with some training improvements introduced in Megatron-\\nLM\\n• Application: Same as GPT-3\\n• Date (of ﬁrst known publication): 05/2022\\n• Num. Params: 175B (and other smaller versions)\\n• Corpus: 180B tokens = RoBERTa + the Pile + PushShift.io Reddit\\n• Lab: Facebook\\n2.5.45 PalM\\n• Reference:67[51]\\n• Family: Transformer\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Palm uses a typical decoder-only transformer architecture, but adds quite a few extensions:\\nSwiGLU activations, parallel layers, multi-query attention, RoPE embeddings, Shared Input-Output Embed-\\ndings, no biases, and a 256k SentencePiece vocabulary generated from the training data.\\n• Application: PalM is designed as a general purpose language model with applicability to hundreds of different\\nlanguage tasks\\n• Date (of ﬁrst known publication): 04/2022\\n• Num. Params:540B\\n• Corpus: 780B tokens from ﬁltered webpages, books, Wikipedia, news articles, source code, and social media\\nconversations. Code includes 24 programming languages.\\n• Lab: Google\\n2.5.46 Pegasus\\n• Reference:68[52]\\n• Family: Transformer\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE (more concretely GSG) and MLM\\n• Extension: Extends vanilla Transformer by using a different pretraining task (GSG: Gap Sentence Generation)\\nthat is better suited for summarization\\n• Application: Summarization\\n• Date (of ﬁrst known publication): 12/2019\\n• Num. Params: Base = 223M, Large = 568M\\n• Corpus: C4 (750GB) + HugeNews (3.8 TB)\\n• Lab: UCL/Google\\n65https://arxiv.org/abs/2101.00027\\n66https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/\\n67https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html\\n68https://huggingface.co/docs/transformers/model_doc/pegasus\\n26'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 26}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.47 RoBERTa\\n• Reference:69[53]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM (Dynamic)\\n• Extension: Extension of BERT with optimized training procedure and more data\\n• Application: Same as BERT\\n• Date (of ﬁrst known publication): 07/2019\\n• Num. Params: 356M\\n• Corpus: Same as BERT + CC News + OpenWebText + Stories ( 33B Tokens)\\n• Lab: UW/Google\\n2.5.48 SeeKer\\n• Reference:70[54]\\n• Family: GPT (but can extend any family)\\n• Pretraining Architecture: Encoder/decoder or decoder only, depending on the base model it’s extending\\n• Pretraining Task: Depends on the base model\\n• Extension: SeeKer is an extension that can be applied to any Transformer architecture by introducing “search”,\\n“knowledge”, and “response” modules that are introduced during pretraining\\n• Application: Same as base models\\n• Date (of ﬁrst known publication): 03/2022\\n• Num. Params:Depends on the base model\\n• Corpus: Same as base model\\n• Lab: Facebook\\n2.5.49 Sparrow\\n• Reference:71[55]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Starts from the Chinchilla 70B model but adds RLHF (Reinforcement Learning with Human\\nFeedback). It also adds inline evidence a la GopherCite\\n• Application: Dialog agents and general language generation applications like Q&A\\n• Date (of ﬁrst known publication): 09/2022\\n• Num. Params: 70B\\n• Corpus: Same as Chinchilla + interactive data gathering with human annotators during the RLHF process\\n• Lab: Deepmind\\n69https://huggingface.co/docs/transformers/model_doc/roberta\\n70https://parl.ai/projects/seeker/\\n71https://arxiv.org/abs/2209.14375\\n27'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 27}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.50 StableDiffusion\\n• Reference:72[56]\\n• Family: Diffusion\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: Caption prediction\\n• Extension: Stable diffusion is basically the Latent Diffusion model developed by LMU Munich researchers +\\nsome learnings on conditional diffusion from DALL-e and Imagen\\n• Application: Text to image\\n• Date (of ﬁrst known publication): 12/2021\\n• Num. Params: 890M (although there are different, smaller, variants)\\n• Corpus: LAION-5B, a publicly available dataset derived from Common Crawl\\n• Lab: LMU Munich + Stability.ai + Eleuther.ai\\n2.5.51 Swin Transformer\\n• Reference:73[57]\\n• Family: ViT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: Same as ViT\\n• Extension: Extends ViT by replacing the standard multi-head self attention (MSA) module by a module based\\non shifted windows (Swin) allowing ViT-like architectures to generalize to higher resolution images\\n• Application: Image (object detection, image classiﬁcation..)\\n• Date (of ﬁrst known publication): 03/2021\\n• Num. Params: 29M-197M\\n• Corpus: Imagenet and Imagenet-22k\\n• Lab: Facebook\\n2.5.52 Switch\\n• Reference:74[58]\\n• Family: T5\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE\\n• Extension: Goal to increase parameter count while keeping FLOP operations constant by using efﬁcient\\nrouting of MoE (Mixture of Experts)\\n• Application: General language tasks (e.g. question answering)\\n• Date (of ﬁrst known publication): 01/2021\\n• Num. Params: 1T\\n• Corpus: Colossal Clean Crawled Corpus\\n• Lab: Google\\n72https://huggingface.co/CompVis/stable-diffusion\\n73https://github.com/microsoft/Swin-Transformer\\n74https://arxiv.org/abs/2101.03961\\n28'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 28}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.53 T5\\n• Reference:75[59]\\n• Family:\\n• Pretraining Architecture: Encoder/Decoder\\n• Pretraining Task: DAE\\n• Extension: Same as original Transformer with some additions such as relative positional embeddings like\\nTransformer XL\\n• Application: General language tasks including machine translation, question answering, abstractive summa-\\nrization, and text classiﬁcation\\n• Date (of ﬁrst known publication): 10/2019\\n• Num. Params: 11 B (up to)\\n• Corpus: Colossal Clean Crawled Corpus (C4) — Cleaned up version of the Common Crawl dataset — 750\\nGB\\n• Lab: Google\\n2.5.54 Trajectory Transformers\\n• Reference:76[60]\\n• Family: GPT, Control Transformers” (not per se a family, but grouping here those transformers that try to\\nmodel more general control, RL-like, tasks)\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: predict most likely sequence\\n• Extension: Similarly to the Decision transformers, the main extension introduced by Trajectory Transformers\\nis a way to encode a trajectory (state, actions, rewards)\\n• Application: General RL (reinforcement learning tasks)\\n• Date (of ﬁrst known publication): 06/2021\\n• Num. Params: Smaller architecture than GPT\\n• Corpus: D4RL dataset and other RL datasets depending on the task at hand\\n• Lab: UC Berkeley\\n2.5.55 Transformer XL\\n• Reference:77[61]\\n• Family:\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Relative positioned embeddings enable longer-context attention when compared to vanilla Trans-\\nformer model\\n• Application: General language tasks\\n• Date (of ﬁrst known publication): 01/2019\\n• Num. Params: 151M\\n• Corpus: Different training datasets depending on experiments, but baseline is Wikitext-103\\n• Lab: CMU/Google\\n75https://huggingface.co/docs/transformers/model_doc/t5\\n76https://arxiv.org/abs/2106.02039\\n77https://huggingface.co/docs/transformers/model_doc/transfo-xl\\n29'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 29}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.56 Turing-NLG\\n• Reference:78[62]\\n• Family: GPT\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: LM\\n• Extension: Optimized version of GPT2 with optimal hyperparameters and software/hardware platform to\\nimprove training\\n• Application: Same as GPT-2/3\\n• Date (of ﬁrst known publication): 02/2020\\n• Num. Params: 17B originally, up to 530B more recently\\n• Corpus: Highest quality subset from The Pile + 2 CC snapshots (339B tokens)\\n• Lab: Microsoft\\n2.5.57 ViT\\n• Reference:79[63]\\n• Family: BERT\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: Image classiﬁcation\\n• Extension: Extension of BERT architecture to train on patches of images\\n• Application: Image classiﬁcation\\n• Date (of ﬁrst known publication): 10/2020\\n• Num. Params: 86M(Base) to 632M (Huge)\\n• Corpus: From standard Imagenet to JFT-300M (large inhouse dataset)\\n• Lab: Google\\n2.5.58 Wu Dao 2.0\\n• Reference:80\\n• Family: GLM (General Language Model)\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: Autoregressive blank inﬁlling\\n• Extension: Similar to GPT in that it uses a Decoder/autoregressive architecture but applies a different\\npretraining task proposed in the GLM family of models. Besides, Wu Dao uses a Fast Mixture of Experts\\nhttps://github.com/laekov/fastmoe) approach to scale training to trillions of parameters\\n• Application: Language and multimodal (particularly image)\\n• Date (of ﬁrst known publication): 06/2021\\n• Num. Params: 1.75T\\n• Corpus: ?\\n• Lab: Beijing Academy of Artiﬁcial Intelligence\\n78https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/\\n79https://huggingface.co/docs/transformers/model_doc/vit\\n80https://en.wikipedia.org/wiki/Wu_Dao\\n30'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 30}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n2.5.59 XLM-RoBERTa\\n• Reference:81[64]\\n• Family: RoBERTa\\n• Pretraining Architecture: Encoder\\n• Pretraining Task: MLM (Dynamic)\\n• Extension: An extension of RoBERTa that introduces small parameter tuning insights in the context of\\nmultilingual applications\\n• Application: Translation and other cross-lingual language tasks\\n• Date (of ﬁrst known publication): 10/2019\\n• Num. Params: Base = 270M, Large = 550M\\n• Corpus: Cleaned Common Crawl in 100 languages\\n• Lab: Facebook\\n2.5.60 XLNet\\n• Reference:82[65]\\n• Family: Transformer XL\\n• Pretraining Architecture: Decoder\\n• Pretraining Task: PLM\\n• Extension: This model basically adapts Transformer XL architecture to permutation-based LM\\n• Application: General language tasks\\n• Date (of ﬁrst known publication): 05/2019\\n• Num. Params: Base=117M, Large=360M\\n• Corpus: Same as BERT + Giga5 (16GB text) + and aggressively ﬁltered ClueWeb 2012-B (19GB), Common\\nCrawl (110 GB)\\n• Lab: CMU/Google\\n3 Further reading\\nMost of the following references have already been mentioned in the post. However, it is worth listing them here in\\ncase you need more details:\\n• The Huggingface Transformers documentation83 and course is extremely good and comprehensive. I have\\nused myself in this post, and I can’t recommend enough as a natural follow up to what you will ﬁnd here.\\n• A survey of transformers 84[66] includes a 40 page long survey wit over 170 references and a full blown\\ntaxonomy.\\n• Pre-trained Models for Natural Language Processing: A Survey[10] is also a very comprehensive survey that\\nincludes many of the pretrained models with a particular focus on NLP\\nReferences\\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n[2] Kyunghyun Cho, Bart Van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural\\nmachine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\\n81https://huggingface.co/docs/transformers/model_doc/xlm-roberta\\n82https://huggingface.co/docs/transformers/model_doc/xlnet\\n83https://huggingface.co/course/chapter1/1?fw=pt\\n84https://arxiv.org/abs/2106.04554\\n31'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 31}, page_content='A PREPRINT - FEBRUARY 16, 2023\\nFigure 9: Transformer taxonomy from [10]\\n32'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 32}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n[3] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997.\\n[4] Tomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent neural network\\nbased language model. In Interspeech, volume 2, pages 1045–1048. Makuhari, 2010.\\n[5] Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak Shah.\\nTransformers in vision: A survey. ACM computing surveys (CSUR), 54(10s):1–41, 2022.\\n[6] David Noever, Matt Ciolino, and Josh Kalin. The chess transformer: Mastering play using generative language\\nmodels. arXiv preprint arXiv:2008.04057, 2020.\\n[7] Kimia Noorbakhsh, Modar Sulaiman, Mahdi Shariﬁ, Kallol Roy, and Pooyan Jamshidi. Pretrained language\\nmodels are symbolic mathematics solvers too! arXiv preprint arXiv:2110.03501, 2021.\\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Nee-\\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances\\nin neural information processing systems, 33:1877–1901, 2020.\\n[9] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang,\\nBin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. arXiv\\npreprint arXiv:2209.00796, 2022.\\n[10] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for\\nnatural language processing: A survey. Science China Technological Sciences, 63(10):1872–1897, 2020.\\n[11] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A\\nlite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\\n[12] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn\\nTunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure\\nprediction with alphafold. Nature, 596(7873):583–589, 2021.\\n[13] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav\\nFort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning\\nfrom human feedback. arXiv preprint arXiv:2204.05862, 2022.\\n[14] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas\\nJoseph, Ben Mann, Nova DasSarma, et al. A general language assistant as a laboratory for alignment. arXiv\\npreprint arXiv:2112.00861, 2021.\\n[15] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\\nStoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n[17] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in\\nneural information processing systems, 33:17283–17297, 2020.\\n[18] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen,\\nKushal Arora, Joshua Lane, et al. Blenderbot 3: a deployed conversational agent that continually learns to\\nresponsibly engage. arXiv preprint arXiv:2208.03188, 2022.\\n[19] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language\\nmodels. arXiv preprint arXiv:2203.15556, 2022.\\n[20] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\\nsupervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.\\n[21] Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko,\\nMandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multimodal model of the internet. arXiv\\npreprint arXiv:2201.07520, 2022.\\n[22] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional\\ntransformer language model for controllable generation. arXiv preprint arXiv:1909.05858, 2019.\\n[23] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\\nSutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–\\n8831. PMLR, 2021.\\n33'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 33}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n[25] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind\\nSrinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in\\nneural information processing systems, 34:15084–15097, 2021.\\n[26] Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu,\\nand Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv\\npreprint arXiv:1911.00536, 2019.\\n[27] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,\\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\\n[28] Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew Arnold, Bing Xiang, and Dan\\nRoth. Dq-bart: Efﬁcient sequence-to-sequence model via joint distillation and quantization. arXiv preprint\\narXiv:2203.11239, 2022.\\n[29] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders as\\ndiscriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\\n[30] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language\\nrepresentation with informative entities. arXiv preprint arXiv:1905.07129, 2019.\\n[31] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\\nMensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning.\\narXiv preprint arXiv:2204.14198, 2022.\\n[32] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-\\nMaron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.arXiv preprint\\narXiv:2205.06175, 2022.\\n[33] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi\\nZhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efﬁcient scaling of language models with mixture-of-experts. In\\nInternational Conference on Machine Learning, pages 5547–5569. PMLR, 2022.\\n[34] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\\narXiv preprint arXiv:2112.10741, 2021.\\n[35] Ali Hatamizadeh, Hongxu Yin, Jan Kautz, and Pavlo Molchanov. Global context vision transformers. arXiv\\npreprint arXiv:2206.09959, 2022.\\n[36] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides,\\nSarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights\\nfrom training gopher. arXiv preprint arXiv:2112.11446, 2021.\\n[37] Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese,\\nSusannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support\\nanswers with veriﬁed quotes. arXiv preprint arXiv:2203.11147, 2022.\\n[38] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\\ngenerative pre-training. 2018.\\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\\n[40] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\\nAgarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.\\narXiv preprint arXiv:2203.02155, 2022.\\n[41] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor\\nLeahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv\\npreprint arXiv:2204.06745, 2022.\\n[42] Armen Aghajanyan, Dmytro Okhonko, Mike Lewis, Mandar Joshi, Hu Xu, Gargi Ghosh, and Luke Zettlemoyer.\\nHtlm: Hyper-text pre-training and prompting of language models. arXiv preprint arXiv:2107.06955, 2021.\\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image\\ndiffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\\n34'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 34}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n[44] Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White\\nPaper. AI21 Labs, 1, 2021.\\n[45] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint\\narXiv:2201.08239, 2022.\\n[46] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke\\nZettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association\\nfor Computational Linguistics, 8:726–742, 2020.\\n[47] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\\nMegatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint\\narXiv:1909.08053, 2019.\\n[48] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,\\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems\\nwith language models. arXiv preprint arXiv:2206.14858, 2022.\\n[49] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun\\nLiu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train\\nmegatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022.\\n[50] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\\nMona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint\\narXiv:2205.01068, 2022.\\n[51] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\\npathways. arXiv preprint arXiv:2204.02311, 2022.\\n[52] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. Pegasus: Pre-training with extracted gap-sentences\\nfor abstractive summarization. In International Conference on Machine Learning, pages 11328–11339. PMLR,\\n2020.\\n[53] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\\narXiv:1907.11692, 2019.\\n[54] Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. Language\\nmodels that seek for knowledge: Modular search & generation for dialogue and prompt completion.arXiv preprint\\narXiv:2203.13224, 2022.\\n[55] Amelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura\\nWeidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dialogue agents via targeted human\\njudgements. arXiv preprint arXiv:2209.14375, 2022.\\n[56] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, pages 10684–10695, 2022.\\n[57] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\\ntransformer: Hierarchical vision transformer using shifted windows. InProceedings of the IEEE/CVF international\\nconference on computer vision, pages 10012–10022, 2021.\\n[58] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with\\nsimple and efﬁcient sparsity. J. Mach. Learn. Res, 23:1–40, 2021.\\n[59] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. The Journal of\\nMachine Learning Research, 21(1):5485–5551, 2020.\\n[60] Michael Janner, Qiyang Li, and Sergey Levine. Ofﬂine reinforcement learning as one big sequence modeling\\nproblem. Advances in neural information processing systems, 34:1273–1286, 2021.\\n[61] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl:\\nAttentive language models beyond a ﬁxed-length context. arXiv preprint arXiv:1901.02860, 2019.\\n[62] Corby Rosset. Turing-nlg: A 17-billion-parameter language model by microsoft. Microsoft Blog, 1(2), 2020.\\n35'),\n",
              " Document(metadata={'source': 'research_papers/Transformer models an introduction and catalog.pdf', 'page': 35}, page_content='A PREPRINT - FEBRUARY 16, 2023\\n[63] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n[64] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán,\\nEdouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation\\nlearning at scale. arXiv preprint arXiv:1911.02116, 2019.\\n[65] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\\nGeneralized autoregressive pretraining for language understanding. Advances in neural information processing\\nsystems, 32, 2019.\\n[66] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022.\\n36'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 0}, page_content='Speech and Language Processing. Daniel Jurafsky & James H. Martin. Copyright © 2024. All\\nrights reserved. Draft of August 20, 2024.\\nCHAPTER\\n9\\nThe Transformer\\n“The true art of memory is the art of attention ”\\nSamuel Johnson, Idler #74, September 1759\\nIn this chapter we introduce thetransformer, the standard architecture for build-\\ning large language models. Transformer-based large language models have com-\\npletely changed the ﬁeld of speech and language processing. Indeed, every subse-\\nquent chapter in this textbook will make use of them. We’ll focus for now on left-\\nto-right (sometimes called causal or autoregressive) language modeling, in which\\nwe are given a sequence of input tokens and predict output tokens one by one by\\nconditioning on the prior context.\\nThe transformer is a neural network with a speciﬁc structure that includes a\\nmechanism called self-attention or multi-head attention.1 Attention can be thought\\nof as a way to build contextual representations of a token’s meaning byattending to\\nand integrating information from surrounding tokens, helping the model learn how\\ntokens relate to each other over large spans.\\nStacked\\nTransformer\\nBlocks\\nSo long and thanks for\\nlong and thanks forNext token all\\n…\\n…\\n…\\nU\\nInput tokens\\nx1 x2\\nLanguage\\nModeling\\nHead\\nx3 x4 x5\\nInput\\nEncoding\\n E\\n1+\\nE\\n2+\\nE\\n3+\\nE\\n4+\\nE\\n5+\\n…\\n… ………\\nU\\n U\\n U\\n U\\n…\\nlogits logits logits logits logits\\nFigure 9.1 The architecture of a (left-to-right) transformer, showing how each input token\\nget encoded, passed through a set of stacked transformer blocks, and then a language model\\nhead that predicts the next token.\\nFig. 9.1 sketches the transformer architecture. A transformer has three major\\ncomponents. At the center are columns of transformer blocks. Each block is a\\nmultilayer network (a multi-head attention layer, feedforward networks and layer\\nnormalization steps) that maps an input vectorxi in column i (corresponding to input\\n1 Although multi-head attention developed historically from theRNN attention mechanism (Chapter 8),\\nwe’ll deﬁne attention from scratch here for readers who haven’t yet read Chapter 8.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 1}, page_content='2 CHAPTER 9 • T HE TRANSFORMER\\ntoken i) to an output vector hi. The set of n blocks maps an entire context window\\nof input vectors (x1,..., xn) to a window of output vectors (h1,..., hn) of the same\\nlength. A column might contain from 12 to 96 or more stacked blocks.\\nThe column of blocks is preceded by theinput encoding component, which pro-\\ncesses an input token (like the wordthanks) into a contextual vector representation,\\nusing an embedding matrix E and a mechanism for encoding token position. Each\\ncolumn is followed by a language modeling head, which takes the embedding out-\\nput by the ﬁnal transformer block, passes it through anunembedding matrix U and\\na softmax over the vocabulary to generate a single token for that column.\\nTransformer-based language models are complex, and so the details will unfold\\nover the next 5 chapters. In the next sections we’ll introduce multi-head attention,\\nthe rest of the transformer block, and the input encoding and language modeling\\nhead components. Chapter 10 discusses how language models are pretrained, and\\nhow tokens are generated via sampling. Chapter 11 introduces masked language\\nmodeling and the BERT family of bidirectional transformer encoder models. Chap-\\nter 12 shows how toprompt LLMs to perform NLP tasks by giving instructions and\\ndemonstrations, and how to align the model with human preferences. Chapter 13\\nwill introduce machine translation with the encoder-decoder architecture.\\n9.1 Attention\\nRecall from Chapter 6 that for word2vec and other static embeddings, the repre-\\nsentation of a word’s meaning is always the same vector irrespective of the context:\\nthe word chicken, for example, is always represented by the same ﬁxed vector. So\\na static vector for the word it might somehow encode that this is a pronoun used\\nfor animals and inanimate entities. But in context it has a much richer meaning.\\nConsider it in one of these two sentences:\\n(9.1) The chicken didn’t cross the road becauseit was too tired.\\n(9.2) The chicken didn’t cross the road becauseit was too wide.\\nIn (9.1) it is the chicken (i.e., the reader knows that the chicken was tired), while\\nin (9.2) it is the road (and the reader knows that the road was wide). 2 That is, if\\nwe are to compute the meaning of this sentence, we’ll need the meaning ofit to be\\nassociated with the chickenin the ﬁrst sentence and associated with the roadin\\nthe second one, sensitive to the context.\\nFurthermore, consider reading left to right like a causal language model, pro-\\ncessing the sentence up to the word it:\\n(9.3) The chicken didn’t cross the road becauseit\\nAt this point we don’t yet know which thingit is going to end up referring to! So a\\nrepresentation of it at this point might have aspects of both chicken and road as\\nthe reader is trying to guess what happens next.\\nThis fact that words have rich linguistic relationships with other words that may\\nbe far away pervades language. Consider two more examples:\\n(9.4) The keys to the cabinet are on the table.\\n(9.5) I walked along the pond, and noticed one of the trees along the bank.\\n2 We say that in the ﬁrst example it corefers with the chicken, and in the second it corefers with the\\nroad; we’ll return to this in Chapter 23.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 2}, page_content='9.1 • A TTENTION 3\\nIn (9.4), the phrase The keys is the subject of the sentence, and in English and many\\nlanguages, must agree in grammatical number with the verbare; in this case both are\\nplural. In English we can’t use a singular verb like is with a plural subject like keys\\n(we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers\\nto the side of a pond or river and not a ﬁnancial institution because of the context,\\nincluding words like pond. (We’ll discuss word senses more in Chapter 11.)\\nThe point of all these examples is that these contextual words that help us com-\\npute the meaning of words in context can be quite far away in the sentence or para-\\ngraph. Transformers can build contextual representations of word meaning, contex-\\ntual embeddings, by integrating the meaning of these helpful contextual words. In acontextual\\nembeddings\\ntransformer, layer by layer, we build up richer and richer contextualized representa-\\ntions of the meanings of input tokens. At each layer, we compute the representation\\nof a token i by combining information about i from the previous layer with infor-\\nmation about the neighboring tokens to produce a contextualized representation for\\neach word at each position.\\nAttention is the mechanism in the transformer that weighs and combines the\\nrepresentations from appropriate other tokens in the context from layerk−1 to build\\nthe representation for tokens in layer k.\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nThe\\nchicken\\ndidn’t\\ncross\\nthe\\nroad\\nbecause\\nit\\nwas\\ntoo\\ntired\\nLayer k+1\\nLayer k\\nself-attention distribution\\ncolumns corresponding to input tokens\\nFigure 9.2 The self-attention weight distribution α that is part of the computation of the\\nrepresentation for the word it at layer k +1. In computing the representation for it, we attend\\ndifferently to the various words at layer l, with darker shades indicating higher self-attention\\nvalues. Note that the transformer is attending highly to the columns corresponding to the\\ntokens chicken and road , a sensible result, since at the point whereit occurs, it could plausibly\\ncorefers with the chicken or the road, and hence we’d like the representation forit to draw on\\nthe representation for these earlier words. Figure adapted from Uszkoreit (2017).\\nFig. 9.2 shows a schematic example simpliﬁed from a transformer (Uszkoreit,\\n2017). The ﬁgure describes the situation when the current token is it and we need\\nto compute a contextual representation for this token at layerk+1 of the transformer,\\ndrawing on the representations (from layer k) of every prior token. The ﬁgure uses\\ncolor to represent the attention distribution over the contextual words: the tokens\\nchicken and road both have a high attention weight, meaning that as we are com-\\nputing the representation for it, we will draw most heavily on the representation for\\nchicken and road. This will be useful in building the ﬁnal representation for it,\\nsince it will end up coreferring with either chicken or road.\\nLet’s now turn to how this attention distribution is represented and computed.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 3}, page_content='4 CHAPTER 9 • T HE TRANSFORMER\\n9.1.1 Attention more formally\\nAs we’ve said, the attention computation is a way to compute a vector representation\\nfor a token at a particular layer of a transformer, by selectively attending to and\\nintegrating information from prior tokens at the previous layer. Attention takes an\\ninput representation xi corresponding to the input token at position i, and a context\\nwindow of prior inputs x1..xi−1, and produces an output ai.\\nIn causal, left-to-right language models, the context is any of the prior words.\\nThat is, when processing xi, the model has access toxi as well as the representations\\nof all the prior tokens in the context window (context windows consist of thousands\\nof tokens) but no tokens afteri. (By contrast, in Chapter 11 we’ll generalize attention\\nso it can also look ahead to future words.)\\nFig. 9.3 illustrates this ﬂow of information in an entire causal self-attention layer,\\nin which this same attention computation happens in parallel at each token position\\ni. Thus a self-attention layer maps input sequences (x1,..., xn) to output sequences\\nof the same length (a1,..., an).\\nattentionattentionSelf-Attention\\nLayer\\nattentionattentionattention\\na1 a2 a3 a4 a5\\nx3 x4 x5x1 x2\\nFigure 9.3 Information ﬂow in causal self-attention. When processing each input xi, the\\nmodel attends to all the inputs up to, and including xi.\\nSimpliﬁed version of attention At its heart, attention is really just a weighted\\nsum of context vectors, with a lot of complications added to how the weights are\\ncomputed and what gets summed. For pedagogical purposes let’s ﬁrst describe a\\nsimpliﬁed intuition of attention, in which the attention output ai at token position i\\nis simply the weighted sum of all the representations xj, for all j ≤i; we’ll use αi j\\nto mean how much xi should contribute to aj:\\nSimpliﬁed version: ai =\\n∑\\nj≤i\\nαi jxj (9.6)\\nEach αi j is a scalar used for weighing the value of input xj when summing up\\nthe inputs to compute ai. How shall we compute this α weighting? In attention we\\nweight each prior embedding proportionally to howsimilar it is to the current token\\ni. So the output of attention is a sum of the embeddings of prior tokens weighted\\nby their similarity with the current token embedding. We compute similarity scores\\nvia dot product, which maps two vectors into a scalar value ranging from −∞ to\\n∞. The larger the score, the more similar the vectors that are being compared. We’ll\\nnormalize these scores with a softmax to create the vector of weights αi j, j ≤i.\\nSimpliﬁed Version: score(xi,xj) = xi ·xj (9.7)\\nαi j = softmax(score(xi,xj)) ∀j ≤i (9.8)\\nThus in Fig. 9.3 we computea3 by computing three scores: x3 ·x1, x3 ·x2 and x3 ·x3,\\nnormalizing them by a softmax, and using the resulting probabilities as weights\\nindicating each of their proportional relevance to the current position i. Of course,'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 4}, page_content='9.1 • A TTENTION 5\\nthe softmax weight will likely be highest for xi, since xi is very similar to itself,\\nresulting in a high dot product. But other context words may also be similar toi, and\\nthe softmax will also assign some weight to those words. Then we use these weights\\nas the α values in Eq. 9.6 to compute the weighted sum that is our a3.\\nThe simpliﬁed attention in equations 9.6 – 9.8 demonstrates the attention-based\\napproach to computing ai: compare the xi to prior vectors, normalize those scores\\ninto a probability distribution used to weight the sum of the prior vector. But now\\nwe’re ready to remove the simpliﬁcations.\\nA single attention head using query, key, and value matrices Now that we’ve\\nseen a simple intuition of attention, let’s introduce the actual attention head, theattention head\\nversion of attention that’s used in transformers. (The word head is often used inhead\\ntransformers to refer to speciﬁc structured layers). The attention head allows us to\\ndistinctly represent three different roles that each input embedding plays during the\\ncourse of the attention process:\\n• As the current element being compared to the preceding inputs. We’ll refer to\\nthis role as a query.query\\n• In its role as a preceding input that is being compared to the current element\\nto determine a similarity weight. We’ll refer to this role as akey.key\\n• And ﬁnally, as a value of a preceding element that gets weighted and summedvalue\\nup to compute the output for the current element.\\nTo capture these three different roles, transformers introduce weight matrices\\nWQ, WK, and WV. These weights will project each input vector xi into a represen-\\ntation of its role as a key, query, or value:\\nqi = xiWQ; ki = xiWK; vi = xiWV (9.9)\\nGiven these projections, when we are computing the similarity of the current ele-\\nment xi with some prior element xj, we’ll use the dot product between the current\\nelement’squery vector qi and the preceding element’skey vector kj. Furthermore,\\nthe result of a dot product can be an arbitrarily large (positive or negative) value, and\\nexponentiating large values can lead to numerical issues and loss of gradients during\\ntraining. To avoid this, we scale the dot product by a factor related to the size of the\\nembeddings, via diving by the square root of the dimensionality of the query and\\nkey vectors (dk). We thus replace the simpliﬁed Eq. 9.7 with Eq. 9.11. The ensuing\\nsoftmax calculation resulting in αi j remains the same, but the output calculation for\\nai is now based on a weighted sum over the value vectors v (Eq. 9.13).\\nHere’s a ﬁnal set of equations for computing self-attention for a single self-\\nattention output vector ai from a single input vector xi. This version of attention\\ncomputes ai by summing the values of the prior elements, each weighted by the\\nsimilarity of its key to the query from the current element:\\nqi = xiWQ; kj = xjWK; vj = xjWV (9.10)\\nscore(xi,xj) = qi ·kj√dk\\n(9.11)\\nαi j = softmax(score(xi,xj)) ∀j ≤i (9.12)\\nai =\\n∑\\nj≤i\\nαi jvj (9.13)\\nWe illustrate this in Fig. 9.4 for the case of calculating the value of the third output\\na3 in a sequence.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 5}, page_content='6 CHAPTER 9 • T HE TRANSFORMER\\n6. Sum the weighted \\nvalue vectors\\n4. Turn into 𝛼i,j weights via softmax\\na3\\n1. Generate \\nkey, query, value \\nvectors\\n2. Compare x3’s query with\\nthe keys for x1, x2, and x3\\nOutput of self-attention\\nWk\\nWv\\nWq\\nx1\\nk\\nq\\nv x3\\nk\\nq\\nvx2\\nk\\nq\\nv\\n×\\n×\\nWk Wk\\nWq Wq\\nWvWv\\n5. Weigh each value vector\\n÷\\n√dk\\n3. Divide score by √dk\\n÷\\n√dk\\n÷\\n√dk\\n𝛼3,1 𝛼3,2 𝛼3,3\\nFigure 9.4 Calculating the value of a3, the third element of a sequence using causal (left-\\nto-right) self-attention.\\nLet’s talk shapes. The input to attention xi and the output from attention ai both\\nhave the same dimensionality 1 ×d (We often call d the model dimensionality,\\nand indeed as we’ll discuss in Section 9.2 the output hi of each transformer block,\\nas well as the intermediate vectors inside the transformer block also have the same\\ndimensionality 1 ×d.).\\nWe’ll have a dimensiondk for the key and query vectors. The query vector and\\nthe key vector are both dimensionality 1×dk, so we can take their dot productqi ·kj.\\nWe’ll have a separate dimensiondv for the value vectors. The transform matrix WQ\\nhas shape [d ×dk], WK is [d ×dk], and WV is [d ×dv]. In the original transformer\\nwork (Vaswani et al., 2017),d was 512, dk and dv were both 64.\\nMulti-head Attention Equations 9.11-9.13 describe a single attention head. But\\nactually, transformers use multiple attention heads. The intuition is that each head\\nmight be attending to the context for different purposes: heads might be special-\\nized to represent different linguistic relationships between context elements and the\\ncurrent token, or to look for particular kinds of patterns in the context.\\nSo in multi-head attention we have h separate attention heads that reside inmulti-head\\nattention\\nparallel layers at the same depth in a model, each with its own set of parameters that\\nallows the head to model different aspects of the relationships among inputs. Thus\\neach head i in a self-attention layer has its own set of key, query and value matrices:\\nWKi, WQi and WVi. These are used to project the inputs into separate key, value,\\nand query embeddings for each head.\\nWhen using multiple heads the model dimension d is still used for the input\\nand output, the key and query embeddings have dimensionality dk, and the value\\nembeddings are of dimensionality dv (again, in the original transformer paper dk =\\ndv = 64, h = 8, and d = 512). Thus for each head i, we have weight layers WQi of\\nshape [d ×dk], WKi of shape [d ×dk], and WVi of shape [d ×dv].\\nBelow are the equations for attention augmented with multiple heads; Fig. 9.5'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 6}, page_content='9.2 • T RANSFORMER BLOCKS 7\\nshows an intuition.\\nqc\\ni = xiWQc; kc\\nj = xjWKc; vc\\nj = xjWVc; ∀c 1 ≤c ≤h (9.14)\\nscorec(xi,xj) =\\nqc\\ni ·kc\\nj√dk\\n(9.15)\\nαc\\ni j = softmax(scorec(xi,xj)) ∀j ≤i (9.16)\\nheadc\\ni =\\n∑\\nj≤i\\nαc\\ni jvc\\nj (9.17)\\nai = (head1 ⊕head2... ⊕headh)WO (9.18)\\nMultiHeadAttention(xi,[x1,···,xN ]) = ai (9.19)\\nThe output of each of the h heads is of shape 1 ×dv, and so the output of the\\nmulti-head layer with h heads consists of h vectors of shape 1 ×dv. These are con-\\ncatenated to produce a single output with dimensionality 1 ×hdv. Then we use yet\\nanother linear projection WO ∈Rhdv×d to reshape it, resulting in the multi-head\\nattention vector ai with the correct output shape [1xd] at each input i.\\nai\\nxi-1 xixi-2xi-3\\nWK\\n1\\nHead 1\\nWV\\n1 WQ\\n1\\n…\\n…\\nWK\\n2\\nHead 2\\nWV\\n2 WQ\\n2 WK\\n8\\nHead 8\\nWV\\n8 WQ\\n8\\nai\\nWO  [hdv x d]\\n[1 x dv ]\\n[1 x d]\\n[1 x d]\\n[1 x hdv ]\\nProject down to d\\nConcatenate Outputs\\nEach head\\nattends diﬀerently\\nto context\\n…\\n[1 x dv ]\\nFigure 9.5 The multi-head attention computation for input xi, producing output ai. A multi-head attention\\nlayer has h heads, each with its own key, query and value weight matrices. The outputs from each of the heads\\nare concatenated and then projected down to d, thus producing an output of the same size as the input.\\n9.2 Transformer Blocks\\nThe self-attention calculation lies at the core of what’s called a transformer block,\\nwhich, in addition to the self-attention layer, includes three other kinds of layers: (1)\\na feedforward layer, (2) residual connections, and (3) normalizing layers (colloqui-\\nally called “layer norm”).\\nFig. 9.6 illustrates a transformer block, sketching a common way of thinking\\nabout the block that is called the residual stream (Elhage et al., 2021). In the resid-residual stream\\nual stream viewpoint, we consider the processing of an individual token i through\\nthe transformer block as a single stream of d-dimensional representations for token\\nposition i. This residual stream starts with the original input vector, and the various'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 7}, page_content='8 CHAPTER 9 • T HE TRANSFORMER\\nLayer Norm\\nxi\\n+\\nhi-1\\nLayer Norm\\nMultiHead\\nAttention\\nFeedforward\\nxi-1 xi+1\\nhi hi+1\\n+\\n……\\nResidual\\nStream\\nFigure 9.6 The architecture of a transformer block showing the residual stream. This\\nﬁgure shows the prenorm version of the architecture, in which the layer norms happen before\\nthe attention and feedforward layers rather than after.\\ncomponents read their input from the residual stream and add their output back into\\nthe stream.\\nThe input at the bottom of the stream is an embedding for a token, which has\\ndimensionality d. This initial embedding gets passed up (by residual connections),\\nand is progressively added to by the other components of the transformer: the at-\\ntention layer that we have seen, and the feedforward layer that we will introduce.\\nBefore the attention and feedforward layer is a computation called the layer norm.\\nThus the initial vector is passed through a layer norm and attention layer, and\\nthe result is added back into the stream, in this case to the original input vector\\nxi. And then this summed vector is again passed through another layer norm and a\\nfeedforward layer, and the output of those is added back into the residual, and we’ll\\nuse hi to refer to the resulting output of the transformer block for token i. (In earlier\\ndescriptions the residual stream was often described using a different metaphor as\\nresidual connections that add the input of a component to its output, but the residual\\nstream is a more perspicuous way of visualizing the transformer.)\\nWe’ve already seen the attention layer, so let’s now introduce the feedforward\\nand layer norm computations in the context of processing a single input xi at token\\nposition i.\\nFeedforward layer The feedforward layer is a fully-connected 2-layer network,\\ni.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights\\nare the same for each token position i , but are different from layer to layer. It\\nis common to make the dimensionality dff of the hidden layer of the feedforward\\nnetwork be larger than the model dimensionality d. (For example in the original\\ntransformer model, d = 512 and dff = 2048.)\\nFFN(xi) =ReLU(xiW1 +b1)W2 +b2 (9.20)\\nLayer Norm At two stages in the transformer block we normalize the vector (Ba\\net al., 2016). This process, called layer norm (short for layer normalization), is onelayer norm'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 8}, page_content='9.2 • T RANSFORMER BLOCKS 9\\nof many forms of normalization that can be used to improve training performance\\nin deep neural networks by keeping the values of a hidden layer in a range that\\nfacilitates gradient-based training.\\nLayer norm is a variation of the z-score from statistics, applied to a single vec-\\ntor in a hidden layer. That is, the term layer norm is a bit confusing; layer norm\\nis not applied to an entire transformer layer, but just to the embedding vector of a\\nsingle token. Thus the input to layer norm is a single vector of dimensionality d\\nand the output is that vector normalized, again of dimensionality d. The ﬁrst step in\\nlayer normalization is to calculate the mean, µ, and standard deviation, σ, over the\\nelements of the vector to be normalized. Given an embedding vector x of dimen-\\nsionality d, these values are calculated as follows.\\nµ = 1\\nd\\nd∑\\ni=1\\nxi (9.21)\\nσ =\\n\\ued6a\\ued6b\\ued6b√1\\nd\\nd∑\\ni=1\\n(xi −µ)2 (9.22)\\nGiven these values, the vector components are normalized by subtracting the mean\\nfrom each and dividing by the standard deviation. The result of this computation is\\na new vector with zero mean and a standard deviation of one.\\nˆ x= (x−µ)\\nσ (9.23)\\nFinally, in the standard implementation of layer normalization, two learnable param-\\neters, γ and β, representing gain and offset values, are introduced.\\nLayerNorm(x) =γ (x−µ)\\nσ +β (9.24)\\nPutting it all together The function computed by a transformer block can be ex-\\npressed by breaking it down with one equation for each component computation,\\nusing t (of shape [1 ×d]) to stand for transformer and superscripts to demarcate\\neach computation inside the block:\\nt1\\ni = LayerNorm(xi) (9.25)\\nt2\\ni = MultiHeadAttention(t1\\ni ,\\n[\\nx1\\n1,···,x1\\nN\\n]\\n) (9.26)\\nt3\\ni = t2\\ni +xi (9.27)\\nt4\\ni = LayerNorm(t3\\ni ) (9.28)\\nt5\\ni = FFN(t4\\ni ) (9.29)\\nhi = t5\\ni +t3\\ni (9.30)\\nNotice that the only component that takes as input information from other tokens\\n(other residual streams) is multi-head attention, which (as we see from (9.27)) looks\\nat all the neighboring tokens in the context. The output from attention, however, is\\nthen added into this token’s embedding stream. In fact, Elhage et al. (2021) show that\\nwe can view attention heads as literally moving information from the residual stream\\nof a neighboring token into the current stream. The high-dimensional embedding\\nspace at each position thus contains information about the current token and about\\nneighboring tokens, albeit in different subspaces of the vector space. Fig. 9.7 shows\\na visualization of this movement.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 9}, page_content='10 CHAPTER 9 • T HE TRANSFORMER\\nToken A\\nresidual\\n stream\\nToken B\\nresidual \\nstream\\nFigure 9.7 An attention head can move information from token A’s residual stream into\\ntoken B’s residual stream.\\nCrucially, the input and output dimensions of transformer blocks are matched so\\nthey can be stacked. Each token vectorxi at the input to the block has dimensionality\\nd, and the output hi also has dimensionality d. Transformers for large language\\nmodels stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small\\nlanguage models) to 96 layers (used for GPT-3 large), to even more for more recent\\nmodels. We’ll come back to this issue of stacking in a bit.\\nEquation (9.27) and following are just the equation for a single transformer\\nblock, but the residual stream metaphor goes through all the transformer layers,\\nfrom the ﬁrst transformer blocks to the 12th, in a 12-layer transformer. At the ear-\\nlier transformer blocks, the residual stream is representing the current token. At the\\nhighest transformer blocks, the residual stream is usually representing the following\\ntoken, since at the very end it’s being trained to predict the next token.\\nOnce we stack many blocks, there is one more requirement: at the very end of\\nthe last (highest) transformer block, there is a single extra layer norm that is run on\\nthe last hi of each token stream (just below the language model head layer that we\\nwill deﬁne soon). 3\\n9.3 Parallelizing computation using a single matrix X\\nThis description of multi-head attention and the rest of the transformer block has\\nbeen from the perspective of computing a single output at a single time step i in\\na single residual stream. But as we pointed out earlier, the attention computation\\nperformed for each token to compute ai is independent of the computation for each\\nother token, and that’s also true for all the computation in the transformer block\\ncomputing hi from the input xi. That means we can easily parallelize the entire\\ncomputation, taking advantage of efﬁcient matrix multiplication routines.\\nWe do this by packing the input embeddings for the N tokens of the input se-\\nquence into a single matrix X of size [N ×d]. Each row of X is the embedding of\\none token of the input. Transformers for large language models commonly have an\\ninput length N = 1K, 2K, or as many as 32K tokens (or more), soX typically has be-\\ntween 1K and 32K rows, each of the dimensionality of the embedding d (the model\\n3 Note that we are using the most common current transformer architecture, which is called theprenorm\\narchitecture. The original deﬁnition of the transformer in Vaswani et al. (2017) used an alternative archi-\\ntecture called the postnorm transformer in which the layer norm happens after the attention and FFN\\nlayers; it turns out moving the layer norm beforehand works better, but does require this one extra layer\\nat the end.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 10}, page_content='9.3 • P ARALLELIZING COMPUTATION USING A SINGLE MATRIX X 11\\ndimension).\\nParallelizing attention Let’s ﬁrst see this for a single attention head and then turn\\nto multiple heads, and then add in the rest of the components in the transformer\\nblock. For one head we multiply X by the key, query, and value matrices WQ of\\nshape [d ×dk], WK of shape [d ×dk], and WV of shape [d ×dv], to produce matrices\\nQ of shape [N ×dk], K ∈RN×dk , and V ∈RN×dv , containing all the key, query, and\\nvalue vectors:\\nQ = XWQ; K = XWK; V = XWV (9.31)\\nGiven these matrices we can compute all the requisite query-key comparisons simul-\\ntaneously by multiplying Q and K⊺ in a single matrix multiplication. The product is\\nof shape N ×N, visualized in Fig. 9.8.\\nq1•k1\\nq2•k1 q2•k2\\nq4•k1 q4•k2 q4•k3 q4•k4\\nq3•k1 q3•k2 q3•k3\\nN\\nN\\nq1•k2 q1•k3 q1•k4\\nq2•k3 q2•k4\\nq3•k4\\nFigure 9.8 The N ×N QK⊺ matrix showing how it computes all qi ·kj comparisons in a\\nsingle matrix multiple.\\nOnce we have this QK⊺ matrix, we can very efﬁciently scale these scores, take\\nthe softmax, and then multiply the result by V resulting in a matrix of shape N ×d:\\na vector embedding representation for each token in the input. We’ve reduced the\\nentire self-attention step for an entire sequence of N tokens for one head to the\\nfollowing computation:\\nA = softmax\\n(\\nmask\\n(QK⊺\\n√dk\\n))\\nV (9.32)\\nMasking out the future You may have noticed that we introduced a mask function\\nin Eq. 9.32 above. This is because the self-attention computation as we’ve described\\nit has a problem: the calculation in QK⊺ results in a score for each query value\\nto every key value, including those that follow the query . This is inappropriate in\\nthe setting of language modeling: guessing the next word is pretty simple if you\\nalready know it! To ﬁx this, the elements in the upper-triangular portion of the\\nmatrix are zeroed out (set to −∞), thus eliminating any knowledge of words that\\nfollow in the sequence. This is done in practice by adding a mask matrix M in\\nwhich Mi j = −∞ ∀j > i (i.e. for the upper-triangular portion) andMi j = 0 otherwise.\\nFig. 9.9 shows the resulting masked QK⊺ matrix. (we’ll see in Chapter 11 how to\\nmake use of words in the future for tasks that need it).\\nFig. 9.10 shows a schematic of all the computations for a single attention head\\nparallelized in matrix form.\\nFig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length\\nof the input, since at each layer we need to compute dot products between each pair\\nof tokens in the input. This makes it expensive to compute attention over very long\\ndocuments (like entire novels). Nonetheless modern large language models manage\\nto use quite long contexts of thousands or tens of thousands of tokens.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 11}, page_content='12 CHAPTER 9 • T HE TRANSFORMER\\nq1•k1\\nq2•k1 q2•k2\\nq4•k1 q4•k2 q4•k3 q4•k4\\nq3•k1 q3•k2 q3•k3\\nN\\nN\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞\\nFigure 9.9 The N ×N QK⊺ matrix showing the qi ·kj values, with the upper-triangle por-\\ntion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero).\\nq1\\nq2\\nq3\\nq4\\nk1\\nk2\\nk3\\nk4\\nQ KT\\nQKT\\nv1\\nv2\\nv3\\nv4\\nV\\nq2•k2\\nq4•k2 q4•k3 q4•k4\\nq3•k2 q3•k3\\n−∞ −∞\\n−∞ −∞\\n−∞\\n−∞q1•k1\\nq2•k1 q2•k2\\nq4•k1 q4•k2 q4•k3 q4•k4\\nq3•k1 q3•k2 q3•k3\\nq1•k2\\nq2•k3\\nq1•k3\\nq3•k4\\nq2•k4\\nq1•k4x =\\nQKT masked\\nmask =\\nq1•k1\\nq2•k1\\nq4•k1\\nq3•k1\\nq1•k1q1•k1\\n=x\\na1\\na2\\na3\\na4\\nA\\nQuery \\nToken 1\\nQuery \\nToken 2\\nQuery \\nToken 3\\nQuery \\nToken 4\\nQ\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nx\\nWQ\\n=\\nValue \\nToken 1\\nValue \\nToken 2\\nValue \\nToken 3\\nValue \\nToken 4\\nV\\nx\\nWV\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nKey \\nToken 1\\nKey \\nToken 2\\nKey \\nToken 3\\nKey \\nToken 4\\nK\\nx\\nWK\\n=\\nInput \\nToken 1\\nInput \\nToken 2\\nInput \\nToken 3\\nInput \\nToken 4\\nX\\nN x dk\\ndk x N\\nN x N N x N N x dv N x dv\\nd x dk\\nd x dk d x dv\\nN x d N x dk N x d N x dk N x d N x dv\\nFigure 9.10 Schematic of the attention computation for a single attention head in parallel. The ﬁrst row shows\\nthe computation of the Q, K, and V matrices. The second row shows the computation of QKT, the masking\\n(the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of\\nthe value vectors to get the ﬁnal attention vectors.\\nParallelizing multi-head attention In multi-head attention, as with self-attention,\\nthe input and output have the model dimension d, the key and query embeddings\\nhave dimensionality dk, and the value embeddings are of dimensionality dv (again,\\nin the original transformer paper dk = dv = 64, h = 8, and d = 512). Thus for each\\nhead i, we have weight layers WQi ∈Rd×dk , WKi ∈Rd×dk , and WVi ∈Rd×dv , and\\nthese get multiplied by the inputs packed into X to produce Q ∈RN×dk , K ∈RN×dk ,\\nand V ∈RN×dv . The output of each of the h heads is of shape N ×dv, and so the\\noutput of the multi-head layer withh heads consists ofh matrices of shapeN ×dv. To\\nmake use of these matrices in further processing, they are concatenated to produce\\na single output with dimensionality N ×hdv. Finally, we use yet another linear\\nprojection WO ∈Rhdv×d, that reshape it to the original output dimension for each\\ntoken. Multiplying the concatenated N ×hdv matrix output by WO ∈Rhdv×d yields'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 12}, page_content='9.4 • T HE INPUT : EMBEDDINGS FOR TOKEN AND POSITION 13\\nthe self-attention output A of shape [N ×d].\\nQi = XWQi ; Ki = XWKi ; Vi = XWVi (9.33)\\nheadi = SelfAttention(Qi,Ki,Vi) = softmax\\n(QiKi⊺\\n√dk\\n)\\nVi (9.34)\\nMultiHeadAttention(X) = (head1 ⊕head2... ⊕headh)WO (9.35)\\nPutting it all together with the parallel input matrix X The function computed\\nin parallel by an entire layer of N transformer block over the entire N input tokens\\ncan be expressed as:\\nO = LayerNorm(X+MultiHeadAttention(X)) (9.36)\\nH = LayerNorm(O+FFN(O)) (9.37)\\nOr we can break it down with one equation for each component computation, using\\nT (of shape [N ×d]) to stand for transformer and superscripts to demarcate each\\ncomputation inside the block:\\nT1 = MultiHeadAttention(X) (9.38)\\nT2 = X+T1 (9.39)\\nT3 = LayerNorm(T2) (9.40)\\nT4 = FFN(T3) (9.41)\\nT5 = T4 +T3 (9.42)\\nH = LayerNorm(T5) (9.43)\\nHere when we use a notation like FFN (T3) we mean that the same FFN is applied\\nin parallel to each of the N embedding vectors in the window. Similarly, each of the\\nN tokens is normed in parallel in the LayerNorm. Crucially, the input and output\\ndimensions of transformer blocks are matched so they can be stacked. Since each\\ntoken xi at the input to the block has dimensionality d, that means the input X and\\noutput H are both of shape [N ×d].\\n9.4 The input: embeddings for token and position\\nLet’s talk about where the inputX comes from. Given a sequence of N tokens (N is\\nthe context length in tokens), the matrix X of shape [N ×d] has an embedding forembedding\\neach word in the context. The transformer does this by separately computing two\\nembeddings: an input token embedding, and an input positional embedding.\\nA token embedding, introduced in Chapter 7 and Chapter 8, is a vector of di-\\nmension d that will be our initial representation for the input token. (As we pass\\nvectors up through the transformer layers in the residual stream, this embedding\\nrepresentation will change and grow, incorporating context and playing a different\\nrole depending on the kind of language model we are building.) The set of initial\\nembeddings are stored in the embedding matrix E, which has a row for each of the\\n|V |tokens in the vocabulary. Thus each word is a row vector of d dimensions, and\\nE has shape [|V |×d].\\nGiven an input token string like Thanks for all the we ﬁrst convert the tokens\\ninto vocabulary indices (these were created when we ﬁrst tokenized the input using'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 13}, page_content='14 CHAPTER 9 • T HE TRANSFORMER\\nBPE or SentencePiece). So the representation of thanks for all the might be w =\\n[5,4000,10532,2224]. Next we use indexing to select the corresponding rows from\\nE, (row 5, row 4000, row 10532, row 2224).\\nAnother way to think about selecting token embeddings from the embedding\\nmatrix is to represent tokens as one-hot vectors of shape [1 ×|V |], i.e., with one\\ndimension for each word in the vocabulary. Recall that in a one-hot vector all theone-hot vector\\nelements are 0 except one, the element whose dimension is the word’s index in the\\nvocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,\\nx5 = 1, and xi = 0 ∀i ̸= 5, as shown here:\\n[0 0 0 0 1 0 0 ... 0 0 0 0]\\n1 2 3 4 5 6 7 ... ... |V|\\nMultiplying by a one-hot vector that has only one non-zero elementxi = 1 simply\\nselects out the relevant row vector for wordi, resulting in the embedding for word i,\\nas depicted in Fig. 9.11.\\nE\\n|V|\\nd\\n1\\n|V| d\\n=✕\\n55\\n0 0 0 0 1 0 0 … 0 0 0 0 1\\nFigure 9.11 Selecting the embedding vector for word V5 by multiplying the embedding\\nmatrix E with a one-hot vector with a 1 in index 5.\\nWe can extend this idea to represent the entire token sequence as a matrix of one-\\nhot vectors, one for each of the N positions in the transformer’s context window, as\\nshown in Fig. 9.12.\\nE\\n|V|\\nd\\nd\\nN\\n=✕\\n|V|\\nN\\n0 0 0 0 0 0 0 … 0 0 1 0 \\n0 0 0 0 1 0 0 … 0 0 0 0 \\n1 0 0 0 0 0 0 … 0 0 0 0 \\n0 0 0 0 1 0 0 … 0 0 0 0 \\n…\\nFigure 9.12 Selecting the embedding matrix for the input sequence of token idsW by mul-\\ntiplying a one-hot matrix corresponding to W by the embedding matrix E.\\nThese token embeddings are not position-dependent. To represent the position\\nof each token in the sequence, we combine these token embeddings with positional\\nembeddings speciﬁc to each position in an input sequence.positional\\nembeddings\\nWhere do we get these positional embeddings? The simplest method, called\\nabsolute position, is to start with randomly initialized embeddings correspondingabsolute\\nposition\\nto each possible input position up to some maximum length. For example, just as\\nwe have an embedding for the wordﬁsh, we’ll have an embedding for the position 3.\\nAs with word embeddings, these positional embeddings are learned along with other\\nparameters during training. We can store them in a matrix Epos of shape [1 ×N].\\nTo produce an input embedding that captures positional information, we just\\nadd the word embedding for each input to its corresponding positional embedding.\\nThe individual token and position embeddings are both of size[1×d], so their sum is'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 14}, page_content='9.5 • T HE LANGUAGE MODELING HEAD 15\\nX = Composite\\nEmbeddings\\n(word + position)\\nTransformer Block\\nJanet\\n1\\nwill\\n2\\nback\\n3\\nJanet will back the bill\\nthe\\n4\\nbill\\n5\\n+\\n+\\n+\\n+\\n+\\nPosition\\nEmbeddings\\nWord\\nEmbeddings\\nFigure 9.13 A simple way to model position: add an embedding of the absolute position to\\nthe token embedding to produce a new embedding of the same dimensionality.\\nalso [1×d], This new embedding serves as the input for further processing. Fig. 9.13\\nshows the idea.\\nThe ﬁnal representation of the input, the matrix X, is an [N ×d] matrix in which\\neach row i is the representation of the ith token in the input, computed by adding\\nE[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],\\nthe positional embedding of position i.\\nA potential problem with the simple absolute position embedding approach is\\nthat there will be plenty of training examples for the initial positions in our inputs\\nand correspondingly fewer at the outer length limits. These latter embeddings may\\nbe poorly trained and may not generalize well during testing. An alternative ap-\\nproach to absolute positional embeddings is to choose a static function that maps\\ninteger inputs to real-valued vectors in a way that captures the inherent relation-\\nships among the positions. That is, it captures the fact that position 4 in an input is\\nmore closely related to position 5 than it is to position 17. A combination of sine\\nand cosine functions with differing frequencies was used in the original transformer\\nwork. Even more complex positional embedding methods exist, such as ones that\\nrepresent relative position instead of absolute position, often implemented in the\\nattention mechanism at each layer rather than being added once at the initial input.\\n9.5 The Language Modeling Head\\nThe last component of the transformer we must introduce is thelanguage modeling\\nhead. Here we are using the word head to mean the additional neural circuitry welanguage\\nmodeling head\\nhead add on top of the basic transformer architecture when we apply pretrained trans-\\nformer models to various tasks. The language modeling head is the circuitry we\\nneed to do language modeling.\\nRecall that language models, from the simple n-gram models of Chapter 3 through\\nthe feedforward and RNN language models of Chapter 7 and Chapter 8, are word\\npredictors. Given a context of words, they assign a probability to each possible next\\nword. For example, if the preceding context is “Thanks for all the” and we want to\\nknow how likely the next word is “ﬁsh” we would compute:\\nP(ﬁsh|Thanks for all the)\\nLanguage models give us the ability to assign such a conditional probability to every\\npossible next word, giving us a distribution over the entire vocabulary. The n-gram'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 15}, page_content='16 CHAPTER 9 • T HE TRANSFORMER\\nlanguage models of Chapter 3 compute the probability of a word given counts of\\nits occurrence with the n −1 prior words. The context is thus of size n −1. For\\ntransformer language models, the context is the size of the transformer’s context\\nwindow, which can be quite large: 2K, 4K, even 32K tokens for very large models.\\nThe job of the language modeling head is to take the output of the ﬁnal trans-\\nformer layer from the last token N and use it to predict the upcoming word at posi-\\ntion N +1. Fig. 9.14 shows how to accomplish this task, taking the output of the last\\ntoken at the last layer (the d-dimensional output embedding of shape [1 ×d]) and\\nproducing a probability distribution over words (from which we will choose one to\\ngenerate).\\nLayer L\\nTransformer\\nBlock\\nSoftmax over vocabulary V\\nUnembedding layer\\n…\\n1 x |V|\\nLogits \\nWord probabilities\\n1 x |V|\\nhL\\n1\\nw1 w2 wN\\nhL\\n2 hL\\nN\\nd x |V|\\n1 x d\\n   Unembedding layer\\nU = ET\\ny1 y2 y|V|…\\nu1 u2 u|V|…\\nLanguage Model Head\\ntakes hL\\nN and outputs a\\ndistribution over vocabulary V\\nFigure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output\\nembedding for token N from the last transformer layer ( hL\\nN ) to a probability distribution over words in the\\nvocabulary V .\\nThe ﬁrst module in Fig. 9.14 is a linear layer, whose job is to project from the\\noutput hL\\nN , which represents the output token embedding at positionN from the ﬁnal\\nblock L, (hence of shape [1 ×d]) to the logit vector, or score vector, that will have alogit\\nsingle score for each of the |V |possible words in the vocabularyV . The logit vector\\nu is thus of dimensionality 1 ×|V |.\\nThis linear layer can be learned, but more commonly we tie this matrix to (the\\ntranspose of) the embedding matrix E. Recall that in weight tying , we use theweight tying\\nsame weights for two different matrices in the model. Thus at the input stage of the\\ntransformer the embedding matrix (of shape[|V |×d]) is used to map from a one-hot\\nvector over the vocabulary (of shape [1 ×|V |]) to an embedding (of shape [1 ×d]).\\nAnd then in the language model head,ET, the transpose of the embedding matrix (of\\nshape [d ×|V |]) is used to map back from an embedding (shape [1 ×d]) to a vector\\nover the vocabulary (shape [1×|V |]). In the learning process, E will be optimized to\\nbe good at doing both of these mappings. We therefore sometimes call the transpose\\nET the unembedding layer because it is performing this reverse mapping.unembedding\\nA softmax layer turns the logits u into the probabilities y over the vocabulary.\\nu = hL\\nN ET (9.44)\\ny = softmax(u) (9.45)\\nWe can use these probabilities to do things like help assign a probability to a\\ngiven text. But the most important usage to generate text, which we do bysampling'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 16}, page_content='9.5 • T HE LANGUAGE MODELING HEAD 17\\na word from these probabilities y. We might sample the highest probability word\\n(‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Sec-\\ntion ??. In either case, whatever entry yk we choose from the probability vector y,\\nwe generate the word that has that index k.\\nwi\\nSample token to\\ngenerate at position i+1\\nfeedforward\\nlayer norm\\nattention\\nlayer norm\\nU\\nInput token\\nLanguage\\nModeling\\nHead\\nInput\\nEncoding\\n E\\ni+\\n…\\nlogits\\nfeedforward\\nlayer norm\\nattention\\nlayer norm\\nLayer 1\\nLayer 2\\nh1\\ni  =  x2\\ni\\nx1\\ni\\nh2\\ni  =  x3\\ni\\nfeedforward\\nlayer norm\\nattention\\nlayer norm\\nhL\\ni  \\nhL-1\\ni  =  xL\\ni\\ny1 y2 y|V|…Token probabilities\\nu1 u2 u|V|…\\nsoftmax\\nwi+1\\nLayer L\\nFigure 9.15 A transformer language model (decoder-only), stacking transformer blocks\\nand mapping from an input token wi to to a predicted next token wi+1.\\nFig. 9.15 shows the total stacked architecture for one tokeni. Note that the input\\nto each transformer layer xi is the same as the output from the preceding layer h−\\ni .\\nNow that we see all these transformer layers spread out on the page, we can point\\nout another useful feature of the unembedding layer: as a tool for interpretability of\\nthe internals of the transformer that we call the logit lens (Nostalgebraist, 2020).logit lens\\nWe can take a vector from any layer of the transformer and, pretending that it is\\nthe preﬁnal embedding, simply multiply it by the unembedding layer to get logits,\\nand compute a softmax to see the distribution over words that that vector might\\nbe representing. This can be a useful window into the internal representations of\\nthe model. Since the network wasn’t trained to make the internal representations\\nfunction in this way, the logit lens doesn’t always work perfectly, but this can still\\nbe a useful trick.\\nA terminological note before we conclude: You will sometimes see a trans-\\nformer used for this kind of unidirectional causal language model called a decoder-\\nonly model. This is because this model constitutes roughly half of the encoder-decoder-only\\nmodel'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 17}, page_content='18 CHAPTER 9 • T HE TRANSFORMER\\ndecoder model for transformers that we’ll see how to apply to machine translation\\nin Chapter 13. (Confusingly, the original introduction of the transformer had an\\nencoder-decoder architecture, and it was only later that the standard paradigm for\\ncausal language model was deﬁned by using only the decoder part of this original\\narchitecture).\\n9.6 Summary\\nThis chapter has introduced the transformer and its components for the task of lan-\\nguage modeling. We’ll continue the task of language modeling including issues like\\ntraining and sampling in the next chapter.\\nHere’s a summary of the main points that we covered:\\n• Transformers are non-recurrent networks based on multi-head attention, a\\nkind of self-attention. A multi-head attention computation takes an input\\nvector xi and maps it to an output ai by adding in vectors from prior tokens,\\nweighted by how relevant they are for the processing of the current word.\\n• A transformer block consists of a residual stream in which the input from\\nthe prior layer is passed up to the next layer, with the output of different com-\\nponents added to it. These components include a multi-head attention layer\\nfollowed by a feedforward layer, each preceded by layer normalizations.\\nTransformer blocks are stacked to make deeper and more powerful networks.\\n• The input to a transformer is a computing by adding an embedding (computed\\nwith an embedding matrix) to a positional encoding that represents the se-\\nquential position of the token in the window.\\n• Language models can be built out of stacks of transformer blocks, with a\\nlanguage model head at the top, which applies an unembedding matrix to\\nthe output H of the top layer to generate the logits, which are then passed\\nthrough a softmax to generate word probabilities.\\n• Transformer-based language models have a wide context window (as wide\\nas 32768 tokens for very large models) allowing them to draw on enormous\\namounts of context to predict upcoming words.\\nBibliographical and Historical Notes\\nThe transformer (Vaswani et al., 2017) was developed drawing on two lines of prior\\nresearch: self-attention and memory networks.\\nEncoder-decoder attention, the idea of using a soft weighting over the encodings\\nof input words to inform a generative decoder (see Chapter 13) was developed by\\nGraves (2013) in the context of handwriting generation, and Bahdanau et al. (2015)\\nfor MT. This idea was extended to self-attention by dropping the need for separate\\nencoding and decoding sequences and instead seeing attention as a way of weighting\\nthe tokens in collecting information passed from lower layers to higher layers (Ling\\net al., 2015; Cheng et al., 2016; Liu et al., 2016).\\nOther aspects of the transformer, including the terminology of key, query, and\\nvalue, came from memory networks, a mechanism for adding an external read-'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 18}, page_content='BIBLIOGRAPHICAL AND HISTORICAL NOTES 19\\nwrite memory to networks, by using an embedding of a query to match keys rep-\\nresenting content in an associative memory (Sukhbaatar et al., 2015; Weston et al.,\\n2015; Graves et al., 2014).\\nMORE HISTORY TBD IN NEXT DRAFT.'),\n",
              " Document(metadata={'source': 'research_papers/9.pdf', 'page': 19}, page_content='20 Chapter 9 • The Transformer\\nBa, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normal-\\nization. NeurIPS workshop.\\nBahdanau, D., K. H. Cho, and Y . Bengio. 2015. Neural ma-\\nchine translation by jointly learning to align and translate.\\nICLR 2015.\\nCheng, J., L. Dong, and M. Lapata. 2016. Long short-term\\nmemory-networks for machine reading. EMNLP.\\nElhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph,\\nB. Mann, A. Askell, Y . Bai, A. Chen, T. Conerly, N. Das-\\nSarma, D. Drain, D. Ganguli, Z. Hatﬁeld-Dodds, D. Her-\\nnandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse,\\nD. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCan-\\ndlish, and C. Olah. 2021. A mathematical framework for\\ntransformer circuits. White paper.\\nGraves, A. 2013. Generating sequences with recurrent neural\\nnetworks. ArXiv.\\nGraves, A., G. Wayne, and I. Danihelka. 2014. Neural Tur-\\ning machines. ArXiv.\\nLing, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez,\\nS. Amir, L. Marujo, and T. Lu´ıs. 2015. Finding function\\nin form: Compositional character models for open vocab-\\nulary word representation. EMNLP.\\nLiu, Y ., C. Sun, L. Lin, and X. Wang. 2016. Learning natural\\nlanguage inference using bidirectional LSTM model and\\ninner-attention. ArXiv.\\nNostalgebraist. 2020. Interpreting gpt: the logit lens. White\\npaper.\\nSukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015.\\nEnd-to-end memory networks. NeurIPS.\\nUszkoreit, J. 2017. Transformer: A novel neural network ar-\\nchitecture for language understanding. Google Research\\nblog post, Thursday August 31, 2017.\\nVaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\\nA. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Atten-\\ntion is all you need. NeurIPS.\\nWeston, J., S. Chopra, and A. Bordes. 2015. Memory net-\\nworks. ICLR 2015.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 0}, page_content='Citation: Kotsiantis, S.; Verykios, V .;\\nTzagarakis, M. AI-Assisted\\nProgramming Tasks Using Code\\nEmbeddings and Transformers.\\nElectronics 2024, 13, 767. https://\\ndoi.org/10.3390/electronics13040767\\nAcademic Editors: Galina Ilieva\\nand George A. Tsihrintzis\\nReceived: 19 January 2024\\nRevised: 8 February 2024\\nAccepted: 13 February 2024\\nPublished: 15 February 2024\\nCopyright: © 2024 by the authors.\\nLicensee MDPI, Basel, Switzerland.\\nThis article is an open access article\\ndistributed under the terms and\\nconditions of the Creative Commons\\nAttribution (CC BY) license (https://\\ncreativecommons.org/licenses/by/\\n4.0/).\\nelectronics\\nReview\\nAI-Assisted Programming Tasks Using Code Embeddings\\nand Transformers\\nSotiris Kotsiantis 1, *\\n , Vassilios Verykios2\\n and Manolis Tzagarakis 3\\n1 Department of Mathematics, University of Patras, 265 04 Patras, Greece\\n2 School of Science and Technology, Hellenic Open University, 263 35 Patras, Greece; verykios@eap.gr\\n3 Department of Economics, University of Patras, 265 04 Patras, Greece; tzagara@upatras.gr\\n* Correspondence: sotos@math.upatras.gr\\nAbstract: This review article provides an in-depth analysis of the growing ﬁeld of AI-assisted pro-\\ngramming tasks, speciﬁcally focusing on the use of code embeddings and transformers. With the\\nincreasing complexity and scale of software development, traditional programming methods are\\nbecoming more time-consuming and error-prone. As a result, researchers have turned to the applica-\\ntion of artiﬁcial intelligence to assist with various programming tasks, including code completion,\\nbug detection, and code summarization. The utilization of artiﬁcial intelligence for programming\\ntasks has garnered signiﬁcant attention in recent times, with numerous approaches adopting code\\nembeddings or transformer technologies as their foundation. While these technologies are popular\\nin this ﬁeld today, a rigorous discussion, analysis, and comparison of their abilities to cover AI-\\nassisted programming tasks is still lacking. This article discusses the role of code embeddings and\\ntransformers in enhancing the performance of AI-assisted programming tasks, highlighting their\\ncapabilities, limitations, and future potential in an attempt to outline a future roadmap for these\\nspeciﬁc technologies.\\nKeywords: AI-assisted programming; code embeddings; transformers\\n1. Introduction\\nAI-assisted programming or development is deﬁned as the utilization of machine\\nlearning models trained on the vast amount of available source code. Its purpose is\\nto support various aspects of programming and, more broadly, software engineering\\nimplementation tasks. According to the software naturalness conjecture [1], which posits\\nthat source code, like natural language, is often repetitive and predictable, this technology\\nhas become integrated into popular integrated development environments (IDEs) and\\ngained widespread popularity among developers [2]. Noteworthy applications such as\\nIntelliCode [3], Github Copilot [4,5], Codex [6], and DeepMind AlphaCode [7] exemplify\\nAI-assisted programming tools accessible to the public.\\nThe impact of AI on software development tasks is expected to enhance precision,\\nspeed, and efﬁciency [ 8]. These beneﬁts extend beyond professional programmers to\\ninclude novice programmers [ 9], with ongoing studies exploring the potential of AI in\\nvarious ﬁelds. Research reports highlight the sensitivity of these tools to the speciﬁc tasks\\nthey support.\\nDespite generating code, AI-assisted programming tools may produce complex and\\nerror-prone code [10]. In the domains of data science and data analysis, these tools con-\\ntribute positively to addressing challenging problems [11,12]. For novice programmers in\\neducational settings, AI-assisted programming tools increase project completion rates and\\ngrades [13,14]. Nevertheless, novices may encounter difﬁculties in comprehending and\\nutilizing these tools proﬁciently [15].\\nCode embeddings and transformers represent popular approaches to AI-assisted pro-\\ngramming, signiﬁcantly impacting software engineering by improving task performance\\nElectronics 2024, 13, 767. https://doi.org/10.3390/electronics13040767 https://www.mdpi.com/journal/electronics'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 1}, page_content='Electronics 2024, 13, 767 2 of 25\\nand efﬁciency. These techniques reduce manual effort in coding, debugging, and mainte-\\nnance, thereby decreasing overall development time and costs. Furthermore, they enable\\ncross-language development, allowing seamless work with multiple programming languages.\\nCode embedding [16] is a machine learning technique representing code as dense vec-\\ntors in a continuous vector space. Unlike traditional methods that treat code as sequences,\\ncode embedding captures semantic relationships between code snippets by training a neu-\\nral network to learn ﬁxed-size vector representations. These embeddings ﬁnd application\\nin various software engineering tasks, such as code completion, correction, summarization,\\nand search [17].\\nResearchers like Azcona et al. [ 18] propose using embeddings to proﬁle individ-\\nual Computer Science students, analyzing Python source code submissions to predict\\ncode correctness. Similarly, Ding et al. [ 19] introduce GraphCodeVec, employing graph\\nconvolutional networks to generate generalizable and task-agnostic code embeddings,\\ndemonstrating superior performance in multiple tasks.\\nTransformers [20], a type of neural network utilizing attention mechanisms for sequen-\\ntial data processing, stand out from traditional recurrent neural networks. Transformers can\\nhandle parallel input and self-attention mechanisms, processing a sequence of tokens by\\nattending to all input tokens simultaneously. In contrast, code embeddings use traditional\\ndeep learning models like recurrent neural networks (RNNs) and convolutional neural\\nnetworks (CNNs). Transformers, trained through pre-training and ﬁne-tuning, can handle\\nvariable input lengths, whereas code embeddings require ﬁxed input lengths.\\nFor code-related tasks, input code snippets feed into the transformer model, which\\nemploys self-attention mechanisms to capture contextual relationships within the code. The\\ntransformed representations generated using the model ﬁnd application in downstream\\nsoftware engineering tasks, such as code generation, summarization, translation, or identi-\\nfying patterns and anomalies in code. Chirkova and Troshin [21] demonstrated improved\\nperformance with syntax-capture modiﬁcations in transformer models.\\nIn Figure 1, the timeline showcases the evolution of AI-assisted programming tasks\\nutilizing code embeddings and transformers from their early stages of experimentation to\\ntheir integration as indispensable tools in modern software development workﬂows.\\nElectronics 2024, 13, x FOR PEER REVIEW 3 of 27 \\n \\n \\n \\nFigure 1. Timeline of the main contributions of AI-assisted programming tasks. \\nIn summary, this paper details code embeddings and transformers, discussing their char-\\nacteristics. It explores existing AI-supported programming approaches, contextualizing their \\nsupport for various tasks. The paper concludes with insights and acknowledged limitations. \\n2. Code Embeddings and Transformers \\nCode embeddings, also referred to as source code embeddings or program embed-\\ndings, have garnered signi ﬁcant attention in the realms of na tural language processing \\n(NLP) and code generation. These techniques, categorized as representation learning, aim \\nto encode both syntactic and semantic information from source code into a lower-dimen-\\nsional vector space. While code embeddings have demonstrated promising results in var-\\nious NLP tasks such as code similarity, bug detection, and code completion, recent ad-\\nvancements in transformer models have led to a shift in focus towards using transformers \\nfor code representation and generation tasks. This section delves into the concept of code \\nembeddings and their relationship with transformers. \\nIn contrast to conventional approaches relying on static program analysis techniques, \\ncode embeddings o ﬀer more e ﬀective ways to represent and analyze source code [16]. \\nThese embeddings employ neural networks to create a semantic representation of code \\nsequences, learning from a substantial code corpus. The speciﬁc architectures and training \\ntechniques employed enable these networks to capture inherent pa tterns and relation-\\nships between code elements. Consequently, code embeddings encode both structural and \\nlexical characteristics of source code, presenting a comprehensive representation applica-\\nble to diverse downstream tasks [19]. \\nThe process of creating code embeddings involves several steps. Initially, the source \\ncode undergoes tokenization, breaking it down  into a sequence of tokens, which can be \\ncharacters, words, or syntactic constructs of the programming language. Subsequently, this \\nFigure 1. Timeline of the main contributions of AI-assisted programming tasks.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 2}, page_content='Electronics 2024, 13, 767 3 of 25\\nIn summary, this paper details code embeddings and transformers, discussing their\\ncharacteristics. It explores existing AI-supported programming approaches, contextualizing\\ntheir support for various tasks. The paper concludes with insights and acknowledged limitations.\\n2. Code Embeddings and Transformers\\nCode embeddings, also referred to as source code embeddings or program embed-\\ndings, have garnered signiﬁcant attention in the realms of natural language processing\\n(NLP) and code generation. These techniques, categorized as representation learning,\\naim to encode both syntactic and semantic information from source code into a lower-\\ndimensional vector space. While code embeddings have demonstrated promising results\\nin various NLP tasks such as code similarity, bug detection, and code completion, recent\\nadvancements in transformer models have led to a shift in focus towards using transform-\\ners for code representation and generation tasks. This section delves into the concept of\\ncode embeddings and their relationship with transformers.\\nIn contrast to conventional approaches relying on static program analysis techniques,\\ncode embeddings offer more effective ways to represent and analyze source code [ 16].\\nThese embeddings employ neural networks to create a semantic representation of code\\nsequences, learning from a substantial code corpus. The speciﬁc architectures and training\\ntechniques employed enable these networks to capture inherent patterns and relationships\\nbetween code elements. Consequently, code embeddings encode both structural and lexical\\ncharacteristics of source code, presenting a comprehensive representation applicable to\\ndiverse downstream tasks [19].\\nThe process of creating code embeddings involves several steps. Initially, the source\\ncode undergoes tokenization, breaking it down into a sequence of tokens, which can be\\ncharacters, words, or syntactic constructs of the programming language. Subsequently, this\\ntoken sequence is fed into a neural network, and trained to generate a vector representation\\nfor each token. This process is repeated for all code sequences in the training data, and the\\nresulting vectors are stored in an embedding matrix. The embedding matrix is considered a\\nform of “learned parameters” in the neural network architecture. In detail, at the beginning\\nof training, the embedding matrix is initialized with random values or pre-trained word\\nembeddings. During the forward pass of the neural network, the input tokens (words) are\\nrepresented as one-hot vectors or integer indices that correspond to their positions in the\\nvocabulary. These indices are then used to index into the embedding matrix, retrieving\\nthe dense vector representations (embeddings) of the input tokens. During training, the\\nvalues of the embedding matrix are adjusted via backpropagation and gradient descent. The\\nobjective is to learn meaningful representations of words that capture semantic relationships\\nand contextual information from the training data. This process involves updating the\\nparameters of the embedding matrix to minimize the loss function of the neural network.\\nOne key technique employed in training code embeddings is the use of skip-gram\\nmodels [22]. Initially developed for natural language processing tasks, skip-gram models\\nare adapted for code embeddings to learn semantic relationships between code tokens\\nbased on their contextual surroundings. This enables the model to capture both syntactic\\nand semantic aspects, yielding a more holistic representation.\\nIn the realm of program synthesis, code embeddings ﬁnd application in generating\\ncode from natural language descriptions [23]. This entails training the model to compre-\\nhend the relationship between natural language descriptions and code sequences, facil-\\nitating the generation of code aligned with the provided description. This application\\nextends to automatic code documentation and the development of programming tools for\\nnon-technical users.\\nTo summarize, the mathematical representation of code embeddings involves\\nthe following:\\n• Tokenizing the code snippet S to obtain a sequence of tokens (t1,t2,. . .,tn).\\n• Obtaining the embedding E(ti) for each token ti.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 3}, page_content='Electronics 2024, 13, 767 4 of 25\\n• Combining the token embeddings to obtain the code embedding C, for example,\\nby averaging.\\nRabin et al. [17] evaluated the use of code2vec embeddings compared to handcrafted\\nfeatures for machine learning tasks, ﬁnding that code2vec embeddings offered even in-\\nformation gains distribution and exhibited resilience to dimension removal compared to\\nhandcrafted feature vectors.\\nSikka et al. [24] introduced a machine learning problem related to estimating the time\\ncomplexity of programming code. Comparing feature engineering and code embeddings,\\nboth methods performed well, showcasing their applicability in estimating time complexity\\nacross various applications.\\nWhile code embeddings demonstrate signiﬁcant potential in diverse applications,\\nchallenges and limitations exist. Incorporating semantic knowledge into embeddings\\nremains challenging, as models may struggle to interpret contextual or domain-speciﬁc\\ninformation that human programmers easily grasp. Kang et al. [ 25] investigated the\\npotential beneﬁts of embeddings in downstream tasks for source code models but found\\nno tangible improvement in existing models, calling for further research in this direction.\\nRomanov and Ivanov [26] experimentally explored the use of pre-trained graph neural\\nnetworks for type prediction, revealing that pre-training did not enhance type prediction\\nperformance. Ding et al. [27] studied the generalizability of pre-trained code embeddings\\nfor various software engineering tasks, introducing StrucTexVec, a two-stage unsupervised\\ntraining framework. Their experiments demonstrated that pre-trained code embeddings,\\nincorporating structural context, could be advantageous in most software engineering tasks.\\nAnother challenge lies in the requirement for substantial amounts of training data.\\nCode embeddings rely on a large corpus of code for training, limiting their applicability in\\nspeciﬁc programming languages or domains with smaller codebases.\\nAs previously mentioned, NLP transformers, utilizing attention mechanisms, have\\nbecome dominant in handling sequential data. Unlike traditional models such as recurrent\\nneural networks (RNNs) and long short-term memory (LSTM), transformers, particu-\\nlarly in the form of bidirectional encoder representations from transformers (BERT), have\\ndemonstrated superior performance in various NLP tasks, including those related to code.\\nThe self-attention mechanism in transformers can be mathematically described as the\\nfollowing: Attention(Q,K,V) = softmax\\n(\\nQKT\\n√\\ndk\\n)\\n, where Q represents the query matrix, K\\nrepresents the key matrix, V represents the value matrix, and dk represents the dimension\\nof the key vectors.\\nNLP transformers leverage self-attention mechanisms [28] to process words in a sen-\\ntence. This involves the model learning to focus on other words in the sentence for each\\nword, assigning weights based on signiﬁcance. In code-related tasks, the transformer’s\\ninput comprises a sequence of tokens representing parts of the code (e.g., function names,\\nvariable names). These tokens traverse the transformer model, enabling it to learn relation-\\nships between different parts of the code. The model then predicts the next token based on\\nthe acquired relationships.\\nA notable advantage of NLP transformers in code-related tasks is their proﬁciency\\nin handling long sequences [29]. Unlike traditional models like RNNs and LSTMs, trans-\\nformers circumvent the vanishing gradient problem when processing lengthy sequences,\\nleading to signiﬁcantly improved performance.\\nTransformers introduce positional encoding [28], conveying information about the\\nposition of words in a sentence. This proves beneﬁcial in code-related tasks where the order\\nof code is crucial for functionality. Positional encoding aids the model in distinguishing\\nbetween words with similar meanings but from different parts of the code, enhancing\\noverall performance. In Figure 1, we present a timeline of the main contributions of\\nAI-assisted programming tasks.\\nNLP transformers ﬁnd successful applications in various code-related tasks. In code\\ncompletion [30], the model predicts the next tokens of code given a partial snippet. Code'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 4}, page_content='Electronics 2024, 13, 767 5 of 25\\nsummarization [31] involves generating a concise summary of a piece of code, aiding\\ncomprehension of large and complex codebases. Code translation [ 32] sees the model\\ntranslating code from one programming language to another, particularly useful for dealing\\nwith legacy code.\\nOne prominent NLP transformer model is bidirectional encoder representations\\nfrom transformers (BERT) [ 33], widely applied in code-related tasks due to its success\\nin natural language tasks. Another transformer model, gated transformer [34], addresses\\nthe limitations of the original transformer, enhancing efﬁciency on long sequences with\\nrepetitive elements.\\nThe transformer architecture consists of encoder and decoder components. The en-\\ncoder processes input text, converting it into a sequence of vectors, while the decoder\\ngenerates output text based on these vectors. The encoder comprises multiple identical\\nlayers, each featuring self-attention and feed-forward network sub-layers. Input to the\\nencoder passes through an embedding layer, converting the input sequence into ﬁxed-\\ndimensional embeddings. Self-attention within the encoder captures relevant information\\nin the input sequence, utilizing multiple heads or parallel attention mechanisms to attend\\nto different parts of the sequence. These mechanisms compute weighted sums based on\\nword importance, capturing long-term dependencies.\\nThe self-attention and feed-forward network layers repeat within the encoder, allowing\\nhierarchical processing of the input sequence. This hierarchical approach captures different\\nlevels of abstraction [33], resulting in the hidden representation of the input sequence for\\nfurther processing by the decoder.\\nAn advantage of transformers lies in the encoder’s ability to process input sequences\\nof variable lengths, offering versatility for various NLP tasks. The use of multiple heads [35]\\nin self-attention mechanisms allows transformers to learn diverse representations of the\\ninput sequence, enhancing encoder robustness.\\nIn simple terms, multi-head attention empowers the transformer model to attend to\\nmultiple pieces of information simultaneously. Instead of relying on a single attention\\nmechanism, multi-head attention deploys several attention mechanisms in parallel, creating\\nmultiple representations of the input sequence. These parallel mechanisms, or “heads”,\\nperform the same operation with different sets of parameters, enabling the model to attend\\nto different aspects of the input sequence. Context vectors generated by each head are\\nconcatenated, resulting in the ﬁnal representation of the input sequence.\\nPre-trained models in transformers are large neural network architectures pre-trained\\non extensive text data. These models learn statistical patterns and language structures,\\nallowing them to understand and generate human-like text. Unlike traditional language\\nmodels, pre-trained transformers use bidirectional attention to consider both previous and\\nfuture words, providing a better understanding of the overall context.\\nThe effectiveness of transformer models in software engineering tasks relies on domain-\\nspeciﬁc data availability and the relevance of pre-training data to the target domain. Exper-\\nimentation and adaptation are crucial for optimal results in diverse software engineering\\napplications [36].\\nThe general process in software engineering tasks using transformers can be outlined\\nin the following steps:\\n• Data preprocessing: The initial step involves preprocessing the input data, typically\\nthrough tokenization and vectorization of code snippets. This step is crucial to feed\\nmeaningful data into the transformer model.\\n• Transformer architecture: The transformer model comprises an encoder and a decoder.\\nThe encoder processes input data to create a code representation, and the decoder\\nutilizes this representation to generate the code.\\n• Attention mechanism: Transformers incorporate an attention mechanism, a pivotal\\nelement allowing the model to focus on speciﬁc parts of the input data while generat-\\ning the output. This enhances efﬁciency in handling long sequences and capturing\\ncomplex dependencies.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 5}, page_content='Electronics 2024, 13, 767 6 of 25\\n• Training the model: Following data preprocessing and setting up the transformer\\nmodel, the next step involves training the model using backpropagation. Batches of\\ndata pass through the model, loss is calculated, and model parameters are updated to\\nminimize the loss.\\n• Fine-tuning: It is essential to assess its quality and make any necessary adjustments\\nto the model. Fine-tuning may involve retraining on a labeled dataset or adjusting\\nhyperparameters.\\nCodeBERT [37], a transformer model pre-trained on a comprehensive dataset of source\\ncode and natural language, excels in understanding the relationship between code and\\ncorresponding comments. It demonstrates state-of-the-art performance in code completion,\\nsummarization, and translation tasks, generating accurate and human-like code. CodeBERT\\nfollows the underlying architecture of BERT with modiﬁcations to suit the programming\\nlanguage domain [38]. The bidirectional transformer encoder takes in code and natural\\nlanguage sequences, encoding them into contextualized representations. A decoder then\\ngenerates a human-readable description of the code. Code and natural language sequences\\nare concatenated with special tokens to indicate the input type. Pre-trained on extensive\\ndata from GitHub, Stack Overﬂow, Wikipedia, and other sources, CodeBERT undergoes\\nﬁne-tuning for downstream tasks like code summarization, classiﬁcation, and retrieval.\\nThis transfer learning model adapts to different codebases and programming languages,\\nfacilitating code generation and retrieval for non-programmers.\\nT5 (text-to-text transfer transformer) [39], another large-scale transformer model, caters\\nto various natural language tasks. Pre-trained on diverse datasets, T5 can handle tasks such\\nas translation, summarization, and question answering. It has proven effective in code\\ngeneration tasks, producing high-quality code with detailed explanations.\\nGPT-3 (generative pre-trained transformer) [40], developed by OpenAI, excels in natu-\\nral language generation tasks, including code completion. Its large size and pre-training\\non a wide range of tasks make it adept at generating code for different programming\\nlanguages, often matching human writing.\\nXLNet [41], based on the permutation language model, outperforms BERT in many\\nNLP tasks, including code completion. Similar to BERT, XLNet comprehends code syntax\\nand context well, generating code for various programming languages. CCBERT [ 42], a\\ndeep learning model for generating Stack Overﬂow question titles, exhibits strong perfor-\\nmance in regular and low-resource datasets.\\nEL-CodeBert [43], a pre-trained model combining programming languages and natural\\nlanguages, utilizes representational information from each layer of CodeBert for down-\\nstream source code-related tasks. Outperforming state-of-the-art baselines in four tasks,\\nEL-CodeBert demonstrates effectiveness in leveraging both programming and natural\\nlanguage information.\\nTransformers have demonstrated impressive performance across various natural\\nlanguage processing (NLP) tasks and have found successful applications in AI-assisted\\nprogramming. However, they also exhibit certain inherent weaknesses within this domain.\\nOne limitation lies in their capacity for contextual understanding. While transformers\\nexcel at capturing context within a ﬁxed-length window, typically around 512 tokens, pro-\\ngramming tasks often involve extensive codebases where understanding context beyond\\nthis window becomes crucial for accurate analysis and generation. Additionally, trans-\\nformers lack domain-speciﬁc knowledge. Being pre-trained on general-purpose corpora,\\nthey may not adequately capture the intricacies and specialized knowledge required for\\nprogramming tasks. This deﬁciency can result in suboptimal performance when dealing\\nwith programming languages, libraries, and frameworks.\\nIn AI-assisted programming tasks, code embeddings and transformers are closely\\nconnected, often complementing each other to enhance the capabilities of programming\\nassistance tools. There are connections between code embeddings and transformers in\\nthis context:'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 6}, page_content='Electronics 2024, 13, 767 7 of 25\\n• Representation learning: Both code embeddings and transformers aim to learn mean-\\ningful representations of code. Code embeddings convert source code into ﬁxed-\\ndimensional vectors, capturing syntactic and semantic information. Similarly, trans-\\nformers utilize self-attention mechanisms to learn contextual representations of code\\nsnippets, allowing them to capture dependencies between different parts of the code.\\n• Semantic understanding: Code embeddings and transformers facilitate semantic un-\\nderstanding of code. Code embeddings map code snippets into vector representations\\nwhere similar code fragments are closer in the embedding space, aiding tasks like code\\nsearch, code similarity analysis, and clone detection. Transformers, with their ability\\nto capture contextual information, excel at understanding the semantics of code by\\nconsidering the relationships between tokens and their context.\\n• Feature extraction: Both techniques serve as effective feature extractors for down-\\nstream tasks in AI-assisted programming. Code embeddings provide compact repre-\\nsentations of code that can be fed into traditional machine learning models or neural\\nnetworks for tasks like code classiﬁcation, bug detection, or code summarization.\\nTransformers, on the other hand, extract features directly from code snippets using\\nself-attention mechanisms, enabling end-to-end learning for various programming-\\nrelated tasks.\\n• Model architecture: Code embeddings and transformers are often integrated into the\\nsame model architecture to leverage their complementary strengths. For instance, mod-\\nels like CodeBERT combine transformer-based architectures with code embeddings\\nto enhance code understanding and generation capabilities. This fusion allows the\\nmodel to capture both local and global dependencies within code snippets, resulting\\nin more accurate and context-aware predictions.\\n• Fine-Tuning: Pre-trained transformers, such as BERT or GPT, can be ﬁne-tuned on\\ncode-related tasks using code embeddings as input features. This ﬁne-tuning process\\nadapts the transformer’s parameters to better understand the speciﬁc characteristics\\nof programming languages and code structures, leading to improved performance on\\nprogramming-related tasks.\\nIn conclusion, the use of code embeddings and transformers in software engineering\\ntasks has witnessed substantial growth. Code embeddings, capturing both syntactic and\\nsemantic information, offer effective representation learning techniques. Transformers,\\nparticularly in the form of BERT and its derivatives, demonstrate superior performance\\nin various code-related tasks, owing to their ability to handle long sequences and con-\\nsider both past and future context. The pre-trained models, such as CodeBERT and T5,\\nhave shown remarkable success in code generation, summarization, and translation tasks.\\nHowever, challenges such as incorporating semantic knowledge into embeddings and the\\nneed for extensive training data persist. Continuous experimentation and adaptation are\\ncrucial for harnessing the full potential of these advanced techniques in diverse software\\nengineering applications.\\n3. Methodology\\nA comprehensive review of literature pertaining to AI-supported programming tasks\\nwas conducted. The selection criteria were based on both content and publication year.\\nSpeciﬁcally, papers were chosen based on their utilization of code-embeddings or trans-\\nformer technologies to facilitate AI-assisted programming tasks. The focus was on papers\\nexplicitly mentioning speciﬁc programming tasks that were supported. The scope of the\\nresearch encompassed papers published within the last 5 years.\\nTo ensure a thorough examination, only publications indexed in Scopus were taken\\ninto consideration. The identiﬁcation of relevant papers was achieved through a keyword-\\nbased search using terms such as “code embeddings” and “transformers”, coupled with\\nspeciﬁc programming tasks (e.g., “code embeddings bug detection”).'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 7}, page_content='Electronics 2024, 13, 767 8 of 25\\n4. AI-Supported Programming Tasks\\nIn this section, the current body of literature on AI-assisted programming is examined,\\nemphasizing the speciﬁc tasks addressed by the studied approaches. The discussion is or-\\nganized around a framework comprising nine programming tasks identiﬁed in the relevant\\nliterature. These tasks encompass code summarization, bug detection and correction, code\\ncompletion, code generation process, code translation, code comment generation, duplicate\\ncode detection and similarity, code reﬁnement, and code security.\\n4.1. Code Summarization\\nCode summarization involves generating natural language descriptions for source\\ncode written in various programming languages, primarily to support documentation\\ngeneration. During this process, input source code is transformed into a descriptive\\nnarrative, typically in English, providing an overview of the code’s functionality at the\\nfunction level.\\nAn enhanced code embedding approach known as Flow2Vec [16] improved the rep-\\nresentation of inter-procedural program dependence (value ﬂows) with precision. It ac-\\ncommodated control ﬂows and data ﬂows with alias recognition, mapping them into a\\nlow-dimensional vector space. Experiments on 32 open-source projects demonstrated\\nFlow2Vec’s effectiveness in enhancing the performance of existing code embedding tech-\\nniques for code classiﬁcation and code summarization tasks.\\nTransformers play a crucial role in generating summaries, involving preprocessing\\nthe text by removing unnecessary characters and segmenting them into smaller sentences\\nor phrases. The transformer model, trained on extensive text data, utilizes its attention\\nmechanism to identify key words and phrases, producing a summary based on these\\nessential elements.\\nWang et al. [ 44] introduced Fret, a functional reinforced transformer with BERT,\\nwhich outperformed existing approaches in both Java and Python. Achieving a BLEU-4\\nscore of 24.32 and a ROUGE-L score of 40.12, Fret demonstrated superior performance\\nin automatic code summarization. For smart contracts, Yang et al. [45] proposed a multi-\\nmodal transformer-based code summarization model, showcasing its ability to generate\\nhigher-quality code comments compared to state-of-the-art baselines.\\nHou et al. [ 46] presented TreeXFMR, an automatic code summarization paradigm\\nwith hierarchical attention, using abstract syntax trees and positional encoding for code\\nrepresentation. Pre-trained and tested on GitHub, TreeXFMR achieved signiﬁcantly better\\nresults than baseline methods.\\nGypSum [47] incorporated a graph attention network and a pre-trained programming\\nand natural language model for code summarization. Utilizing a dual-copy mechanism,\\nGypSum achieved effective hybrid representations and improved the summary generation\\nprocess. Gu et al. [48] introduced AdaMo, a method for automated code summarization\\nleveraging adaptive strategies like pre-training and intermediate ﬁne-tuning to optimize\\nlatent representations.\\nMa et al. [ 49] proposed a multi-modal ﬁne-grained feature fusion model for code\\nsummarization, effectively aligning and fusing information from token and abstract syntax\\ntree modalities. Outperforming current state-of-the-art models, this approach demonstrated\\nsuperior results.\\nGong et al. [31] presented SCRIPT, a structural relative position-guided transformer,\\nusing ASTs to capture source code structural dependencies. SCRIPT outperformed exist-\\ning models on benchmark datasets in terms of BLEU, ROUGE-L, and METEOR metrics.\\nGao and Lyu [50] proposed M2TS, an AST-based source code summarization technique\\nintegrating AST and token features to capture the structure and semantics of source code,\\ndemonstrating performance on Java and Python language datasets.\\nFerretti and Saletta [51] introduced a novel summarization approach using a pseudo-\\nlanguage to enhance the BRIO model, outperforming CodeBERT and PLBART. The study ex-'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 8}, page_content='Electronics 2024, 13, 767 9 of 25\\nplored the limitations of existing NLP-based approaches and suggested further\\nresearch directions.\\nChoi et al. [52] presented READSUM, a model combining abstractive and extractive\\napproaches for generating concise and informative code summaries. READSUM considered\\nboth structural and temporal aspects of input code, utilizing a multi-head self-attention\\nmechanism to create augmented code representations. The extractive procedure veriﬁed the\\nrelevancy of important keywords, while the abstractive approach generated high-quality\\nsummaries considering both structural and temporal information from the source code.\\nIn summary, code embeddings and transformers both play crucial roles in code summa-\\nrization, yet they operate in distinct ways. Code embeddings typically involve representing\\ncode snippets as ﬁxed-length vectors in a continuous vector space, capturing semantic\\nand syntactic information. This approach offers simplicity and efﬁciency in handling\\ncode representations but may struggle with capturing long-range dependencies. On the\\nother hand, transformers excel in modeling sequential data by processing the entire in-\\nput sequence simultaneously through self-attention mechanisms. This allows them to\\ncapture intricate dependencies across code snippets effectively, resulting in more com-\\nprehensive summarizations. However, transformers often require larger computational\\nresources compared to code embeddings. Thus, while code embeddings offer efﬁciency\\nand simplicity, transformers provide a more powerful and context-aware solution for code\\nsummarization tasks.\\n4.2. Bug Detection and Correction\\nThis task focuses on identifying errors in code (Figure 2), emphasizing the detection\\nof unknown errors to enhance software reliability. Traditional bug detection methods rely\\non manual code reviews, which are often tedious and time-consuming. In contrast, code\\nembedding presents an efﬁcient approach, capable of processing large volumes of code and\\nidentifying potential bugs within minutes. The effectiveness of code embedding depends\\non a diverse training dataset, as a lack of diversity may hinder its ability to capture all types\\nof bugs.\\nElectronics 2024, 13, x FOR PEER REVIEW 10 of 27 \\n \\n \\n \\nFigure 2. Code bug detection and correction example. \\nAladics et al. [53] demonstrat ed that representing source code as vectors, based on \\nan abstract syntax tree and the Doc2Vec algorithm, improved bug prediction accuracy and \\nwas suitable for machine learning tasks involving source code. Cheng et al. [54] proposed \\na self-supervised contrastive learning approach for static vulnerability detection, leverag-\\ning pre-trained path embedding models to reduce the need for labeled data. Their ap-\\nproach outperformed eight baselines for bug detection in real-world projects. \\nHegedus and Ferenc [55] used a machine learning model to ﬁlter out false positive \\ncode analysis warnings from an open-source Java dataset, achieving an accuracy of 91%, \\nan F1-score of 81.3%, and an AUC of 95.3%. NLP transformers oﬀer an eﬃcient and accu-\\nrate method for bug detection by analyzing source code, identifying patterns, and detect-\\ning inconsistencies indicative of bugs. Bagheri and Hegedus [56] compared text represen-\\ntation methods (word2vec, fastText, and BER T) for detecting vulnerabilities in Python \\ncode, with BERT exhibiting the highest accuracy rate (93.8%). Gomes et al. [57] found that \\nBERT-based feature extraction signiﬁcantly outperformed TF-IDF-based extraction in pre-\\ndicting long-lived bugs, with support vector machines and random forests producing bet-\\nter results when using BERT. \\nCode summarization, utilizing NLP transfor mers, presents an approach to bug de-\\ntection by automatically generating human-readable summaries of code fragments. This \\nmethod has shown promise in detecting bugs in open-source projects with ample code \\nand bug data available for training. \\nEvaluation of four new CodeBERT models for predicting software defects demon-\\nstrated their ability to improve predictive accuracy across diﬀerent software versions and \\nprojects [58]. The choice of di stinct prediction approaches inﬂuenced the accuracy of the \\nCodeBERT models. \\nDistilBERT, a lightweight version of BERT, pre-trained and ﬁne-tuned on various \\nNLP tasks, including bug detection and correction, o ﬀers faster and more e ﬃcient bug \\ndetection, albeit with potentially lower pe rformance than other transformer models. \\nAttSum, a deep attention-based summarization model, surpassed existing models in eval-\\nuating bug report titles [59]. \\nBugsplainer, a transformer-based generative model for explaining software bugs to \\ndevelopers, presented more precise, accurate, concise, and helpful explanations than pre-\\nvious models [60]. Transformers contribute to bug localization, identifying the exact loca-\\ntion of bugs in the code. Validation of patches in automated program repair (APR) remains \\na crucial area, with Csuvik et al. [61] demonstrating the utility of Doc2Vec models in gen-\\nerating patches for JavaScript code. \\nMashhadi and Hemmati [62] introduced an automated program repair approach re-\\nlying on CodeBERT, generating qualitative ﬁxes in various bug cases. Chakraborty et al. \\n[63] created Modit, a multi-modal NMT code editing engine, which outperformed existing \\nmodels in obtaining correct code patches, especially when developer hints were included. \\nGenerate and validate, a strategy for automatic bug repair using the generative pre-\\ntrained transformer (GPT) model, achieved up to 17.25% accuracy [64]. SeqTrans, pro-\\nposed by Chi et al. [65], demonstrated superior  accuracy in addressing certain types of \\nFigure 2. Code bug detection and correction example.\\nAladics et al. [53] demonstrated that representing source code as vectors, based on an\\nabstract syntax tree and the Doc2Vec algorithm, improved bug prediction accuracy and\\nwas suitable for machine learning tasks involving source code. Cheng et al. [54] proposed a\\nself-supervised contrastive learning approach for static vulnerability detection, leveraging\\npre-trained path embedding models to reduce the need for labeled data. Their approach\\noutperformed eight baselines for bug detection in real-world projects.\\nHegedus and Ferenc [55] used a machine learning model to ﬁlter out false positive\\ncode analysis warnings from an open-source Java dataset, achieving an accuracy of 91%, an\\nF1-score of 81.3%, and an AUC of 95.3%. NLP transformers offer an efﬁcient and accurate\\nmethod for bug detection by analyzing source code, identifying patterns, and detecting\\ninconsistencies indicative of bugs. Bagheri and Hegedus [56] compared text representation\\nmethods (word2vec, fastText, and BERT) for detecting vulnerabilities in Python code, with\\nBERT exhibiting the highest accuracy rate (93.8%). Gomes et al. [57] found that BERT-based\\nfeature extraction signiﬁcantly outperformed TF-IDF-based extraction in predicting long-'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 9}, page_content='Electronics 2024, 13, 767 10 of 25\\nlived bugs, with support vector machines and random forests producing better results\\nwhen using BERT.\\nCode summarization, utilizing NLP transformers, presents an approach to bug de-\\ntection by automatically generating human-readable summaries of code fragments. This\\nmethod has shown promise in detecting bugs in open-source projects with ample code and\\nbug data available for training.\\nEvaluation of four new CodeBERT models for predicting software defects demon-\\nstrated their ability to improve predictive accuracy across different software versions and\\nprojects [58]. The choice of distinct prediction approaches inﬂuenced the accuracy of the\\nCodeBERT models.\\nDistilBERT, a lightweight version of BERT, pre-trained and ﬁne-tuned on various NLP\\ntasks, including bug detection and correction, offers faster and more efﬁcient bug detection,\\nalbeit with potentially lower performance than other transformer models. AttSum, a deep\\nattention-based summarization model, surpassed existing models in evaluating bug report\\ntitles [59].\\nBugsplainer, a transformer-based generative model for explaining software bugs\\nto developers, presented more precise, accurate, concise, and helpful explanations than\\nprevious models [60]. Transformers contribute to bug localization, identifying the exact\\nlocation of bugs in the code. Validation of patches in automated program repair (APR)\\nremains a crucial area, with Csuvik et al. [61] demonstrating the utility of Doc2Vec models\\nin generating patches for JavaScript code.\\nMashhadi and Hemmati [62] introduced an automated program repair approach rely-\\ning on CodeBERT, generating qualitative ﬁxes in various bug cases. Chakraborty et al. [63]\\ncreated Modit, a multi-modal NMT code editing engine, which outperformed existing\\nmodels in obtaining correct code patches, especially when developer hints were included.\\nGenerate and validate, a strategy for automatic bug repair using the generative pre-\\ntrained transformer (GPT) model, achieved up to 17.25% accuracy [64]. SeqTrans, proposed\\nby Chi et al. [65], demonstrated superior accuracy in addressing certain types of vulner-\\nabilities, outperforming previous strategies in the context of neural machine translation\\n(NMT) technology.\\nVRepair, an approach by Chen et al. [66], utilized deep learning and transfer learning\\ntechniques for automatic software vulnerability repair, showing effectiveness in repair-\\ning security vulnerabilities in C. Kim and Yang [ 67], who utilized the BERT algorithm\\nto predict duplicated bug reports, outperforming existing models and improving bug\\nresolution times.\\nA technique for developing test oracles, combined with automated testing, improved\\naccuracy by 33%, identifying 57 real-world bugs [68]. da Silva et al. [69] explored various\\nprogram embeddings and learning models for predictive compilation, with surprisingly\\nsimple embeddings performing comparably to more complex ones.\\nIn summary, code embeddings and transformers serve as valuable tools for bug\\ndetection and correction, each with its unique strengths. Code embeddings offer a concise\\nrepresentation of code snippets, capturing their semantic and syntactic properties in a\\nﬁxed-length vector format. This can facilitate efﬁcient similarity comparisons between\\ncode segments, aiding in identifying similar bug patterns across projects. However, code\\nembeddings may struggle with capturing complex contextual information and long-range\\ndependencies, potentially leading to limitations in detecting subtle bugs. In contrast,\\ntransformers excel in modeling sequential data through self-attention mechanisms, enabling\\nthem to capture intricate patterns and contextual information across code segments. This\\nmakes transformers particularly effective in detecting and correcting bugs that involve\\ncomplex interactions and dependencies between code components. Despite the promising\\nresults of NLP transformers in bug detection, challenges include the scarcity of large, high-\\nquality datasets and the signiﬁcant computational resources and training time required.\\nExisting datasets are often language-speciﬁc, making generalization to different codebases'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 10}, page_content='Electronics 2024, 13, 767 11 of 25\\nchallenging. Additionally, the resource-intensive nature of NLP transformers may limit\\ntheir suitability for real-time bug detection.\\n4.3. Code Completion\\nCode completion, a crucial aspect of programming, involves suggesting code to assist\\nprogrammers in efﬁciently completing the code they are currently typing. This suggestion\\ncan span variable and function names to entire code snippets. The application of transform-\\ners in code completion harnesses advanced language models, trained on extensive text data,\\nto enhance developers’ coding efﬁciency. These models exhibit a deep understanding of the\\ncontext of the code under construction, predicting and suggesting the next code sequence as\\ndevelopers type. This extends beyond basic keyword suggestions, encompassing variable\\nnames, function calls, and even the generation of complete code snippets.\\nThe model’s proﬁciency in comprehending syntactic and semantic structures in pro-\\ngramming languages ensures accurate and contextually relevant suggestions. It plays a\\nrole in identifying and preventing common coding mistakes by offering real-time correc-\\ntions. Moreover, code completion with transformers often entails providing contextual\\ninformation such as function signatures, parameter details, and relevant documentation.\\nThis not only accelerates the coding process but also aids developers in effectively utilizing\\nvarious functions and methods.\\nRoberta [70], another transformer model, has demonstrated impressive results in\\nvarious natural language processing tasks, showcasing noteworthy performance in code\\ncompletion. It excels in generating code for diverse programming languages, showcasing a\\nrobust understanding of code syntax and context.\\nTransformer-XL [71], designed to handle longer sequences compared to traditional\\ntransformers, has exhibited promising outcomes in code completion tasks, especially when\\ndealing with extensive and intricate sequences. It showcases proﬁciency in generating code\\nfor various programming languages.\\nCodeFill, proposed by Izadi et al. [72], is a language model for autocompletion lever-\\naging learned structure and naming information. Outperforming several baseline and\\nstate-of-the-art models, including GPT-C and TravTrans+, CodeFill excels in both single-\\ntoken and multi-token prediction. All code and datasets associated with CodeFill are\\npublicly available.\\nCCMC, presented by Yang and Kuang [ 29], is a code completion model utilizing\\na Transformer-XL model for handling long-range dependencies and a pointer network\\nwith CopyMask for copying OOV tokens from inputs. The model demonstrates excellent\\nperformance in code completion on real-world datasets.\\nDevelopers can seamlessly integrate code completion into their preferred integrated\\ndevelopment environments (IDEs) or code editors, enhancing the overall coding experience.\\nThe interactive and adaptive nature of transformer-based code completion renders it a pow-\\nerful tool for developers working across various programming languages and frameworks.\\nLiu et al. [ 73] introduced a multi-task learning-based pre-trained language model\\nwith a transformer-based neural architecture to address challenges in code completion\\nwithin integrated development environments (IDEs). Experimental results highlight the\\neffectiveness of this approach compared to existing state-of-the-art methods.\\nBART (bidirectional and auto-regressive transformer), another popular transformer\\nmodel developed [74], is trained using a combination of supervised and unsupervised learn-\\ning techniques. Speciﬁcally designed for text generation tasks, BART has shown promising\\nresults in code generation, achieving state-of-the-art performance in code completion tasks\\nwhere it predicts the remaining code based on the given context.\\nA novel neural architecture based on transformer models was proposed and evaluated\\nfor autocomplete systems in IDEs, showcasing an accuracy increase of 14–18%. Additionally,\\nan open-source code and data pipeline were released [75]. While transformer models exhibit\\npromise for code completion, further enhancements in accuracy are essential for addressing\\ncomplex scenarios [30].'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 11}, page_content='Electronics 2024, 13, 767 12 of 25\\nIn summary, code embeddings and transformers are both valuable tools for code\\ncompletion, each offering distinct advantages. Code embeddings provide a compact rep-\\nresentation of code snippets in a continuous vector space, capturing their semantic and\\nsyntactic properties. This allows for efﬁcient retrieval of similar code segments, aiding in\\nsuggesting relevant completions based on the context of the code being written. However,\\ncode embeddings may struggle with capturing long-range dependencies and contextual\\nnuances, potentially leading to less accurate suggestions in complex coding scenarios.\\nTransformers, on the other hand, excel in modeling sequential data through self-attention\\nmechanisms, enabling them to capture intricate patterns and contextual information across\\ncode sequences. This results in more accurate and context-aware code completions, espe-\\ncially in scenarios where understanding broader context and dependencies is crucial.\\n4.4. Code Generation Process\\nCode generation involves the task of creating source code based on constraints speci-\\nﬁed by the programmer in natural language. Hu et al. [23] introduced a supervised code\\nembedding approach along with a tree representation of code snippets, demonstrating\\nenhanced accuracy and efﬁciency in generating code from natural language compared to\\ncurrent state-of-the-art methods.\\nTransformers, a type of neural network architecture widely used for various natural\\nlanguage processing (NLP) tasks, including code generation, utilize an attention mechanism\\nto capture long-term dependencies. They excel in handling sequential data without relying\\non recurrent connections, making them well-suited for tasks involving code generation.\\nTransformers can be applied to generate functions or methods based on high-level\\nspeciﬁcations. Developers can articulate the desired functionality in natural language, and\\nthe transformer generates the corresponding code.\\nSvyatkovskiy et al. [ 3] introduced IntelliCode Compose, a versatile, multilingual\\ncode completion tool capable of predicting arbitrary code tokens and generating correctly\\nstructured code lines. It was trained on 1.2 billion lines of code across four languages and\\nutilized in the Visual Studio Code IDE and Azure Notebook.\\nGemmell et al. [76] explored Transformer architectures for code generation beyond\\nexisting IDE capabilities, proposing a “Relevance Transformer” model. Benchmarking\\nresults demonstrated improvement over the current state-of-the-art.\\nSoliman et al. [77] presented MarianCG-NL-to-Code, a code generation transformer\\nmodel for generating Python code from natural language descriptions. Outperforming\\nstate-of-the-art models, it was downloadable on GitHub and evaluated on CoNaLa and\\nDJANGO datasets.\\nExploitedGen [78], an exploit code generation approach based on CodeBERT, achieved\\nbetter accuracy in generating exploit code than existing methods. It incorporated a template-\\naugmented parser and a semantic attention layer, with additional experiments assessing\\ngenerated code for syntax and semantic accuracy.\\nLaskari et al. [ 79] discussed Seq2Code, a transformer-based solution for translat-\\ning natural language problem statements into Python source code. Using an encoder–\\ndecoder transformer design with multi-head attention and separate embeddings for special\\ncharacters, the model demonstrated improved perplexity compared to similarly struc-\\ntured models.\\nTo summarize the code generation process, code embeddings and transformers offer\\ndistinctive approaches, each with its own strengths. Code embeddings condense code\\nsnippets into ﬁxed-length vectors, capturing semantic and syntactic information efﬁciently.\\nThis simpliﬁes the generation process by enabling quick retrieval of similar code segments\\nand facilitating straightforward manipulation in vector space. However, code embeddings\\nmight struggle with capturing complex dependencies and contextual nuances, potentially\\nlimiting their ability to produce diverse and contextually accurate code. In contrast, trans-\\nformers excel in modeling sequential data through self-attention mechanisms, allowing\\nthem to capture intricate patterns and long-range dependencies across code sequences.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 12}, page_content='Electronics 2024, 13, 767 13 of 25\\nThis enables transformers to generate code with greater context awareness and ﬂexibil-\\nity, resulting in more accurate and diverse outputs. Nevertheless, transformers typically\\ndemand signiﬁcant computational resources and extensive training data compared to\\ncode embeddings.\\n4.5. Code Translation\\nCode translation (Figure 3) involves the conversion of source code from one program-\\nming language to another, commonly employed for managing legacy source code. Unlike\\ncode generation, which takes natural language as input, code translation deals directly with\\nsource code. Bui et al. [80] introduced a bilingual neural network (Bi-NN) architecture for\\nautomatically classifying Java and C++ programs. Comprising two sub-networks dedicated\\nto Java and C++ source code, Bi-NN utilized an additional neural network layer to recog-\\nnize similarities in algorithms and data structures across different languages. Evaluation\\nof a code corpus containing 50 diverse algorithms and data structures revealed promis-\\ning classiﬁcation results, with increased accuracy attributed to encoding more semantic\\ninformation from the source code.\\nElectronics 2024, 13, x FOR PEER REVIEW 13 of 27 \\n \\n \\nGemmell et al. [76] explored Transformer architectures for code generation beyond \\nexisting IDE capabilities, proposing a “Relevance Transformer” model. Benchmarking re-\\nsults demonstrated improvement over the current state-of-the-art. \\nSoliman et al. [77] presented MarianCG-NL-to-Code, a code generation transformer \\nmodel for generating Python code from natu ral language descriptions. Outperforming \\nstate-of-the-art models, it was downloadable  on GitHub and evaluated on CoNaLa and \\nDJANGO datasets. \\nExploitedGen [78], an exploit code gene ration approach based on CodeBERT, \\nachieved better accuracy in generating exploit code than existing methods. It incorporated \\na template-augmented parser and a semantic attention layer, with additional experiments \\nassessing generated code for syntax and semantic accuracy. \\nLaskari et al. [79] discussed Seq2Code, a transformer-based solution for translating nat-\\nural language problem statements into Python source code. Using an encoder–decoder trans-\\nformer design with multi-head attention and separate embeddings for special characters, the \\nmodel demonstrated improved perplexity compared to similarly structured models. \\nTo summarize the code generation process, code embeddings and transformers oﬀer \\ndistinctive approaches, each with its own strengths. Code embeddings condense code \\nsnippets into ﬁxed-length vectors, capturing semantic and syntactic information e ﬃ-\\nciently. This simpliﬁes the generation process by enabling quick retrieval of similar code \\nsegments and facilitating straightforward manipulation in vector space. However, code \\nembeddings might struggle with capturing complex dependencies and contextual nu-\\nances, potentially limiting their ability to produce diverse and contextually accurate code. \\nIn contrast, transformers excel in modeling sequential data through self-attention mecha-\\nnisms, allowing them to capture intricate pa tterns and long-range dependencies across \\ncode sequences. This enables transformers to  generate code with greater context aware-\\nness and ﬂexibility, resulting in more accurate and diverse outputs. Nevertheless, trans-\\nformers typically demand signiﬁcant computational resources and extensive training data \\ncompared to code embeddings. \\n4.5. Code Translation \\nCode translation (Figure 3) involves the conversion of source code from one pro-\\ngramming language to another, commonly em ployed for managing legacy source code. \\nUnlike code generation, which takes natural language as input, code translation deals di-\\nrectly with source code. Bui et al. [80] intr oduced a bilingual neural network (Bi-NN) ar-\\nchitecture for automatically classifying Java and C++ programs. Comprising two sub-net-\\nworks dedicated to Java and C++ source code, Bi-NN utilized an additional neural net-\\nwork layer to recognize similarities in algorithms and data structures across diﬀerent lan-\\nguages. Evaluation of a code corpus containing 50 diverse algorithms and data structures \\nrevealed promising classiﬁcation results, with increased accuracy attributed to encoding \\nmore semantic information from the source code. \\n \\nFigure 3. Code translation example. \\nIn contrast to traditional machine translation methods, transformers, which employ \\nself-attention mechanisms instead of recurrent networks, play a pivotal role in code trans-\\nlation. Transformers facilitate the automatic conversion of source code written in one pro-\\ngramming language into its equivalent in another language. This capability proves \\nFigure 3. Code translation example.\\nIn contrast to traditional machine translation methods, transformers, which employ\\nself-attention mechanisms instead of recurrent networks, play a pivotal role in code trans-\\nlation. Transformers facilitate the automatic conversion of source code written in one\\nprogramming language into its equivalent in another language. This capability proves\\nvaluable for tasks such as cross-language code migration, integrating code from different\\nlanguages, or aiding developers familiar with one language in comprehending and working\\nwith code written in another.\\nHassan et al. [32] introduced a source code converter based on the neural machine\\ntranslation transformer model, specializing in converting source code between Java and\\nSwift. The model was trained on a merged dataset, and initial results demonstrated promise\\nin terms of the pipeline and code synthesis procedure.\\nDeepPseudo, presented by Yang et al. [ 81], leveraged advancements in sequence-\\nto-sequence learning and code semantic learning to automatically generate pseudo-code\\nfrom source code. Experiment results indicated DeepPseudo’s superiority over seven\\nstate-of-the-art models, providing a valuable tool for novice developers to understand\\nprogramming code more easily.\\nAlokla et al. [82] proposed a new model for generating pseudocode from source code,\\nachieving higher accuracy compared to previous models. This model utilized similarity\\nmeasures and deep learning transformer models, demonstrating promising results on\\ntwo datasets.\\nDLBT, a deep learning-based transformer model for automatically generating pseu-\\ndocode from source code [ 83], tokenized the source code and employed a transformer\\nto assess the relatedness between the source code and its corresponding pseudocode.\\nTested with Python source code, DLBT achieved accuracy and BLEU scores of 47.32 and\\n68.49, respectively.\\nAcharjee et al. [84] suggested a method utilizing natural language processing and a\\nsequence-to-sequence deep learning-based model trained on the SPoC dataset for pseu-\\ndocode conversion. This approach exhibited increased accuracy and efﬁciency compared\\nto other techniques, as evaluated using bilingual understudy scoring.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 13}, page_content='Electronics 2024, 13, 767 14 of 25\\nTo sum up regarding the realm of code generation translation, both code embeddings\\nand transformers offer distinct advantages. Code embeddings condense code snippets into\\nﬁxed-length vectors, effectively capturing the semantic and syntactic information essential\\nfor translation tasks. This approach simpliﬁes the translation process by enabling quick\\nretrieval of similar code segments and facilitating straightforward manipulation in vector\\nspace. However, code embeddings may struggle to capture complex dependencies and\\nnuances present in code, potentially limiting their ability to produce accurate translations.\\nOn the other hand, transformers excel in modeling sequential data through self-attention\\nmechanisms, allowing them to capture intricate patterns and long-range dependencies\\nacross code sequences. This results in more context-aware translations, with the ability to\\nhandle a wide range of coding languages and structures.\\n4.6. Code Comment Generation\\nThe objective of this task is the automatic generation of natural language comments\\nfor a given code snippet. Shahbazi et al. [85] introduced API2Com, a comment generation\\nmodel that utilized Application Programming Interface Documentations (API Docs) as\\nexternal knowledge resources. The authors observed that API Docs could enhance comment\\ngeneration, especially when there was only one API in the method. However, as the number\\nof APIs increased, the model output was negatively impacted.\\nComFormer, proposed by Yang et al. [86], is a novel code comment generator that\\nintegrates transformer and fusion method-based hybrid code presentation. Byte-BPE and\\nSim_SBT were employed to address out-of-vocabulary (OOV) problems during training.\\nThe evaluation involved three metrics and a human study comparing ComFormer to\\nseven state-of-the-art baselines from both code comment and neural machine translation\\n(NMT) domains.\\nChakraborty et al. [87] introduced a new pre-training objective for language models\\nfor source code, aiming to naturalize the code by utilizing its bi-channel structure (formal\\nand informal). The authors employed six categories of semantic maintaining changes\\nto construct unnatural forms of code for model training. After ﬁne-tuning, the model\\nperformed on par with CodeT5, exhibiting improved performance for zero-shot and few-\\nshot learning, as well as better comprehension of code features.\\nGeng et al. [88] proposed a two-stage method for creating natural language comment\\ntexts for code. The approach utilized a model interpretation strategy to reﬁne summaries,\\nenhancing accuracy. Thongtanunam et al. [89] developed AutoTransform, an advanced\\nneural machine translation (NMT) model that signiﬁcantly increased accuracy in auto-\\nmatically transforming code for code review processes. This innovation aimed to reduce\\ndevelopers’ time and effort in manual code review.\\nBASHEXPLAINER [90] automated code comment generation for Bash scripts, outper-\\nforming existing methods based on metrics such as BLEU-3/4, METEOR, and ROUGE-L\\nby up to 9.29%, 8.75%, 4.77%, and 3.86%, respectively. Additionally, it offered a browser\\nplug-in to facilitate the understanding of Bash code.\\nS-Coach, presented by Lin et al. [91], is a two-phase approach to updating software\\ncomments. The ﬁrst phase utilizes a predictive model to determine if comment updates\\nare code-indicative. If afﬁrmative, an off-the-shelf heuristic-based approach is employed;\\notherwise, a specially-designed deep learning model is leveraged. Results demonstrated\\nthat this approach is more effective than the current state-of-the-art by 20%.\\nIn the domain of code comment generation, both code embeddings and transformers\\nplay vital roles, each offering distinct advantages. Code embeddings provide a concise\\nrepresentation of code snippets in a continuous vector space, capturing their semantic\\nand syntactic properties. This facilitates the generation of comments by enabling efﬁcient\\nretrieval of similar code segments and assisting in understanding the context for comment\\ngeneration. However, code embeddings may struggle with capturing the intricacies and\\nnuances of code, potentially leading to less contextually relevant comments. Transformers,\\non the other hand, excel in modeling sequential data through self-attention mechanisms,'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 14}, page_content='Electronics 2024, 13, 767 15 of 25\\nallowing them to capture complex patterns and dependencies across code sequences.\\nThis results in more context-aware and informative comments that better align with the\\nunderlying code logic.\\n4.7. Duplicate Code Detection and Similarity\\nThis task involves identifying duplicate code snippets, whether within the same\\ncodebase or across different codebases. Transformers play a crucial role in duplicate\\ncode detection, automating the identiﬁcation of redundant or duplicated code segments\\nwithin a software project. This process is vital for maintaining code quality, enhancing\\nmaintainability, and preventing potential issues associated with code redundancy.\\nKarakatic et al. [92] introduced a novel method for comparing software systems by\\ncomputing the robust Hausdorff distance between semantic source code embeddings\\nof each program component. The authors utilized a pre-trained neural network model,\\ncode2vec, to generate source code vector representations from various open-source li-\\nbraries. Employing different types of robust Hausdorff distance, the proposed method\\ndemonstrated its suitability for gauging semantic similarity.\\nThe presence of code smells and security smells in various training datasets, a ﬁne-\\ntuned transformer-based GPT-Neo model, and a closed-source code generation tool raised\\nconcerns about the cautious application of language models to code generation tasks [93].\\nYu et al. [94] proposed BEDetector, a two-channel feature extraction method for binary\\nsimilarity detection, encompassing contextual semantic feature extraction and a neural GAE\\nmodel. This system achieved impressive detection rates, including 88.8%, 86.7%, and 100%\\nfor resilience against CVE vulnerabilities ssl3-get-key-exchange, ssl3-get-new-session-ticket,\\nand udhcp-get-option, respectively.\\nMateless et al. [95] developed Pkg2Vec to encode software packages and predict their\\nauthors with remarkable accuracy. Comparisons against state-of-the-art algorithms on the\\nISOT datasets revealed Pkg2Vec’s superior performance, showcasing a 13% increase in\\naccuracy. This demonstrated the efﬁcacy of applying deep learning to improve authorship\\nattribution of software packages, providing deep, interpretable features indicating the\\nunique style and intentions of the programmer.\\nCodeBERT showed effectiveness for Type-1 and Type-4 clone detection, although\\nits performance declined for unseen functionalities. Fine-tuning was identiﬁed as a po-\\ntential avenue to marginally improve recall [ 96]. Kovacevic et al. [ 97] investigated the\\neffectiveness of both ML-based and heuristics-based code smell detection models, utilizing\\ndifferent source code representations (metrics and code embeddings) on the large-scale\\nMLCQ dataset. Transfer learning models were evaluated to analyze the impact of mined\\nknowledge on code smell detection.\\nAn efﬁcient transformer-based code clone detection method was proposed by [ 98],\\npromising accurate and rapid identiﬁcation of code clones while signiﬁcantly reducing\\ncomputational cost.\\nTo sum up, in the realm of duplicate code detection and similarity analysis, both\\ncode embeddings and transformers offer unique advantages. Code embeddings distill\\ncode snippets into ﬁxed-length vectors, effectively capturing their semantic and syntactic\\nfeatures. This enables efﬁcient comparison and retrieval of similar code segments, facil-\\nitating the identiﬁcation of duplicate code instances. However, code embeddings may\\nstruggle to capture complex dependencies and contextual nuances, potentially limiting\\ntheir effectiveness in detecting subtle similarities. Transformers, on the other hand, excel\\nin modeling sequential data through self-attention mechanisms, allowing them to capture\\nintricate patterns and long-range dependencies across code sequences. This results in more\\naccurate and context-aware similarity analysis, enabling the detection of subtle variations\\nand similarities within code snippets. Nonetheless, transformers typically require larger\\ncomputational resources and extensive training data compared to code embeddings.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 15}, page_content='Electronics 2024, 13, 767 16 of 25\\n4.8. Code Reﬁnement\\nCode reﬁnement (Figure 4) involves identifying and correcting pieces of code suscepti-\\nble to bugs or vulnerabilities. In the work of Liu et al. [ 99], a software maintenance method\\nwas introduced for debugging method names by evaluating the consistency between their\\nnames and code to identify discrepancies. Through experiments on over 2.1 million Java\\nmethods, the method achieved an F1-measure of 67.9%, surpassing existing techniques by\\n15%. Notably, the authors successfully ﬁxed 66 inconsistent method names in a live study\\non projects in the wild.\\nElectronics 2024, 13, x FOR PEER REVIEW 16 of 27 \\n \\n \\nattribution of software packages, providing deep, interpretable features indicating the \\nunique style and intentions of the programmer. \\nCodeBERT showed eﬀectiveness for Type-1 and Type-4 clone detection, although its \\nperformance declined for unseen functionalities. Fine-tuning was identiﬁed as a potential \\navenue to marginally improve recall [96]. Kovacevic et al. [97] investigated the e ﬀective-\\nness of both ML-based and heuristics-based code smell detection models, utilizing diﬀer-\\nent source code representations (metrics and code embeddings) on the large-scale MLCQ \\ndataset. Transfer learning models were evaluated to analyze the impact of mined \\nknowledge on code smell detection. \\nAn eﬃcient transformer-based code clone detec tion method was proposed by [98], \\npromising accurate and rapid identi ﬁcation of code clones while signi ﬁcantly reducing \\ncomputational cost. \\nTo sum up, in the realm of duplicate code detection and similarity analysis, both code \\nembeddings and transformers offer unique advantages. Code embeddings distill code snip-\\npets into fixed-length vectors, effectively capturing their semantic and syntactic features. \\nThis enables efficient comparison and retrieval of similar code segments, facilitating the \\nidentification of duplicate code instances. However, code embeddings may struggle to cap-\\nture complex dependencies and contextual nuances, potentially limiting their effectiveness \\nin detecting subtle similarities. Transformers, on the other hand, excel in modeling sequen-\\ntial data through self-attention mechanisms, allowing them to capture intricate patterns and \\nlong-range dependencies across code sequences. This results in more accurate and context-\\naware similarity analysis, enabling the detection of subtle variations and similarities within \\ncode snippets. Nonetheless, transformers typically require larger computational resources \\nand extensive training data compared to code embeddings. \\n4.8. Code Reﬁnement \\nCode reﬁnement (Figure 4) involves identifying and correcting pieces of code sus-\\nceptible to bugs or vulnerabilities. In the work of Liu et al. [99], a software maintenance \\nmethod was introduced for debugging method names by evaluating the consistency be-\\ntween their names and code to identify discrepancies. Through experiments on over 2.1 \\nmillion Java methods, the method achieved an F1-measure of 67.9%, surpassing existing \\ntechniques by 15%. Notably, the authors successfully ﬁxed 66 inconsistent method names \\nin a live study on projects in the wild. \\n \\nFigure 4. Code reﬁnement example. \\nCabrera Lozoya et al. [100] extended a st ate-of-the-art approach for representing \\nsource code to also include changes in the source code (commits). Transfer learning was \\nthen applied to classify security-relevant commits. The study demonstrated that represen-\\ntations based on structural information of the code syntax outperformed token-based rep-\\nresentations. Moreover, pre-training with a small dataset (greater than 10^4 samples) for \\na closely related pretext task showed superior performance compared to pre-training with \\na larger dataset (more than 106 samples) and a loosely related pretext task. \\nFigure 4. Code reﬁnement example.\\nCabrera Lozoya et al. [100] extended a state-of-the-art approach for representing source\\ncode to also include changes in the source code (commits). Transfer learning was then\\napplied to classify security-relevant commits. The study demonstrated that representations\\nbased on structural information of the code syntax outperformed token-based representa-\\ntions. Moreover, pre-training with a small dataset (greater than 10ˆ4 samples) for a closely\\nrelated pretext task showed superior performance compared to pre-training with a larger\\ndataset (more than 106 samples) and a loosely related pretext task.\\nWang et al. [101] introduced Cognac, a context-guidance method name recommender\\nthat incorporated global context from methods related by calls. It utilized prior knowledge\\nto adjust method name recommendations and method name consistency checking tasks.\\nCognac outperformed existing approaches on four datasets with F-scores of 63.2%, 60.8%,\\n66.3%, and 68.5%, respectively, achieving an overall accuracy of 76.6%, surpassing MNire\\nby 11.2%, a machine learning approach to check the consistency between the name of a\\ngiven method and its implementation [102].\\nXie et al. [ 103] proposed DeepLink, a model applying code knowledge graph em-\\nbeddings and deep learning to identify links between issue reports and code commits\\nfor software projects. Evaluation of real-world projects demonstrated its superiority over\\ncurrent state-of-the-art solutions.\\nBorovits et al. [104] presented an automated procedure using word embeddings and\\ndeep learning processes to detect inconsistencies between infrastructure as code (IaC) code\\nunits and their names. Experiments on an open-source dataset showed an accuracy range\\nof 78.5% to 91.5% in ﬁnding such inconsistencies.\\nMa et al. [105] introduced Graph-code2vec, a novel self-supervised pre-training ap-\\nproach using code investigation and graph neural networks to generate agnostic task\\nembeddings for software engineering tasks. The proposed technique proved more effective\\nthan existing generic and task-specific learning-based baselines, including GraphCodeBERT.\\nNaturalCC [106] is an open-source code intelligence toolkit, accessible on the website\\n(http://xcodemind.github.io), built on Fairseq and PyTorch technology. It is designed to\\nenable efﬁcient machine learning-based implementation of code intelligence tasks such as\\ncode summarization, code retrieval, and code completion.\\nIn the context of code reﬁnement, both code embeddings and transformers offer dis-\\ntinct advantages. Code embeddings condense code snippets into ﬁxed-length vectors,\\ncapturing their semantic and syntactic properties efﬁciently. This facilitates the reﬁnement'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 16}, page_content='Electronics 2024, 13, 767 17 of 25\\nprocess by enabling quick retrieval of similar code segments and aiding in identifying areas\\nfor improvement. However, code embeddings may struggle to capture complex depen-\\ndencies and nuanced coding patterns, potentially limiting their ability to suggest reﬁned\\nsolutions accurately. Conversely, transformers excel in modeling sequential data through\\nself-attention mechanisms, enabling them to capture intricate patterns and dependencies\\nacross code sequences. This results in more contextually aware reﬁnements, with the ability\\nto suggest solutions that align closely with the underlying logic of the code.\\n4.9. Code Security\\nCode security involves checking source code for exploits that may allow unauthorized\\naccess to restricted resources. Zaharia et al. [107] proposed the use of an intermediate rep-\\nresentation that strikes a balance between stringency to retain security ﬂaws, as per MITRE\\nstandards, and dynamism that does not strictly rely on the lexicon of a programming\\nlanguage. This intermediate representation is based on the semantical clusterization of com-\\nmands in C/C++ programs through word embeddings. These embeddings are distributed\\nthrough the formed intermediate representation to different classiﬁers for recognizing\\nsecurity vulnerability patterns.\\nIn related work, Zaharia et al. [108] developed a security scanning system employing\\nmachine learning algorithms to detect various patterns of vulnerabilities listed in the\\nCommon Weaknesses Enumeration (CWE) from NIST. This system, independent of the\\nprogramming language, achieved a recall value exceeding 0.94, providing a robust defense\\nagainst cyber-attacks.\\nBarr et al. [109] conducted an in-depth analysis of the Fluoride Bluetooth module’s\\nsource code using deep learning, machine learning, heuristics, and combinatorial optimiza-\\ntion techniques. They employed byte-pair encoding to lower dimensionality, embedded\\ntokens into a low-dimensional Euclidean space using LSTM, and created a distance matrix\\nbased on cosines between vectors of functions. The authors used cluster-editing to segment\\nthe graph’s vertices into nearly complete subgraphs, assessing vulnerability risk based on\\nvectors and features of each component.\\nSaletta and Ferretti [110] discussed a technique using natural language processing to\\nrecognize security weaknesses in source code. This involved mapping code to vector space\\nthrough its abstract syntax trees, and supervised learning to capture distinguishing features\\namong different vulnerabilities. Results demonstrated the model’s ability to accurately\\nrecognize various types of security weaknesses.\\nIn the domain of code security, both code embeddings and transformers serve as\\nvaluable tools, each with its unique strengths. Code embeddings offer a compact repre-\\nsentation of code snippets, capturing their semantic and syntactic properties efﬁciently.\\nThis allows for quick analysis of code similarities, aiding in the identiﬁcation of potential\\nsecurity vulnerabilities based on patterns observed in known security issues. However,\\ncode embeddings may struggle to capture complex interactions and subtle security ﬂaws,\\npotentially leading to limitations in detecting sophisticated attacks. Transformers, on the\\nother hand, excel in modeling sequential data and understanding contextual information\\nthrough self-attention mechanisms. This enables them to capture intricate patterns and\\ndependencies across code sequences, resulting in a more comprehensive and context-aware\\nanalysis of code security. However, transformers typically require larger computational\\nresources and extensive training data compared to code embeddings.\\n5. Datasets\\nSimilar to most deep learning models, transformers demand extensive data to exhibit\\noptimal performance. This becomes a notable challenge in the ﬁeld of programming,\\nwhere acquiring high-quality datasets is not as straightforward as in natural language\\nprocessing (NLP).\\nTo tackle this issue, initiatives like CodeSearchNet (https://github.com/github/\\nCodeSearchNet, accessed on 10 January 2024) and CodeXGLUE (https://github.com/'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 17}, page_content='Electronics 2024, 13, 767 18 of 25\\nmicrosoft/CodeXGLUE, accessed on 10 January 2024) have been established, provid-\\ning valuable datasets for training and evaluating code-related models. CodeSearchNet\\nstands out as a large-scale dataset, encompassing over 6 million GitHub repositories and\\n4.2 million code ﬁles. It spans six programming languages: Java, Python, JavaScript, Go,\\nRuby, and PHP . CodeBERT has undergone training on this comprehensive dataset, enhanc-\\ning its capacity to learn cross-lingual representations of source code.\\nCodeXGLUE, on the other hand, serves as a benchmark dataset strategically crafted\\nfor the advancement and assessment of code intelligence methods, speciﬁcally focusing on\\ncode completion and code retrieval tasks. This dataset incorporates 14 tasks across various\\nprogramming languages such as Python, Java, C++, and PHP . CodeBERT, recognizing the\\nsigniﬁcance of diverse challenges, has undergone training on this dataset to elevate its\\nproﬁciency in code intelligence tasks.\\n6. Conclusions\\nCode embeddings serve as vector representations of source code, acquired through\\ndeep learning techniques. They adeptly encapsulate the lexical, syntactic, and semantic\\nintricacies of code, projecting them into a high-dimensional vector space. Various methods,\\nincluding recurrent neural networks (RNNs), convolutional neural networks (CNNs),\\nand graph neural networks (GNNs), are employed to generate code embeddings. These\\nmethods utilize input source code to establish a mapping between code tokens and their\\ncorresponding vector representations. Subsequently, the vector representations become\\ninputs for downstream natural language processing (NLP) tasks.\\nCode embeddings prove formidable in capturing the semantic essence of code, distin-\\nguishing themselves from traditional approaches reliant on handcrafted features. Unlike\\ntheir predecessors, code embeddings autonomously learn semantic relationships between\\ndistinct code tokens, enhancing efﬁciency in grasping nuances like variable and function\\ndependencies. Furthermore, code embeddings exhibit language agnosticism, enabling\\ntraining on diverse programming languages and proving valuable for tasks demanding\\ncode comprehension across language boundaries. Their capacity to generalize effectively\\nto unseen code snippets stems from training on extensive code corpora, enabling the\\nabsorption of general patterns and structures prevalent in code.\\nTransformers, distinguished by their self-attention mechanisms, have excelled in\\nlearning from substantial datasets in an end-to-end manner, eliminating the need for\\ntask-speciﬁc feature engineering. This adaptability allows a single transformer model to\\nassist in multiple programming tasks, facilitated by ﬁne-tuning speciﬁc languages or tasks.\\nNevertheless, the performance of a transformer model ﬁne-tuned for one task may not\\nseamlessly translate to another task without further adaptation.\\nThe application of transformers extends from natural language processing (NLP) to\\ncode representation and generation tasks. Self-attention mechanisms empower transform-\\ners to discern long-range dependencies within input text, enhancing their ability to capture\\ncontextual nuances in code.\\nIn Table 1, we summarize the literature review presented in this paper.\\nNext, we try to compare the use of code embeddings and transformers in the nine\\nreferred tasks.\\n• Code summarization:\\n• Code embeddings capture the semantic meaning of code snippets, enabling\\nsummarization through techniques like clustering or similarity-based retrieval.\\n• Transformers can learn contextual representations of code, allowing them to gen-\\nerate summaries by attending to relevant parts of the code and its surrounding context.\\n• Bug detection and correction:\\n• By learning embeddings from code, similarity metrics can be applied to detect\\nsimilar code segments containing known bugs, or to identify anomalous patterns.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 18}, page_content='Electronics 2024, 13, 767 19 of 25\\n• Transformers can learn to detect bugs by learning from labeled data, and they can\\nalso be ﬁne-tuned for speciﬁc bug detection tasks. For bug correction, they can\\ngenerate patches by learning from examples of ﬁxed code.\\n• Code completion:\\n• Embeddings can be used to predict the next tokens in code, enabling code com-\\npletion by suggesting relevant completions based on learned representations.\\n• Transformers excel at predicting sequences and can provide context-aware code\\ncompletions by considering the surrounding code.\\n• Code generation:\\n• Code embeddings can be used to generate code by sampling from the learned\\nembedding space, potentially leading to diverse outputs.\\n• Transformers can generate code by conditioning on input sequences and gen-\\nerating output sequences token by token, allowing for precise control over the\\ngeneration process.\\n• Code translation:\\n• Embeddings can be leveraged for mapping code from one programming language\\nto another by aligning representations of similar functionality across languages.\\n• Transformers can be trained for sequence-to-sequence translation tasks, allowing\\nfor direct translation of code between different programming languages.\\n• Code comment generation:\\n• By learning embeddings from code-comment pairs, embeddings can be used\\nto generate comments for code by predicting the most likely comment given\\nthe code.\\n• Transformers can be trained to generate comments by conditioning on code and\\ngenerating natural language descriptions, capturing the context and intent of\\nthe code.\\n• Duplicate code detection and similarity:\\n• Similarity metrics based on embeddings can efﬁciently identify duplicate or\\nsimilar code snippets by measuring the distance between their embeddings.\\n• Transformers can learn contextual representations of code, enabling them to iden-\\ntify duplicate or similar code snippets by comparing their representations directly.\\n• Code reﬁnement:\\n• Embeddings can be used to reﬁne code by suggesting improvements based on\\nlearned representations and similarity to high-quality code.\\n• Transformers can be ﬁne-tuned for code reﬁnement tasks, such as code formatting\\nor refactoring, by learning from labeled data or reinforcement learning.\\n• Code security:\\n• Embeddings can be utilized for detecting security vulnerabilities by identifying\\npatterns indicative of vulnerabilities or by comparing code snippets to known\\nvulnerable code.\\n• Transformers can be trained to detect security vulnerabilities by learning from\\nlabeled data, and they can also be used for code analysis to identify potential\\nsecurity risks through contextual understanding.\\nFinally, for AI-assisted programming tasks, leveraging both code embeddings and\\ntransformers can signiﬁcantly enhance the efﬁciency and effectiveness of the development\\nprocess. By combining the strengths of both techniques, developers can beneﬁt from a\\ncomprehensive AI-assisted programming environment that offers efﬁcient code analysis,\\naccurate recommendations, and context-aware assistance throughout the development\\nlifecycle. This hybrid approach ensures that developers can leverage the simplicity and\\nefﬁciency of code embeddings alongside the contextual awareness and sophistication of\\ntransformers, thereby maximizing productivity and code quality.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 19}, page_content='Electronics 2024, 13, 767 20 of 25\\nTable 1. Literature overview.\\nTasks Publications\\nCode summarization [16,43–45,48–51]—Code embedding\\n[31,46,47,52]—Transformer\\nBug detection and correction [53–57,61,68,69]—Code embedding\\n[38,58–60,62–67]—Transformer\\nCode completion [29,30,71–75]—Transformer\\nCode generation process [23]—Code embedding\\n[3,76–79]—Transformer\\nCode translation [80,81,84]—Code embedding\\n[32,82,83]—Transformer\\nCode comment generation\\n[85,87,88,90]—Code embedding\\n[86]-Code embedding—Transformer\\n[37,89]—Transformer\\n[91]—Custom\\nDuplicate code detection and similarity\\n[92,94,95]—Code embedding\\n[92,96,98]—Transformer\\n[97]—Custom\\nCode reﬁnement [99–105]—Code embedding\\n[106]—Transformer\\nCode security [107–110]—Code embedding\\nEthical Considerations\\nVarious ethical considerations come to the forefront when employing transformers, or\\nany form of AI, for programming tasks. These considerations encompass aspects related to\\nprivacy, bias, transparency, and accountability [111].\\nA primary ethical concern centers around the potential invasion of privacy inherent in\\nthe utilization of transformers for programming. Given that transformers are engineered\\nto analyze and process extensive datasets, including personal or sensitive information,\\nquestions arise concerning the storage, utilization, and safeguarding of these data. A critical\\naspect involves ensuring individuals are informed about the use of their information for\\nprogramming purposes.\\nAnother ethical dimension revolves around the prospect of bias within the data used\\nfor training the transformer. Should the analyzed data exhibit biases or gaps, it could\\nprofoundly impact the decisions made by the transformer, potentially perpetuating existing\\nbiases and fostering discrimination. Therefore, it becomes imperative to curate training\\ndata that are diverse, representative, and devoid of bias.\\nTransparency emerges as a pivotal ethical consideration in the integration of trans-\\nformers for AI-assisted programming tasks. Programmers must possess a comprehensive\\nunderstanding of the inner workings of the transformer and the rationale behind its deci-\\nsions. Transparency serves not only debugging and troubleshooting purposes but also acts\\nas a safeguard against the occurrence of unethical or harmful decisions.\\nMoreover, accountability assumes a critical role in the ethical framework surround-\\ning the use of transformers in programming. With advancing technology, ascertaining\\nresponsibility for the decisions made by a transformer becomes increasingly challenging.\\nIn scenarios involving errors or ethical breaches, establishing clear frameworks for account-\\nability and liability becomes indispensable. These frameworks serve to assign responsibility\\nand address any ensuing issues with precision and fairness.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 20}, page_content='Electronics 2024, 13, 767 21 of 25\\nAuthor Contributions: Conceptualization, S.K.; methodology, S.K.; investigation, S.K., M.T. and V .V .;\\nresources, S.K.; data curation, M.T.; writing—original draft preparation, S.K.; writing—review and\\nediting, M.T.; supervision, V .V .; project administration, V .V . All authors have read and agreed to the\\npublished version of the manuscript.\\nFunding: This research received no external funding.\\nConﬂicts of Interest: The authors declare no conﬂicts of interest.\\nReferences\\n1. Hindle, A.; Barr, E.T.; Su, Z.; Gabel, M.; Devanbu, P . On The Naturalness of Software. In Proceedings of the 34th International\\nConference on Software Engineering (ICSE), Zurich, Switzerland, 2–9 June 2012; pp. 837–847.\\n2. Shani, I. Survey Reveals AI’s Impact on the Developer Experience. 2023. Available online: https://github.blog/2023-06-13\\n-survey-reveals-ais-impact-on-the-developer-experience (accessed on 24 December 2023).\\n3. Svyatkovskiy, A.; Deng, S.K.; Fu, S.; Sundaresan, N. IntelliCode compose: Code generation using transformer. In Proceedings of\\nthe 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software\\nEngineering, Online, 8–13 November 2020. [CrossRef]\\n4. Bird, C.; Ford, D.; Zimmermann, T.; Forsgren, N.; Kalliamvakou, E.; Lowdermilk, T.; Gazit, I. Taking Flight with Copilot.Commun.\\nACM 2023, 66, 56–62. [CrossRef]\\n5. Friedman, N. Introducing GitHub Copilot: Your AI Pair Programmer. 2021. Available online: https://github.com/features/\\ncopilot (accessed on 24 December 2023).\\n6. Chen, M.; Tworek, J.; Jun, H.; Yuan, Q.; de Oliveira Pinto, H.P .; Kaplan, J.; Edwards, H.; Burda, Y.; Joseph, N.; Brockman, G.; et al.\\nEvaluating large language models trained on code. arXiv 2021, arXiv:2107.03374. [CrossRef]\\n7. Li, Y.; Choi, D.; Chung, J.; Kushman, N.; Schrittwieser, J.; Leblond, R.; Eccles, T.; Keeling, J.; Gimeno, F.; Dal Lago, A.; et al.\\nCompetition-level Code Generation with Alphacode. Science 2022, 378, 1092–1097. [CrossRef] [PubMed]\\n8. Parashar, B.; Kaur, I.; Sharma, A.; Singh, P .; Mishra, D. Revolutionary transformations in twentieth century: Making AI-assisted\\nsoftware development. In Computational Intelligence in Software Modeling; De Gruyter: Berlin, Germany, 2022. [CrossRef]\\n9. Gulwani, S. AI-assisted programming: Applications, user experiences, and neuro-symbolic techniques (keynote). In Proceedings\\nof the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering,\\nSingapore, 14–18 November 2022. [CrossRef]\\n10. Vaithilingam, P .; Zhang, T.; Glassman, E.L. Expectation vs. experience: Evaluating the usability of code generation tools powered\\nby large language models. In Proceedings of the CHI Conference on Human Factors in Computing Systems Extended Abstracts,\\nNew Orleans, LA, USA, 29 April–5 May 2022; Association for Computing Machinery: New York, NY, USA, 2022; pp. 1–7.\\n11. Fernandez, R.C.; Elmore, A.J.; Franklin, M.J.; Krishnan, S.; Tan, C. How Large Language Models Will Disrupt Data Management.\\nProc. VLDB Endow. 2023, 16, 3302–3309. [CrossRef]\\n12. Zhou, H.; Li, J. A Case Study on Scaffolding Exploratory Data Analysis for AI Pair Programmers. In Proceedings of the 2023 CHI\\nConference on Human Factors in Computing Systems, Hamburg, Germany, 23–28 April 2023; pp. 1–7. [CrossRef]\\n13. Kazemitabaar, M.; Chow, J.; Ma, C.K.T.; Ericson, B.J.; Weintrop, D.; Grossman, T. Studying the effect of AI Code Generators on\\nSupporting Novice Learners in Introductory Programming. In Proceedings of the 2023 CHI Conference on Human Factors in\\nComputing Systems, Hamburg, Germany, 23–28 April 2023; pp. 1–23. [CrossRef]\\n14. Daun, M.; Brings, J. How ChatGPT Will Change Software Engineering Education. In Proceedings of the 2023 Conference on\\nInnovation and Technology in Computer Science Education V . 1, Turku, Finland, 7–12 July 2023; pp. 110–116. [CrossRef]\\n15. Prather, J.; Reeves, B.N.; Denny, P .; Becker, B.A.; Leinonen, J.; Luxton-Reilly, A.; Powell, G.; Finnie-Ansley, J.; Santos, E.A. “It’s\\nWeird That It Knows What I Want”: Usability and Interactions with Copilot for Novice Programmers. ACM Trans. Comput.\\nInteract. 2023, 31, 1–31. [CrossRef]\\n16. Sui, Y.; Cheng, X.; Zhang, G.; Wang, H. Flow2Vec: Value-ﬂow-based precise code embedding.Proc. ACM Program. Lang. 2020,\\n4, 233. [CrossRef]\\n17. Rabin, M.R.I.; Mukherjee, A.; Gnawali, O.; Alipour, M.A. Towards demystifying dimensions of source code embeddings. In\\nProceedings of the 1st ACM SIGSOFT International Workshop on Representation Learning for Software Engineering and Program\\nLanguages, Online, 8–13 November 2020. [CrossRef]\\n18. Azcona, D.; Arora, P .; Hsiao, I.-H.; Smeaton, A. user2code2vec: Embedding for Proﬁling Students Based on Distributinal\\nRepresentations of Source Code. In Proceedings of the 9th International Conference on Learning Analytics and Knowledge,\\nTempe, AZ, USA, 4–8 March 2019. [CrossRef]\\n19. Ding, Z.; Li, H.; Shang, W.; Chen, T.-H. Towards Learning Generalizable Code Embeddings Using Task-agnostic Graph\\nConvolutional Networks. ACM Trans. Softw. Eng. Methodol. 2023, 32, 48. [CrossRef]\\n20. Wolf, T.; Debut, L.; Sanh, V .; Chaumond, J.; Delangue, C.; Moi, A.; Cistac, P .; Rault, T.; Louf, R.; Funtowicz, M.; et al. Transformers:\\nState-of-the-Art Natural Language Processing. In EMNLP 2020—Conference on Empirical Methods in Natural Language Processing:\\nSystems Demonstrations; Association for Computational Linguistics: Kerrville, TX, USA, 2020; pp. 38–45.'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 21}, page_content='Electronics 2024, 13, 767 22 of 25\\n21. Chirkova, N.; Troshin, S. Empirical study of transformers for source code. In Proceedings of the 29th ACM Joint Meeting on\\nEuropean Software Engineering Conference and Symposium on the Foundations of Software Engineering, Athens, Greece, 23–28\\nAugust 2021. [CrossRef]\\n22. Song, Y.; Shi, S.; Li, J.; Zhang, H. Directional skip-gram: Explicitly distinguishing left and right context forword embeddings. In\\nProceedings of the NAACL HLT 2018—2018 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, New Orleans, LA, USA, 1–6 June 2018; pp. 175–180.\\n23. Hu, H.; Chen, Q.; Liu, Z. Code Generation from Supervised Code Embeddings. In Neural Information Processing; Springer: Cham,\\nSwitzerland, 2019; pp. 388–396. [CrossRef]\\n24. Sikka, J.; Satya, K.; Kumar, Y.; Uppal, S.; Shah, R.R.; Zimmermann, R. Learning Based Methods for Code Runtime Complexity\\nPrediction. In Advances in Information Retrieval; Springer: Cham, Switzerland, 2020; pp. 313–325. [CrossRef]\\n25. Kang, H.J.; Bissyande, T.F.; Lo, D. Assessing the Generalizability of Code2vec Token Embeddings. In Proceedings of the 2019 34th\\nIEEE/ACM International Conference on Automated Software Engineering (ASE), San Diego, CA, USA, 11–15 November 2019.\\n[CrossRef]\\n26. Romanov, V .; Ivanov, V . Prediction of Types in Python with Pre-trained Graph Neural Networks. In Proceedings of the 2022\\nIvannikov Memorial Workshop (IVMEM), Moscow, Russia, 23–24 September 2022. [CrossRef]\\n27. Ding, Z.; Li, H.; Shang, W.; Chen, T.-H.P . Can pre-trained code embeddings improve model performance? Revisiting the use of\\ncode embeddings in software engineering tasks. Empir. Softw. Eng. 2022, 27, 63. [CrossRef]\\n28. Shaw, P .; Uszkoreit, J.; Vaswani, A. Self-attention with relative position representations. In Proceedings of the NAACL HLT\\n2018—2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\\nTechnologies, New Orleans, LA, USA, 1–6 June 2018; pp. 464–468.\\n29. Yang, H.; Kuang, L. CCMC: Code Completion with a Memory Mechanism and a Copy Mechanism. In Proceedings of the EASE\\n2021: Evaluation and Assessment in Software Engineering, Trondheim, Norway, 21–23 June 2021. [CrossRef]\\n30. Ciniselli, M.; Cooper, N.; Pascarella, L.; Mastropaolo, A.; Aghajani, E.; Poshyvanyk, D.; Di Penta, M.; Bavota, G. An Empirical\\nStudy on the Usage of Transformer Models for Code Completion. IEEE Trans. Softw. Eng. 2021, 48, 4818–4837. [CrossRef]\\n31. Gong, Z.; Gao, C.; Wang, Y.; Gu, W.; Peng, Y.; Xu, Z. Source Code Summarization with Structural Relative Position Guided\\nTransformer. In Proceedings of the 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering\\n(SANER), Honolulu, HI, USA, 15–18 March 2022. [CrossRef]\\n32. Hassan, M.H.; Mahmoud, O.A.; Mohammed, O.I.; Baraka, A.Y.; Mahmoud, A.T.; Yousef, A.H. Neural Machine Based Mobile\\nApplications Code Translation. In Proceedings of the 2020 2nd Novel Intelligent and Leading Emerging Sciences Conference\\n(NILES), Giza, Egypt, 24–26 October 2020. [CrossRef]\\n33. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. BERT: Pre-training of deep bidirectional transformers for language understanding.\\nIn Proceedings of the NAACL HLT 2019—2019 Conference of the North American Chapter of the Association for Computational\\nLinguistics: Human Language Technologies, Minneapolis, MN, USA, 2–7 June 2019; pp. 4171–4186.\\n34. Sengupta, A.; Kumar, A.; Bhattacharjee, S.K.; Roy, S. Gated Transformer for Robust De-noised Sequence-to-Sequence Modelling.\\nIn Proceedings of the 2021 Findings of the Association for Computational Linguistics, Punta Cana, Dominican Republic, 7–11\\nNovember 2021.\\n35. Wu, C.; Wu, F.; Ge, S.; Qi, T.; Huang, Y.; Xie, X. Neural news recommendation with multi-head self-attention. In Proceedings of\\nthe 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural\\nLanguage Processing, Hong Kong, China, 3–7 November 2019.\\n36. Chernyavskiy, A.; Ilvovsky, D.; Nakov, P . Transformers: ‘The End of History’ for Natural Language Processing? In Ma-\\nchine Learning and Knowledge Discovery in Databases ; Lecture Notes in Computer Science; Springer: Cham, Switzerland, 2021;\\npp. 677–693. [CrossRef]\\n37. Feng, Z.; Guo, D.; Tang, D.; Duan, N.; Feng, X.; Gong, M.; Shou, L.; Qin, B.; Liu, T.; Jiang, D.; et al. CodeBERT: A pre-trained\\nmodel for programming and natural languages. In Findings of the Association for Computational Linguistics Findings of ACL: EMNLP\\n2020; Association for Computational Linguistics: Kerrville, TX, USA, 2020; pp. 1536–1547.\\n38. Zhou, X.; Han, D.; Lo, D. Assessing Generalizability of CodeBERT. In Proceedings of the 2021 IEEE International Conference on\\nSoftware Maintenance and Evolution (ICSME), Luxembourg, 27 September–1 October 2021. [CrossRef]\\n39. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y.; Li, W.; Liu, P .J. Exploring the limits of transfer\\nlearning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res. 2020, 21, 1–67. Available online: http://jmlr.org/papers/v21/\\n20-074.html (accessed on 24 December 2023).\\n40. Brown, T.B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P .; Neelakantan, A.; Shyam, P .; Sastry, G.; Askell, A.; et al.\\nLanguage models are few-shot learners. Adv. Neural Inf. Process. Syst. 2020, 33, 1877–1901.\\n41. Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; Le, Q.V . XLNet: Generalized autoregressive pretraining for language\\nunderstanding. Adv. Neural Inf. Process. Syst. 2019, 32, 5753–5763.\\n42. Zhang, F.; Yu, X.; Keung, J.; Li, F.; Xie, Z.; Yang, Z.; Ma, C.; Zhang, Z. Improving Stack Overﬂow question title generation with\\ncopying enhanced CodeBERT model and bi-modal information. Inf. Softw. Technol. 2022, 148, 106922. [CrossRef]\\n43. Liu, K.; Yang, G.; Chen, X.; Zhou, Y. EL-CodeBert: Better Exploiting CodeBert to Support Source Code-Related Classiﬁcation\\nTasks. In Proceedings of the 13th Asia-Paciﬁc Symposium on Internetware, Hohhot, China, 11–12 June 2022. [CrossRef]'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 22}, page_content='Electronics 2024, 13, 767 23 of 25\\n44. Wang, R.; Zhang, H.; Lu, G.; Lyu, L.; Lyu, C. Fret: Functional Reinforced Transformer with BERT for Code Summarization.IEEE\\nAccess 2020, 8, 135591–135604. [CrossRef]\\n45. Yang, Z.; Keung, J.; Yu, X.; Gu, X.; Wei, Z.; Ma, X.; Zhang, M. A Multi-Modal Transformer-based Code Summarization Approach\\nfor Smart Contracts. In Proceedings of the 2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),\\nMadrid, Spain, 20–21 May 2021. [CrossRef]\\n46. Hou, S.; Chen, L.; Ye, Y. Summarizing Source Code from Structure and Context. In Proceedings of the 2022 International Joint\\nConference on Neural Networks (IJCNN), Padua, Italy, 18–23 July 2022. [CrossRef]\\n47. Wang, Y.; Dong, Y.; Lu, X.; Zhou, A. GypSum: Learning hybrid representations for code summarization. In Proceedings of the\\n30th IEEE/ACM International Conference on Program Comprehension, Online, 16–17 May 2022. [CrossRef]\\n48. Gu, J.; Salza, P .; Gall, H.C. Assemble Foundation Models for Automatic Code Summarization. In Proceedings of the 2022 IEEE\\nInternational Conference on Software Analysis, Evolution and Reengineering (SANER), Honolulu, HI, USA, 15–18 March 2022.\\n[CrossRef]\\n49. Ma, Z.; Gao, Y.; Lyu, L.; Lyu, C. MMF3: Neural Code Summarization Based on Multi-Modal Fine-Grained Feature Fusion. In\\nProceedings of the 16th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement, Helsinki,\\nFinland, 29–23 September 2022. [CrossRef]\\n50. Gao, Y.; Lyu, C. M2TS: Multi-scale multi-modal approach based on transformer for source code summarization. In Proceedings of\\nthe 30th IEEE/ACM International Conference on Program Comprehension, Online, 16–17 May 2022. [CrossRef]\\n51. Ferretti, C.; Saletta, M. Naturalness in Source Code Summarization. How Signiﬁcant is it? In Proceedings of the 2023 IEEE/ACM\\n31st International Conference on Program Comprehension (ICPC), Melbourne, VI, Australia, 15–16 May 2023. [CrossRef]\\n52. Choi, Y.; Na, C.; Kim, H.; Lee, J.-H. READSUM: Retrieval-Augmented Adaptive Transformer for Source Code Summarization.\\nIEEE Access 2023, 11, 51155–51165. [CrossRef]\\n53. Aladics, T.; Jasz, J.; Ferenc, R. Bug Prediction Using Source Code Embedding Based on Doc2Vec. InComputational Science and Its\\nApplications; Lecture Notes in Computer Science; Springer: Cham, Switzerland, 2021; pp. 382–397. [CrossRef]\\n54. Cheng, X.; Zhang, G.; Wang, H.; Sui, Y. Path-sensitive code embedding via contrastive learning for software vulnerability\\ndetection. In Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis, Online,\\nRepublic of Korea, 18–22 July 2022. [CrossRef]\\n55. Hegedus, P .; Ferenc, R. Static Code Analysis Alarms Filtering Reloaded: A New Real-World Dataset and its ML-Based Utilization.\\nIEEE Access 2022, 10, 55090–55101. [CrossRef]\\n56. Bagheri, A.; Hegedus, P . A Comparison of Different Source Code Representation Methods for Vulnerability Prediction in Python.\\nIn Quality of Information and Communications Technology; Springer: Cham, Switzerland, 2021; pp. 267–281. [CrossRef]\\n57. Gomes, L.; da Silva Torres, R.; Cortes, M.L. BERT- and TF-IDF-based feature extraction for long-lived bug prediction in FLOSS: A\\ncomparative study. Inf. Softw. Technol. 2023, 160, 107217. [CrossRef]\\n58. Pan, C.; Lu, M.; Xu, B. An Empirical Study on Software Defect Prediction Using CodeBERT Model. Appl. Sci. 2021, 11, 4793.\\n[CrossRef]\\n59. Ma, X.; Keung, J.W.; Yu, X.; Zou, H.; Zhang, J.; Li, Y. AttSum: A Deep Attention-Based Summarization Model for Bug Report Title\\nGeneration. IEEE Trans. Reliab. 2023, 72, 1663–1677. [CrossRef]\\n60. Mahbub, P .; Shuvo, O.; Rahman, M.M. Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation. In\\nProceedings of the 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE), Melbourne, VI, Australia,\\n14–20 May 2023. [CrossRef]\\n61. Csuvik, V .; Horvath, D.; Lajko, M.; Vidacs, L. Exploring Plausible Patches Using Source Code Embeddings in JavaScript. In\\nProceedings of the 2021 IEEE/ACM International Workshop on Automated Program Repair (APR), Madrid, Spain, 1 June 2021.\\n[CrossRef]\\n62. Mashhadi, E.; Hemmati, H. Applying CodeBERT for Automated Program Repair of Java Simple Bugs. In Proceedings of the 2021\\nIEEE/ACM 18th International Conference on Mining Software Repositories (MSR), Madrid, Spain, 17–19 May 2021. [CrossRef]\\n63. Chakraborty, S.; Ray, B. On Multi-Modal Learning of Editing Source Code. In Proceedings of the 2021 36th IEEE/ACM\\nInternational Conference on Automated Software Engineering (ASE), Melbourne, VI, Australia, 15–19 November 2021. [CrossRef]\\n64. Lajko, M.; Csuvik, V .; Vidacs, L. Towards JavaScript program repair with generative pre-trained transformer (GPT-2). In\\nProceedings of the Third International Workshop on Automated Program Repair, Pittsburgh, PA, USA, 19 May 2022. [CrossRef]\\n65. Chi, J.; Qu, Y.; Liu, T.; Zheng, Q.; Yin, H. SeqTrans: Automatic Vulnerability Fix Via Sequence to Sequence Learning.IEEE Trans.\\nSoftw. Eng. 2023, 49, 564–585. [CrossRef]\\n66. Chen, Z.; Kommrusch, S.; Monperrus, M. Neural Transfer Learning for Repairing Security Vulnerabilities in C Code. IEEE Trans.\\nSoftw. Eng. 2023, 49, 147–165. [CrossRef]\\n67. Kim, T.; Yang, G. Predicting Duplicate in Bug Report Using Topic-Based Duplicate Learning with Fine Tuning-Based BERT\\nAlgorithm. IEEE Access 2022, 10, 129666–129675. [CrossRef]\\n68. Dinella, E.; Ryan, G.; Mytkowicz, T.; Lahiri, S.K. TOGA: A neural method for test oracle generation. In Proceedings of the 44th\\nInternational Conference on Software Engineering, Pittsburgh, PA, USA, 21–29 May 2022. [CrossRef]\\n69. da Silva, A.F.; Borin, E.; Pereira, F.M.Q.; Queiroz, N.L.; Napoli, O.O. Program representations for predictive compilation: State of\\naffairs in the early 20’s.J. Comput. Lang. 2022, 73, 101171. [CrossRef]'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 23}, page_content='Electronics 2024, 13, 767 24 of 25\\n70. Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; Stoyanov, V . RoBERTa: A Robustly\\nOptimized BERT Pretraining Approach. arXiv 2019, arXiv:1907.11692.\\n71. Dai, Z.; Yang, Z.; Yang, Y.; Carbonell, J.; Le, Q.V .; Salakhutdinov, R. Transformer-XL: Attentive language models beyond a\\nﬁxed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy,\\n28 July–2 August 2019; pp. 2978–2988.\\n72. Izadi, M.; Gismondi, R.; Gousios, G. CodeFill: Multi-token code completion by jointly learning from structure and naming\\nsequences. In Proceedings of the 44th International Conference on Software Engineering, Pittsburgh, PA, USA, 21–29 May 2022.\\n[CrossRef]\\n73. Liu, F.; Li, G.; Zhao, Y.; Jin, Z. Multi-task learning based pre-trained language model for code completion. In Proceedings of the\\n35th IEEE/ACM International Conference on Automated Software Engineering, Virtual Event Australia, 21–25 December 2020.\\n[CrossRef]\\n74. Lewis, M.; Liu, Y.; Goyal, N.; Ghazvininejad, M.; Mohamed, A.; Levy, O.; Stoyanov, V .; Zettlemoyer, L. BART: Denoising\\nsequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the\\nAnnual Meeting of the Association for Computational Linguistics, Online, 5–10 July 2020; pp. 7871–7880.\\n75. Kim, S.; Zhao, J.; Tian, Y.; Chandra, S. Code Prediction by Feeding Trees to Transformers. In Proceedings of the 2021 IEEE/ACM\\n43rd International Conference on Software Engineering (ICSE), Madrid, Spania, 22–30 May 2021. [CrossRef]\\n76. Gemmell, C.; Rossetto, F.; Dalton, J. Relevance Transformer: Generating Concise Code Snippets with Relevance Feedback. In\\nProceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual\\nEvent China, 25–30 July 2020. [CrossRef]\\n77. Soliman, A.S.; Hadhoud, M.M.; Shaheen, S.I. MarianCG: A code generation transformer model inspired by machine translation.\\nJ. Eng. Appl. Sci. 2022, 69, 104. [CrossRef]\\n78. Yang, G.; Zhou, Y.; Chen, X.; Zhang, X.; Han, T.; Chen, T. ExploitGen: Template-augmented exploit code generation based on\\nCodeBERT. J. Syst. Softw. 2023, 197, 111577. [CrossRef]\\n79. Laskari, N.K.; Reddy, K.A.N.; Indrasena Reddy, M. Seq2Code: Transformer-Based Encoder-Decoder Model for Python Source\\nCode Generation. In Third Congress on Intelligent Systems; Lecture Notes in Networks and Systems; Springer: Singapore, 2023;\\npp. 301–309. [CrossRef]\\n80. Bui, N.D.Q.; Yu, Y.; Jiang, L. Bilateral Dependency Neural Networks for Cross-Language Algorithm Classiﬁcation. In Proceedings\\nof the 2019 IEEE 26th International Conference on Software Analysis, Evolution and Reengineering (SANER), Hangzhou, China,\\n24–27 February 2019. [CrossRef]\\n81. Yang, G.; Zhou, Y.; Chen, X.; Yu, C. Fine-grained Pseudo-code Generation Method via Code Feature Extraction and Transformer.\\nIn Proceedings of the 2021 28th Asia-Paciﬁc Software Engineering Conference (APSEC), Taipei, Taiwan, 6–9 December 2021.\\n[CrossRef]\\n82. Alokla, A.; Gad, W.; Nazih, W.; Aref, M.; Salem, A.-B. Retrieval-Based Transformer Pseudocode Generation.Mathematics 2022,\\n10, 604. [CrossRef]\\n83. Gad, W.; Alokla, A.; Nazih, W.; Aref, M.; Salem, A. DLBT: Deep Learning-Based Transformer to Generate Pseudo-Code from\\nSource Code. Comput. Mater. Contin. 2022, 70, 3117–3132. [CrossRef]\\n84. Acharjee, U.K.; Areﬁn, M.; Hossen, K.M.; Uddin, M.N.; Uddin, M.A.; Islam, L. Sequence-to-Sequence Learning-Based Conversion\\nof Pseudo-Code to Source Code Using Neural Translation Approach. IEEE Access 2022, 10, 26730–26742. [CrossRef]\\n85. Shahbazi, R.; Sharma, R.; Fard, F.H. API2Com: On the Improvement of Automatically Generated Code Comments Using API\\nDocumentations. In Proceedings of the 2021 IEEE/ACM 29th International Conference on Program Comprehension (ICPC),\\nMadrid, Spain, 20–21 May 2021. [CrossRef]\\n86. Yang, G.; Chen, X.; Cao, J.; Xu, S.; Cui, Z.; Yu, C.; Liu, K. ComFormer: Code Comment Generation via Transformer and Fusion\\nMethod-based Hybrid Code Representation. In Proceedings of the 2021 8th International Conference on Dependable Systems\\nand Their Applications (DSA), Yinchuan, China, 5–6 August 2021. [CrossRef]\\n87. Chakraborty, S.; Ahmed, T.; Ding, Y.; Devanbu, P .T.; Ray, B. NatGen: Generative pre-training by “naturalizing” source code. In\\nProceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software\\nEngineering, Singapore, 14–18 November 2022. [CrossRef]\\n88. Geng, M.; Wang, S.; Dong, D.; Wang, H.; Cao, S.; Zhang, K.; Jin, Z. Interpretation-based Code Summarization. In Proceedings\\nof the 2023 IEEE/ACM 31st International Conference on Program Comprehension (ICPC), Melbourne, VI, Australia, 15–16\\nMay 2023. [CrossRef]\\n89. Thongtanunam, P .; Pornprasit, C.; Tantithamthavorn, C. AutoTransform: Automated code transformation to support modern\\ncode review process. In Proceedings of the 44th International Conference on Software Engineering, Pittsburgh, PA, USA, 21–29\\nMay 2022. [CrossRef]\\n90. Yu, C.; Yang, G.; Chen, X.; Liu, K.; Zhou, Y. BashExplainer: Retrieval-Augmented Bash Code Comment Generation based on\\nFine-tuned CodeBERT. In Proceeding of the 2022 IEEE International Conference on Software Maintenance and Evolution (ICSME),\\nLimassol, Cyprus, 3–7 October 2022. [CrossRef]\\n91. Lin, B.; Wang, S.; Liu, Z.; Xia, X.; Mao, X. Predictive Comment Updating with Heuristics and AST-Path-Based Neural Learning:\\nA Two-Phase Approach. IEEE Trans. Softw. Eng. 2023, 49, 1640–1660. [CrossRef]'),\n",
              " Document(metadata={'source': 'research_papers/AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf', 'page': 24}, page_content='Electronics 2024, 13, 767 25 of 25\\n92. Karakatic, S.; MiloÅ¡evic, A.; Hericko, T. Software system comparison with semantic source code embeddings. Empir. Softw. Eng.\\n2022, 27, 70. [CrossRef]\\n93. Siddiq, M.L.; Majumder, S.H.; Mim, M.R.; Jajodia, S.; Santos, J.C.S. An Empirical Study of Code Smells in Transformer-based\\nCode Generation Techniques. In Proceedings of the 2022 IEEE 22nd International Working Conference on Source Code Analysis\\nand Manipulation (SCAM), Limassol, Cyprus, 3 October 2022. [CrossRef]\\n94. Yu, L.; Lu, Y.; Shen, Y.; Huang, H.; Zhu, K. BEDetector: A Two-Channel Encoding Method to Detect Vulnerabilities Based on\\nBinary Similarity. IEEE Access 2021, 9, 51631–51645. [CrossRef]\\n95. Mateless, R.; Tsur, O.; Moskovitch, R. Pkg2Vec: Hierarchical package embedding for code authorship attribution.Future Gener.\\nComput. Syst. 2021, 116, 49–60. [CrossRef]\\n96. Arshad, S.; Abid, S.; Shamail, S. CodeBERT for Code Clone Detection: A Replication Study. In Proceedings of the 2022 IEEE 16th\\nInternational Workshop on Software Clones (IWSC), Limassol, Cyprus, 2 October 2022. [CrossRef]\\n97. Kovacevic, A.; Slivka, J.; Vidakovic, D.; Grujic, K.-G.; Luburic, N.; Prokic, S.; Sladic, G. Automatic detection of Long Method and\\nGod Class code smells through neural source code embeddings. Expert Syst. Appl. 2022, 204, 117607. [CrossRef]\\n98. Zhang, A.; Fang, L.; Ge, C.; Li, P .; Liu, Z. Efﬁcient transformer with code token learner for code clone detection.J. Syst. Softw.\\n2023, 197, 111557. [CrossRef]\\n99. Liu, K.; Kim, D.; Bissyande, T.F.; Kim, T.; Kim, K.; Koyuncu, A.; Kim, S.; Le Traon, Y. Learning to Spot and Refactor Inconsistent\\nMethod Names. In Proceedings of the 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), Montreal,\\nQC, Canada, 25–31 May 2019. [CrossRef]\\n100. Cabrera Lozoya, R.; Baumann, A.; Sabetta, A.; Bezzi, M. Commit2Vec: Learning Distributed Representations of Code Changes.\\nSN Comput. Sci. 2021, 2, 150. [CrossRef]\\n101. Wang, S.; Wen, M.; Lin, B.; Mao, X. Lightweight global and local contexts guided method name recommendation with prior\\nknowledge. In Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on\\nthe Foundations of Software Engineering, Athens, Greece, 23–28 August 2021. [CrossRef]\\n102. Nguyen, S.; Phan, H.; Le, T.; Nguyen, T.N. Suggesting natural method names to check name consistencies. In Proceedings of the\\nACM/IEEE 42nd International Conference on Software Engineering (ICSE ‘20). Association for Computing Machinery, New\\nYork, NY, USA; 2020; pp. 1372–1384. [CrossRef]\\n103. Xie, R.; Chen, L.; Ye, W.; Li, Z.; Hu, T.; Du, D.; Zhang, S. DeepLink: A Code Knowledge Graph Based Deep Learning Approach\\nfor Issue-Commit Link Recovery. In Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution\\nand Reengineering (SANER), Hangzhou, China, 24–27 February 2019. [CrossRef]\\n104. Borovits, N.; Kumara, I.; Krishnan, P .; Palma, S.D.; Di Nucci, D.; Palomba, F.; Tamburri, D.A.; van den Heuvel, W.-J. DeepIaC:\\nDeep learning-based linguistic anti-pattern detection in IaC. In Proceedings of the 4th ACM SIGSOFT International Workshop on\\nMachine-Learning Techniques for Software-Quality Evaluation, Virtual, USA, 13 November 2020. [CrossRef]\\n105. Ma, W.; Zhao, M.; Soremekun, E.; Hu, Q.; Zhang, J.M.; Papadakis, M.; Cordy, M.; Xie, X.; Traon, Y.L. GraphCode2Vec: Generic\\ncode embedding via lexical and program dependence analysis. In Proceedings of the 19th International Conference on Mining\\nSoftware Repositories, Pittsburg, PA, USA, 23–24 May 2022. [CrossRef]\\n106. Wan, Y.; He, Y.; Bi, Z.; Zhang, J.; Sui, Y.; Zhang, H.; Hashimoto, K.; Jin, H.; Xu, G.; Xiong, C.; et al. NaturalCC: An Open-Source\\nToolkit for Code Intelligence. In Proceedings of the 2022 IEEE/ACM 44th International Conference on Software Engineering:\\nCompanion Proceedings (ICSE-Companion), Pittsburgh, PA, USA, 22–24 May 2022. [CrossRef]\\n107. Zaharia, S.; Rebedea, T.; Trausan-Matu, S. CWE Pattern Identiﬁcation using Semantical Clustering of Programming Language\\nKeywords. In Proceedings of the 2021 23rd International Conference on Control Systems and Computer Science (CSCS), Bucharest,\\nRomania, 26–28 May 2021. [CrossRef]\\n108. Zaharia, S.; Rebedea, T.; Trausan-Matu, S. Machine Learning-Based Security Pattern Recognition Techniques for Code Developers.\\nAppl. Sci. 2022, 12, 12463. [CrossRef]\\n109. Barr, J.R.; Shaw, P .; Abu-Khzam, F.N.; Thatcher, T.; Yu, S. Vulnerability Rating of Source Code with Token Embedding and\\nCombinatorial Algorithms. Int. J. Semant. Comput. 2020, 14, 501–516. [CrossRef]\\n110. Saletta, M.; Ferretti, C. A Neural Embedding for Source Code: Security Analysis and CWE Lists. In Proceedings of the 2020 IEEE\\nInternational Conference on Dependable, Autonomic and Secure Computing, International Conference on Pervasive Intelligence\\nand Computing, International Conference on Cloud and Big Data Computing, International Conference on Cyber Science and\\nTechnology Congress (DASC/PiCom/CBDCom/CyberSciTech), Calgary, AB, Canada, 17–22 August 2020. [CrossRef]\\n111. Hamed, A.A.; Zachara-Szymanska, M.; Wu, X. Safeguarding authenticity for mitigating the harms of generative AI: Issues,\\nresearch agenda, and policies for detection, fact-checking, and ethical AI. IScience 2024, 27, 108782. [CrossRef]\\nDisclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual\\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.')]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLnTt-oEY340"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF--ob90Y5CK",
        "outputId": "a10ffe09-531e-4730-c410-e67c6e337b66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='Transformers: Learning with Purely Attention\n",
            "Based Networks\n",
            "Lecture Notes on Deep Learning\n",
            "Avi Kak and Charles Bouman\n",
            "Purdue University\n",
            "Saturday 4th May, 2024 05:38\n",
            "© 2024 Avinash Kak, Purdue University\n",
            "Purdue University 1' metadata={'source': 'research_papers/Transformers.pdf', 'page': 0}\n"
          ]
        }
      ],
      "source": [
        "print(wrap_text_preserve_newlines(str(documents[0])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Li5ZfaTbY6-U"
      },
      "outputs": [],
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BfwMr88Y8Lg",
        "outputId": "3dd1ce7e-d20c-4a8d-a690-c6f166070340"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "219"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkniae26Y9OB",
        "outputId": "7ab6bf31-27f0-4a61-8ca0-4616a6ba3374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'research_papers/Transformers.pdf', 'page': 3}, page_content='Preamble (contd.)\\nA neural network with self-attention would be able to accomplish what is\\nmentioned at the bottom of the previous slide. Such a network would therefore be\\nable to answer the question:\\nWhat is the current state of Charlie’s old car?\\nassuming that system already knows that “my friend” in the sentence is referring\\nto Charlie.\\nFor another example, again in seq2seq learning, consider the following Spanish\\ntranslation for the above sentence:\\nYo estaba hablando con mi amigo acerca su viejo coche para averiguar\\nsi todav ´ ıa funcionaba de manera confiable.\\nIn Spanish-to-English translation, the phrase “ su viejo coche ” could go into “ his\\nold car”, “her old car ”, or “ its old car ”. Choosing the correct form would require\\nfor the neural-network based translation system to have established the\\nrelationship between the phrase “ su viejo coche ” and the phrase “ mi amigo”.\\nAgain, a neural network endowed with self-attention should be able to make that\\nconnection.Purdue University 4')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docs[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDtDBshyF9RX"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597,
          "referenced_widgets": [
            "19e2c9a3d0554e50b8ec51cc39f82d1a",
            "07b476b150824069a30227182a8a47f3",
            "9daa2f15c0d6463e8df7e55c1a79551a",
            "f53f776dc13d468cb45690447a65772d",
            "e27c31fc87b341b3bba7609a7c5b89f5",
            "43638e85130b412bb037b4dd2a085df4",
            "fa023a617fc149f49f53e2ddd40b3d8a",
            "5cebfcaadc684ec2961d3f0828ebb2ed",
            "3916536f1b664b75af95cc5aae600096",
            "b3e7b251de544181b31b90f9fb410e5e",
            "ee269fd95e53454fb777d956f8968da1",
            "9d2e6a055dd440f6ab487cbaaca963bc",
            "e74fef631f854be5af8cecaebe3cf162",
            "d204e845e9e74a45b2042b340227e33a",
            "e433a6f849c44f85b81451adc666cfe2",
            "85d37e47c2d64290b75176e5609623c7",
            "9d91746d48714cee99e49ca4a5d8c87f",
            "3df32b8277694f378a40f48e833d09a3",
            "2c46847c070b41e8b4a6ec3e4a9eee13",
            "19fbbc8cd4284529a00ab36b928528f7",
            "65e37ee57d4042ab9b361970597b2e58",
            "a7868fbf9c734c37a4526cae2cfd75e6",
            "a7bac6275f094f72bd85896313530cc8",
            "2f069a4d78fc4ccf8f59daafda2675b9",
            "a4e9bbcea1ea472eb4e727cac24baaa3",
            "a56e3f7ff83f4912bc0d3f64a7af1fb3",
            "4bdf7d5333be4ed2a9f063a7245eff6f",
            "e17e028f3ae049489d7a6d014dfbacd7",
            "e94ef25381584d48a9775ffa2f1d12d7",
            "131dece3f12b427592ded50f5a6ce168",
            "91fd327af09e4611b59aa906bf803212",
            "76ffb7027b684abc8ebc5a8f768b3d9d",
            "99622ad505a945cca0ebd7eefebc4ef6",
            "f21a7cf1ba864e269a562a35e9072d6f",
            "48b31ec54619433b8e2cfe33386616b4",
            "6a9848dee3d64255ab59c91f5d509523",
            "404ffe5fb2dc45919e332f68456b3758",
            "f81996d7b7504e5085815b88ede135ba",
            "8d608f9a070b422183dcb590559d5011",
            "9da2de368cce4b4d95d7048f7a296571",
            "8b6a78f675334d849144934aaa951b6a",
            "2abeb1cb6a61404aa9329e8d1976acd3",
            "a5bc0b8e95e64e8c8c9bf0d7f3b2411f",
            "45e31f1e12b249b8b1472de892ad4682",
            "f8bd99f3b13a404d8d9649a475ddfa77",
            "cfd8945efc1e4df5955ce5ebfd3de24f",
            "0d353f23cf8846a89ca74b1e7e0badde",
            "97494782cac2433bb0e93fb2f3c30b89",
            "39994a7e111947c095d147a1d0a83c04",
            "9fd3e944d38443f8802a07de3e87530f",
            "c89280b80b884de7929f91c34ef4e00f",
            "8146aa8c394249189c801e94f048d579",
            "9d76c10ad89c4da2a25e8392d1d0b8f3",
            "24ff236c48dd4a5093a0e840bdb3a819",
            "f65efab0b61e4164b732544c7dbc6801",
            "e4ac0bca5a4c4db29af38088fd6b8c70",
            "d5af0739550d4ce1baec3f46b5f09054",
            "da5078f62c9c4e10914f0ad25ef9f1a7",
            "4192cd0050fe4ef391cc95ad1d102845",
            "705869d11d6c472381d21373c702fd33",
            "f6ffa6db4fe541c69ba588bb86053191",
            "988867c53d374438ba7d725aa4432caf",
            "3f5ee0fa964643fbb70f64d5b243a0e1",
            "1b6c4593d6da4184b814a028377d40fd",
            "c270fda717da49b2bb22a9a1e72ee6a5",
            "ec5a5093d7f24ef2871911a32891426b",
            "0f4f15a89c194980be7a2e000506e517",
            "39fc20c0a51f4dd58de7d3ec3f77fb84",
            "71f8468bb5f14c75b9e5bd66d7e092c4",
            "95cda4615bc241fc9102858a96c819e2",
            "1617c378cbd049b3b198208895510c4d",
            "9a854232e98641928b58bdce90833cdf",
            "2e6f95f10edc41a4b97eb2b174d561d0",
            "05fcb7ba256e4503a0dec9d1512ac76a",
            "a5454eaf719647808c70fcc11a4c3732",
            "58c3963e35214c47b23006bddad5e4c8",
            "851966760fdd4e94b4589f2a59abddc5",
            "aee65a635a03476f95d07240bc668d23",
            "05321aa85ba049e7ac4249d64916289f",
            "97e6564355844d35a07ef40d2c537bc9",
            "73044da2661c4f04b2e0911df530cb9c",
            "4d888577db344e8caafe6a0dd409427b",
            "4757d88871374b2caeb44fd80c573e2a",
            "1f81351c903b45b7ae057535aa681915",
            "3a0420650d994bf0821ce73203931a8b",
            "5203170acc5540a9965a2507fc2d6d8c",
            "7785f858f8a547ad9e987cd7b2b0f72b",
            "98062cff88044646a14dd033c842f071",
            "51b850e143ff42d58e15c3fc8e872b4c",
            "fb77a3dd77e3435db8ae7e624723a5e5",
            "3e9b974d656f4f67b8553162100774f9",
            "7038d958284a41ba91c0117d26bb4c4a",
            "55a4ec067bc7476b98a5c966ed17ff13",
            "adaee4cba71249d0a844d70392e082d1",
            "788d023c7c4a4dfeb24efa4f723adb34",
            "e6ca7c0ce9c546a2b892bad057c234c4",
            "713dc9f58c0145ac9e81251ea4f48d68",
            "7dccd78f43864e2da423d35d7baddded",
            "a0e82fc50c00472b8bb0736ae14c33df",
            "8a43a5ab1a3f4e17a02f71911ab8032e",
            "b97b95f52204448c8cbd307aad986bd8",
            "a3337b39463f4aafb9b3464566d2df8d",
            "df070fbfc9c4478c9a019282f086b4d3",
            "66695fcb23534dc8ba76e998d415b770",
            "6c5bb70e10574baeb321abdd1ed07293",
            "7b3e6b6d27b140889b7ce153757eee5e",
            "841b59db7f7c460b884ee35ed2325e54",
            "87c7eb69326840269c0078269709a8e5",
            "181278868d76438fa3ae938947f2b34c",
            "6227f935cf624c09891c8915b9031eff",
            "5a2ec683860745bd8825412434202142",
            "e484d54840eb4df5ba9757d05e499deb",
            "271a2d157a584e7c912623b735c3c35f",
            "9856f6e9764344dd84b066b55173e21a",
            "cb143a4fc8ac431ca227824402182534",
            "4d7614bd123a493d8ff0265662f9fe76",
            "b4addbeabde94635826fdbfe2a695587",
            "a2a91d1ba8a940dd91fefa3970b90460",
            "fc13cbed3fc548489f678462b8766c35",
            "39151d6b953840feb3ae34b86c0c8cac",
            "eddf4d2d14e745d7bfd8628680e3c182"
          ]
        },
        "id": "Af_nPlw5Y_TV",
        "outputId": "0f0f2001-7203-4be0-aaa8-d084ffcade07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-73e5645c93de>:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "<ipython-input-12-73e5645c93de>:4: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n",
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19e2c9a3d0554e50b8ec51cc39f82d1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d2e6a055dd440f6ab487cbaaca963bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7bac6275f094f72bd85896313530cc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f21a7cf1ba864e269a562a35e9072d6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8bd99f3b13a404d8d9649a475ddfa77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4ac0bca5a4c4db29af38088fd6b8c70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f4f15a89c194980be7a2e000506e517",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aee65a635a03476f95d07240bc668d23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51b850e143ff42d58e15c3fc8e872b4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a43a5ab1a3f4e17a02f71911ab8032e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5a2ec683860745bd8825412434202142",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rouhZ97QZDqp"
      },
      "outputs": [],
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QG12dd6NZFSh"
      },
      "outputs": [],
      "source": [
        "query = \"What is positional encoding\"\n",
        "docs = db.similarity_search(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2-sOrcWZGVq",
        "outputId": "f5470186-ff72-4031-c218-4bce607a7ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Positional Encoding for the Words\n",
            "Positional Encoding for the Words\n",
            "The main goal of positional encoding is to sensitize a neural network\n",
            "to the position of each word in a sentence and also to each\n",
            "embedding-vector cell for each word.\n",
            "Positional encoding can be achieved by first constructing an array of\n",
            "floating-point values as illustrated on the next slide and then adding\n",
            "that array of numbers to the sentence tensor.\n",
            "The alternating columns of the 2D array shown on the next slide are\n",
            "filled using sine and cosine functions whose periodicities vary with the\n",
            "column index in the pattern.\n",
            "Note that whereas the periodicities are column-specific, the\n",
            "numerators of the args to the sine and cosine functions are\n",
            "word-position-specific. In the depiction shown on the next slide, each\n",
            "row is an embedding vector for a specific word.\n",
            "Purdue University 55\n"
          ]
        }
      ],
      "source": [
        "print(wrap_text_preserve_newlines(str(docs[0].page_content)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sccZa5ynGGGg"
      },
      "source": [
        "#QA Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHIZ79LEZZiU"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hPtUQYlZak-",
        "outputId": "9cee6fd8-7537-447e-d43d-9eaae147c365"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-244c840cd916>:1: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm=HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1, max_length=512)\n",
            "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        }
      ],
      "source": [
        "llm=HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9W-8BpqZdI3",
        "outputId": "c3537d2d-f09b-4512-b6ea-d9a5201e0fdf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-1b5424917bc0>:1: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
          ]
        }
      ],
      "source": [
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "skHOnzl7ZeSd",
        "outputId": "34b18fb6-6567-43da-97ec-17cfeb7540a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-998a67874790>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run(input_documents=docs, question=query)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Positional encoding is a method used in natural language processing to provide information about the position of a word in a sentence to a neural network. It is achieved by adding a fixed array of numbers to the sentence tensor, which varies based on the position of each word. The numbers in the array are generated using sine and cosine functions with varying periodicities. This allows the neural network to learn the relationship between the position of a word and its meaning, even when the order of the words in the sentence changes.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is positional encoding\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "QeENKNWmZfsx",
        "outputId": "7af4ae05-1314-454a-9a3e-dedb67e819de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' A transformer is a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. It is designed for sequence-to-sequence tasks, such as machine translation, and uses an encoder-decoder architecture with self-attention mechanisms. The encoder and decoder each consist of multiple identical layers, and each layer contains a multi-head self-attention mechanism followed by a feed-forward network. The self-attention mechanism allows the model to focus on different parts of the input sequence when generating an output, making it particularly effective for long-range dependencies. Transformers have been widely adopted in natural language processing tasks and have achieved state-of-the-art results on various benchmarks.\\n\\nDetailed Answer:\\n\\nTransformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. published in 2017. The authors proposed this architecture as a more efficient alternative to recurrent neural networks (RNNs) and long short-term memory (LSTM) networks for sequence-to-sequence tasks, such as machine translation.\\n\\nThe transformer architecture consists of an encoder and a decoder, both of which follow an identical structure. Each of these components is made up of multiple identical layers. Each layer contains a multi-head self-attention mechanism followed by a feed-forward network.\\n\\nThe self-attention mechanism is the key innovation in transformers. It allows the model to focus on different parts of the input sequence when generating an output, making it particularly effective for long-range dependencies. In contrast, RNNs and LSTMs rely on recurrent connections to capture dependencies between elements in a sequence, which can be less efficient for long sequences.\\n\\nThe self-attention mechanism works by computing a weighted sum of the input sequence, where the weights are determined by the similarity between each pair of input elements. This is achieved through a dot product between the input vectors and a set of query, key, and value vectors. The resulting attention scores are then used to compute a weighted sum of the input sequence, with the weights determined by the attention scores.\\n\\nThe transformer architecture has been widely adopted in natural language processing tasks and has achieved state-of-the-art results on various benchmarks. It has also been extended to other applications, such as computer vision and speech recognition.\\n'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What is transformer\"\n",
        "docs = db.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdrScs9m6VM7"
      },
      "source": [
        "#Working with PDFs Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0r-D_dFcD9K"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frZbE9UBcGtB",
        "outputId": "8e18895c-c796-4091-b0b8-42875b89cd5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Transformers.pdf',\n",
              " 'From Turing to Transformers A Comprehensive Review and Tutorial on the Evolution and Applications of Generative Transformer Models.pdf',\n",
              " '2304.10557v5.pdf',\n",
              " 'Transformer models an introduction and catalog.pdf',\n",
              " '9.pdf',\n",
              " 'AI-Assisted Programming Tasks Using Code Embeddings and Transformers.pdf']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# connect your Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "pdf_folder_path = './research_papers'\n",
        "os.listdir(pdf_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHdEeqkOcI3e",
        "outputId": "4e76a596-4591-47f3-eab4-870eff2681ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7bd44ff0e5c0>,\n",
              " <langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7bd464b1f1f0>,\n",
              " <langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7bd44ff0da20>,\n",
              " <langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7bd44ff0dc00>,\n",
              " <langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7bd44ff0db40>,\n",
              " <langchain_community.document_loaders.pdf.UnstructuredPDFLoader at 0x7bd435e6bd60>]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaders = [UnstructuredPDFLoader(os.path.join(pdf_folder_path, fn)) for fn in os.listdir(pdf_folder_path)]\n",
        "loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97qdPpswcKYV",
        "outputId": "52a59929-04b3-43a9-95a6-0789c5e797ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9f570648a855>:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding=HuggingFaceEmbeddings(),\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/indexes/vectorstore.py:128: UserWarning: Using InMemoryVectorStore as the default vectorstore.This memory store won't persist data. You should explicitlyspecify a vectorstore when using VectorstoreIndexCreator\n",
            "  warnings.warn(\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1045, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1032, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1026, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1447, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2136, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1371, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1210, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1246, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1064, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1192, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2034, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1837, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1281, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1100, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1414, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3381, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1229, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3456, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1049, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 3135, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1032, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2042, which is longer than the specified 1000\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 2042, which is longer than the specified 1000\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "index = VectorstoreIndexCreator(\n",
        "    embedding=HuggingFaceEmbeddings(),\n",
        "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)).from_loaders(loaders)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf8htedZcLrZ",
        "outputId": "d2212696-7de0-4c5c-8e03-f78574a2e494"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e6e1284ffd76>:3: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm=HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1, max_length=512)\n",
            "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "\n",
        "llm=HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", temperature=0.1, max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PG9liYa3cNZl"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                    chain_type=\"stuff\",\n",
        "                                    retriever=index.vectorstore.as_retriever(),\n",
        "                                    input_key=\"question\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "SGPPVx5ZcQCw",
        "outputId": "f2ab8a39-5431-433a-ec32-f21c4e3a11d3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-93f4657e4879>:1: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  chain.run('What is positional encoding?')\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Positional encoding is a technique used in machine learning models, such as transformers, to provide the model with information about the position or location of data points in a sequence. In the context of the given text, positional encoding is used for both words in a sentence and patches in an image. It is achieved by adding a specific embedding to the base embedding of each data point, which encodes the position information. In the case of language modeling, sinusoidal positional encoding is used, while for patch sequences in vision transformers, per-patch positional encodings are used.'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run('What is positional encoding?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "q0-quBGmITDZ",
        "outputId": "7d9a6c77-c770-46d5-8286-0e8a858b6b8a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Transformers are a class of deep learning models that are defined by some architectural traits, including the use of attention mechanisms as the primary method for deriving dependencies between input and output, and the encoder-decoder architecture. They were introduced in the \"Attention is All you Need\" paper in 2017 and have since become very popular due to their effectiveness in various natural language processing tasks.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run('What are transformers?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "hj_9qTs-Ie_6",
        "outputId": "7ff7b16d-f3cc-4b2a-b44d-0ed142d9b9af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Machine learning is a subfield of artificial intelligence (AI) that involves training algorithms to learn patterns and make predictions or decisions based on data, rather than being explicitly programmed to perform a specific task. It is a type of adaptive learning where the model improves its performance as it is exposed to more data. Machine learning algorithms can be categorized into supervised, unsupervised, and reinforcement learning, depending on the type of data they are trained on and the desired outcome.\\n\\nDetailed Answer: Machine learning is a subset of artificial intelligence (AI) that focuses on enabling systems to automatically learn and improve from experience without being explicitly programmed. It is a type of adaptive learning where the model improves its performance as it is exposed to more data. Machine learning algorithms can be categorized into three main types: supervised learning, unsupervised learning, and reinforcement learning.\\n\\nSupervised learning involves training a model on labeled data, where the desired output is known. The model learns to map inputs to outputs by minimizing the error between its predictions and the actual outputs. Examples of supervised learning algorithms include linear regression, logistic regression, decision trees, and support vector machines.\\n\\nUnsupervised learning, on the other hand, involves training a model on unlabeled data, where the desired output is not known. The model learns to identify patterns and structures in the data by minimizing the internal data representation. Examples of unsupervised learning algorithms include clustering algorithms like K-means and hierarchical clustering, and dimensionality reduction techniques like principal component analysis (PCA) and t-SNE.\\n\\nReinforcement learning is a type of machine learning where an agent learns to make decisions by interacting with its environment and receiving rewards or penalties based on its actions. The agent learns to maximize its rewards by adjusting its actions based on the feedback it receives from the environment. Examples of reinforcement learning algorithms include Q-learning and deep Q-learning.\\n\\nMachine learning has applications in various domains, including image and speech recognition, natural language processing, recommendation systems, fraud detection, and autonomous vehicles, among others. It is a rapidly evolving field, with new algorithms and techniques being developed regularly to address the growing demand for intelligent systems that can learn from data and make accurate predictions or decisions.\\n\\nTuring’s vision of machines learning from experience, as outlined in his lesser-known report “'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.run('what is machine learning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s-fOsMIRY0B"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05321aa85ba049e7ac4249d64916289f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4757d88871374b2caeb44fd80c573e2a",
            "placeholder": "​",
            "style": "IPY_MODEL_1f81351c903b45b7ae057535aa681915",
            "value": "vocab.txt: 100%"
          }
        },
        "05fcb7ba256e4503a0dec9d1512ac76a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b476b150824069a30227182a8a47f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43638e85130b412bb037b4dd2a085df4",
            "placeholder": "​",
            "style": "IPY_MODEL_fa023a617fc149f49f53e2ddd40b3d8a",
            "value": "modules.json: 100%"
          }
        },
        "0d353f23cf8846a89ca74b1e7e0badde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8146aa8c394249189c801e94f048d579",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d76c10ad89c4da2a25e8392d1d0b8f3",
            "value": 571
          }
        },
        "0f4f15a89c194980be7a2e000506e517": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39fc20c0a51f4dd58de7d3ec3f77fb84",
              "IPY_MODEL_71f8468bb5f14c75b9e5bd66d7e092c4",
              "IPY_MODEL_95cda4615bc241fc9102858a96c819e2"
            ],
            "layout": "IPY_MODEL_1617c378cbd049b3b198208895510c4d"
          }
        },
        "131dece3f12b427592ded50f5a6ce168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1617c378cbd049b3b198208895510c4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181278868d76438fa3ae938947f2b34c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19e2c9a3d0554e50b8ec51cc39f82d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_07b476b150824069a30227182a8a47f3",
              "IPY_MODEL_9daa2f15c0d6463e8df7e55c1a79551a",
              "IPY_MODEL_f53f776dc13d468cb45690447a65772d"
            ],
            "layout": "IPY_MODEL_e27c31fc87b341b3bba7609a7c5b89f5"
          }
        },
        "19fbbc8cd4284529a00ab36b928528f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b6c4593d6da4184b814a028377d40fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f81351c903b45b7ae057535aa681915": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "24ff236c48dd4a5093a0e840bdb3a819": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "271a2d157a584e7c912623b735c3c35f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2a91d1ba8a940dd91fefa3970b90460",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc13cbed3fc548489f678462b8766c35",
            "value": 190
          }
        },
        "2abeb1cb6a61404aa9329e8d1976acd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c46847c070b41e8b4a6ec3e4a9eee13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e6f95f10edc41a4b97eb2b174d561d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f069a4d78fc4ccf8f59daafda2675b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e17e028f3ae049489d7a6d014dfbacd7",
            "placeholder": "​",
            "style": "IPY_MODEL_e94ef25381584d48a9775ffa2f1d12d7",
            "value": "README.md: 100%"
          }
        },
        "39151d6b953840feb3ae34b86c0c8cac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3916536f1b664b75af95cc5aae600096": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39994a7e111947c095d147a1d0a83c04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fc20c0a51f4dd58de7d3ec3f77fb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a854232e98641928b58bdce90833cdf",
            "placeholder": "​",
            "style": "IPY_MODEL_2e6f95f10edc41a4b97eb2b174d561d0",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "3a0420650d994bf0821ce73203931a8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3df32b8277694f378a40f48e833d09a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e9b974d656f4f67b8553162100774f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6ca7c0ce9c546a2b892bad057c234c4",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_713dc9f58c0145ac9e81251ea4f48d68",
            "value": 466021
          }
        },
        "3f5ee0fa964643fbb70f64d5b243a0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "404ffe5fb2dc45919e332f68456b3758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5bc0b8e95e64e8c8c9bf0d7f3b2411f",
            "placeholder": "​",
            "style": "IPY_MODEL_45e31f1e12b249b8b1472de892ad4682",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.72kB/s]"
          }
        },
        "4192cd0050fe4ef391cc95ad1d102845": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c270fda717da49b2bb22a9a1e72ee6a5",
            "placeholder": "​",
            "style": "IPY_MODEL_ec5a5093d7f24ef2871911a32891426b",
            "value": " 438M/438M [00:01&lt;00:00, 243MB/s]"
          }
        },
        "43638e85130b412bb037b4dd2a085df4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45e31f1e12b249b8b1472de892ad4682": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4757d88871374b2caeb44fd80c573e2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b31ec54619433b8e2cfe33386616b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d608f9a070b422183dcb590559d5011",
            "placeholder": "​",
            "style": "IPY_MODEL_9da2de368cce4b4d95d7048f7a296571",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "4bdf7d5333be4ed2a9f063a7245eff6f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d7614bd123a493d8ff0265662f9fe76": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d888577db344e8caafe6a0dd409427b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51b850e143ff42d58e15c3fc8e872b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb77a3dd77e3435db8ae7e624723a5e5",
              "IPY_MODEL_3e9b974d656f4f67b8553162100774f9",
              "IPY_MODEL_7038d958284a41ba91c0117d26bb4c4a"
            ],
            "layout": "IPY_MODEL_55a4ec067bc7476b98a5c966ed17ff13"
          }
        },
        "5203170acc5540a9965a2507fc2d6d8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55a4ec067bc7476b98a5c966ed17ff13": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58c3963e35214c47b23006bddad5e4c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2ec683860745bd8825412434202142": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e484d54840eb4df5ba9757d05e499deb",
              "IPY_MODEL_271a2d157a584e7c912623b735c3c35f",
              "IPY_MODEL_9856f6e9764344dd84b066b55173e21a"
            ],
            "layout": "IPY_MODEL_cb143a4fc8ac431ca227824402182534"
          }
        },
        "5cebfcaadc684ec2961d3f0828ebb2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6227f935cf624c09891c8915b9031eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65e37ee57d4042ab9b361970597b2e58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66695fcb23534dc8ba76e998d415b770": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a9848dee3d64255ab59c91f5d509523": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b6a78f675334d849144934aaa951b6a",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2abeb1cb6a61404aa9329e8d1976acd3",
            "value": 53
          }
        },
        "6c5bb70e10574baeb321abdd1ed07293": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7038d958284a41ba91c0117d26bb4c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dccd78f43864e2da423d35d7baddded",
            "placeholder": "​",
            "style": "IPY_MODEL_a0e82fc50c00472b8bb0736ae14c33df",
            "value": " 466k/466k [00:00&lt;00:00, 1.11MB/s]"
          }
        },
        "705869d11d6c472381d21373c702fd33": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "713dc9f58c0145ac9e81251ea4f48d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "71f8468bb5f14c75b9e5bd66d7e092c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05fcb7ba256e4503a0dec9d1512ac76a",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a5454eaf719647808c70fcc11a4c3732",
            "value": 363
          }
        },
        "73044da2661c4f04b2e0911df530cb9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7785f858f8a547ad9e987cd7b2b0f72b",
            "placeholder": "​",
            "style": "IPY_MODEL_98062cff88044646a14dd033c842f071",
            "value": " 232k/232k [00:00&lt;00:00, 12.3MB/s]"
          }
        },
        "76ffb7027b684abc8ebc5a8f768b3d9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7785f858f8a547ad9e987cd7b2b0f72b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788d023c7c4a4dfeb24efa4f723adb34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b3e6b6d27b140889b7ce153757eee5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dccd78f43864e2da423d35d7baddded": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8146aa8c394249189c801e94f048d579": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "841b59db7f7c460b884ee35ed2325e54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "851966760fdd4e94b4589f2a59abddc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "85d37e47c2d64290b75176e5609623c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c7eb69326840269c0078269709a8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a43a5ab1a3f4e17a02f71911ab8032e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b97b95f52204448c8cbd307aad986bd8",
              "IPY_MODEL_a3337b39463f4aafb9b3464566d2df8d",
              "IPY_MODEL_df070fbfc9c4478c9a019282f086b4d3"
            ],
            "layout": "IPY_MODEL_66695fcb23534dc8ba76e998d415b770"
          }
        },
        "8b6a78f675334d849144934aaa951b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d608f9a070b422183dcb590559d5011": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91fd327af09e4611b59aa906bf803212": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95cda4615bc241fc9102858a96c819e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c3963e35214c47b23006bddad5e4c8",
            "placeholder": "​",
            "style": "IPY_MODEL_851966760fdd4e94b4589f2a59abddc5",
            "value": " 363/363 [00:00&lt;00:00, 20.5kB/s]"
          }
        },
        "97494782cac2433bb0e93fb2f3c30b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24ff236c48dd4a5093a0e840bdb3a819",
            "placeholder": "​",
            "style": "IPY_MODEL_f65efab0b61e4164b732544c7dbc6801",
            "value": " 571/571 [00:00&lt;00:00, 43.1kB/s]"
          }
        },
        "97e6564355844d35a07ef40d2c537bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a0420650d994bf0821ce73203931a8b",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5203170acc5540a9965a2507fc2d6d8c",
            "value": 231536
          }
        },
        "98062cff88044646a14dd033c842f071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9856f6e9764344dd84b066b55173e21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39151d6b953840feb3ae34b86c0c8cac",
            "placeholder": "​",
            "style": "IPY_MODEL_eddf4d2d14e745d7bfd8628680e3c182",
            "value": " 190/190 [00:00&lt;00:00, 11.4kB/s]"
          }
        },
        "988867c53d374438ba7d725aa4432caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99622ad505a945cca0ebd7eefebc4ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a854232e98641928b58bdce90833cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d2e6a055dd440f6ab487cbaaca963bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e74fef631f854be5af8cecaebe3cf162",
              "IPY_MODEL_d204e845e9e74a45b2042b340227e33a",
              "IPY_MODEL_e433a6f849c44f85b81451adc666cfe2"
            ],
            "layout": "IPY_MODEL_85d37e47c2d64290b75176e5609623c7"
          }
        },
        "9d76c10ad89c4da2a25e8392d1d0b8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d91746d48714cee99e49ca4a5d8c87f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9da2de368cce4b4d95d7048f7a296571": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9daa2f15c0d6463e8df7e55c1a79551a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cebfcaadc684ec2961d3f0828ebb2ed",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3916536f1b664b75af95cc5aae600096",
            "value": 349
          }
        },
        "9fd3e944d38443f8802a07de3e87530f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0e82fc50c00472b8bb0736ae14c33df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2a91d1ba8a940dd91fefa3970b90460": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3337b39463f4aafb9b3464566d2df8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_841b59db7f7c460b884ee35ed2325e54",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87c7eb69326840269c0078269709a8e5",
            "value": 239
          }
        },
        "a4e9bbcea1ea472eb4e727cac24baaa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_131dece3f12b427592ded50f5a6ce168",
            "max": 10621,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91fd327af09e4611b59aa906bf803212",
            "value": 10621
          }
        },
        "a5454eaf719647808c70fcc11a4c3732": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a56e3f7ff83f4912bc0d3f64a7af1fb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76ffb7027b684abc8ebc5a8f768b3d9d",
            "placeholder": "​",
            "style": "IPY_MODEL_99622ad505a945cca0ebd7eefebc4ef6",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 437kB/s]"
          }
        },
        "a5bc0b8e95e64e8c8c9bf0d7f3b2411f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7868fbf9c734c37a4526cae2cfd75e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7bac6275f094f72bd85896313530cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f069a4d78fc4ccf8f59daafda2675b9",
              "IPY_MODEL_a4e9bbcea1ea472eb4e727cac24baaa3",
              "IPY_MODEL_a56e3f7ff83f4912bc0d3f64a7af1fb3"
            ],
            "layout": "IPY_MODEL_4bdf7d5333be4ed2a9f063a7245eff6f"
          }
        },
        "adaee4cba71249d0a844d70392e082d1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee65a635a03476f95d07240bc668d23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05321aa85ba049e7ac4249d64916289f",
              "IPY_MODEL_97e6564355844d35a07ef40d2c537bc9",
              "IPY_MODEL_73044da2661c4f04b2e0911df530cb9c"
            ],
            "layout": "IPY_MODEL_4d888577db344e8caafe6a0dd409427b"
          }
        },
        "b3e7b251de544181b31b90f9fb410e5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4addbeabde94635826fdbfe2a695587": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b97b95f52204448c8cbd307aad986bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5bb70e10574baeb321abdd1ed07293",
            "placeholder": "​",
            "style": "IPY_MODEL_7b3e6b6d27b140889b7ce153757eee5e",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "c270fda717da49b2bb22a9a1e72ee6a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c89280b80b884de7929f91c34ef4e00f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb143a4fc8ac431ca227824402182534": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfd8945efc1e4df5955ce5ebfd3de24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fd3e944d38443f8802a07de3e87530f",
            "placeholder": "​",
            "style": "IPY_MODEL_c89280b80b884de7929f91c34ef4e00f",
            "value": "config.json: 100%"
          }
        },
        "d204e845e9e74a45b2042b340227e33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c46847c070b41e8b4a6ec3e4a9eee13",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_19fbbc8cd4284529a00ab36b928528f7",
            "value": 116
          }
        },
        "d5af0739550d4ce1baec3f46b5f09054": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ffa6db4fe541c69ba588bb86053191",
            "placeholder": "​",
            "style": "IPY_MODEL_988867c53d374438ba7d725aa4432caf",
            "value": "model.safetensors: 100%"
          }
        },
        "da5078f62c9c4e10914f0ad25ef9f1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f5ee0fa964643fbb70f64d5b243a0e1",
            "max": 437971872,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b6c4593d6da4184b814a028377d40fd",
            "value": 437971872
          }
        },
        "df070fbfc9c4478c9a019282f086b4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_181278868d76438fa3ae938947f2b34c",
            "placeholder": "​",
            "style": "IPY_MODEL_6227f935cf624c09891c8915b9031eff",
            "value": " 239/239 [00:00&lt;00:00, 13.0kB/s]"
          }
        },
        "e17e028f3ae049489d7a6d014dfbacd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e27c31fc87b341b3bba7609a7c5b89f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e433a6f849c44f85b81451adc666cfe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65e37ee57d4042ab9b361970597b2e58",
            "placeholder": "​",
            "style": "IPY_MODEL_a7868fbf9c734c37a4526cae2cfd75e6",
            "value": " 116/116 [00:00&lt;00:00, 4.24kB/s]"
          }
        },
        "e484d54840eb4df5ba9757d05e499deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d7614bd123a493d8ff0265662f9fe76",
            "placeholder": "​",
            "style": "IPY_MODEL_b4addbeabde94635826fdbfe2a695587",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "e4ac0bca5a4c4db29af38088fd6b8c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d5af0739550d4ce1baec3f46b5f09054",
              "IPY_MODEL_da5078f62c9c4e10914f0ad25ef9f1a7",
              "IPY_MODEL_4192cd0050fe4ef391cc95ad1d102845"
            ],
            "layout": "IPY_MODEL_705869d11d6c472381d21373c702fd33"
          }
        },
        "e6ca7c0ce9c546a2b892bad057c234c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e74fef631f854be5af8cecaebe3cf162": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d91746d48714cee99e49ca4a5d8c87f",
            "placeholder": "​",
            "style": "IPY_MODEL_3df32b8277694f378a40f48e833d09a3",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "e94ef25381584d48a9775ffa2f1d12d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec5a5093d7f24ef2871911a32891426b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eddf4d2d14e745d7bfd8628680e3c182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee269fd95e53454fb777d956f8968da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f21a7cf1ba864e269a562a35e9072d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48b31ec54619433b8e2cfe33386616b4",
              "IPY_MODEL_6a9848dee3d64255ab59c91f5d509523",
              "IPY_MODEL_404ffe5fb2dc45919e332f68456b3758"
            ],
            "layout": "IPY_MODEL_f81996d7b7504e5085815b88ede135ba"
          }
        },
        "f53f776dc13d468cb45690447a65772d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3e7b251de544181b31b90f9fb410e5e",
            "placeholder": "​",
            "style": "IPY_MODEL_ee269fd95e53454fb777d956f8968da1",
            "value": " 349/349 [00:00&lt;00:00, 15.3kB/s]"
          }
        },
        "f65efab0b61e4164b732544c7dbc6801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6ffa6db4fe541c69ba588bb86053191": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81996d7b7504e5085815b88ede135ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8bd99f3b13a404d8d9649a475ddfa77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfd8945efc1e4df5955ce5ebfd3de24f",
              "IPY_MODEL_0d353f23cf8846a89ca74b1e7e0badde",
              "IPY_MODEL_97494782cac2433bb0e93fb2f3c30b89"
            ],
            "layout": "IPY_MODEL_39994a7e111947c095d147a1d0a83c04"
          }
        },
        "fa023a617fc149f49f53e2ddd40b3d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb77a3dd77e3435db8ae7e624723a5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adaee4cba71249d0a844d70392e082d1",
            "placeholder": "​",
            "style": "IPY_MODEL_788d023c7c4a4dfeb24efa4f723adb34",
            "value": "tokenizer.json: 100%"
          }
        },
        "fc13cbed3fc548489f678462b8766c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
